"""
Wave Detection 3.0 - FINAL ENHANCED PRODUCTION VERSION
==================================================================
Professional Stock Ranking System with Advanced Market State Analytics
All bugs fixed, optimized for Streamlit Community Cloud
Enhanced with intelligent market regime awareness and adaptive scoring

Version: 3.1.0-PROFESSIONAL
Last Updated: September 2025
Status: PRODUCTION READY - Market State Integration Complete

MARKET STATE SYSTEM:
- Intelligent regime detection (8 states: STRONG_UPTREND, UPTREND, PULLBACK, etc.)
- Dynamic component score adjustments based on market conditions
- Smart bonuses for exceptional market state patterns
- Comprehensive error handling and logging
"""

# ============================================
# STREAMLIT CONFIGURATION - Prevent File Watcher Issues
# ============================================
import os
os.environ['STREAMLIT_SERVER_FILE_WATCHER_TYPE'] = 'none'

# ============================================
# IMPORTS AND SETUP
# ============================================

# Standard library imports
import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
from datetime import datetime, timezone
import logging
from typing import Dict, List, Tuple, Optional, Any, Union  # Add Union back for type hints
from dataclasses import dataclass, field
from functools import wraps  # Remove lru_cache
import time
from io import BytesIO
import warnings
import gc
import re

# Suppress warnings for clean production output.
warnings.filterwarnings('ignore')

# Set NumPy to ignore floating point errors for robust calculations.
np.seterr(all='ignore')

# Set random seed for reproducibility of any random-based operations.
np.random.seed(42)

# ============================================
# SAFE DIVISION UTILITIES
# Comprehensive zero-division protection with type handling
# ============================================

def safe_divide(numerator: Union[float, int, np.ndarray, pd.Series],
                denominator: Union[float, int, np.ndarray, pd.Series],
                default: Union[float, int] = 0,
                handle_inf: bool = True,
                warn_on_zero: bool = False) -> Union[float, np.ndarray, pd.Series]:
    """
    Safely perform division operations, handling division by zero and other edge cases.

    Parameters:
    -----------
    numerator : float, int, np.ndarray, or pd.Series
        The numerator value(s)
    denominator : float, int, np.ndarray, or pd.Series
        The denominator value(s)
    default : float or int, default=0
        Value to return when division by zero occurs
    handle_inf : bool, default=True
        Whether to replace infinity values with default
    warn_on_zero : bool, default=False
        Whether to emit warnings when division by zero occurs

    Returns:
    --------
    float, np.ndarray, or pd.Series
        Result of safe division operation
    """
    # Handle pandas Series and numpy arrays
    if isinstance(numerator, (pd.Series, np.ndarray)) or isinstance(denominator, (pd.Series, np.ndarray)):
        # Convert to numpy arrays for consistent handling
        num_array = np.asarray(numerator)
        den_array = np.asarray(denominator)

        # Create result array initialized with default values
        result = np.full_like(num_array, default, dtype=float)

        # Handle different cases
        if isinstance(denominator, (pd.Series, np.ndarray)):
            # Element-wise division with zero checking
            valid_mask = (den_array != 0) & pd.notna(den_array) & pd.notna(num_array)

            if warn_on_zero and np.any(den_array == 0):
                warnings.warn("Division by zero encountered in safe_divide", RuntimeWarning)

            # Perform division only for valid elements
            result[valid_mask] = num_array[valid_mask] / den_array[valid_mask]
        else:
            # Scalar denominator
            if denominator != 0 and pd.notna(denominator):
                valid_mask = pd.notna(num_array)
                result[valid_mask] = num_array[valid_mask] / denominator
            elif warn_on_zero:
                warnings.warn("Division by zero encountered in safe_divide", RuntimeWarning)

        # Handle infinity values if requested
        if handle_inf:
            result = np.where(np.isinf(result), default, result)

        # Return same type as input
        if isinstance(numerator, pd.Series):
            return pd.Series(result, index=numerator.index)
        else:
            return result

    else:
        # Handle scalar values
        if denominator == 0 or pd.isna(denominator) or pd.isna(numerator):
            if warn_on_zero and denominator == 0:
                warnings.warn("Division by zero encountered in safe_divide", RuntimeWarning)
            return default

        result = numerator / denominator

        # Handle infinity
        if handle_inf and np.isinf(result):
            return default

        return result


def safe_percentage(numerator: Union[float, int, np.ndarray, pd.Series],
                   denominator: Union[float, int, np.ndarray, pd.Series],
                   default: float = 0.0) -> Union[float, np.ndarray, pd.Series]:
    """
    Safely calculate percentage: (numerator / denominator) * 100
    """
    return safe_divide(numerator, denominator, default) * 100


def safe_ratio(numerator: Union[float, int, np.ndarray, pd.Series],
               denominator: Union[float, int, np.ndarray, pd.Series],
               default: float = 1.0) -> Union[float, np.ndarray, pd.Series]:
    """
    Safely calculate ratio, with default of 1.0 for ratios
    """
    return safe_divide(numerator, denominator, default)


def safe_normalize(values: Union[np.ndarray, pd.Series],
                  total: Union[float, np.ndarray, pd.Series],
                  default: float = 0.0) -> Union[np.ndarray, pd.Series]:
    """
    Safely normalize values by their sum or a total
    """
    return safe_divide(values, total, default)

# ============================================
# LOGGING CONFIGURATION
# ============================================

# Configure production-ready logging with a clear format.
log_level = logging.INFO
logging.basicConfig(
    level=log_level,
    format='%(asctime)s | %(name)s | %(levelname)s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# ============================================
# CONFIGURATION AND CONSTANTS
# ============================================

@dataclass(frozen=True)
class Config:
    """System configuration with validated weights and thresholds"""
    
    # Data source - Default configuration
    DEFAULT_SHEET_URL: str = ""
    DEFAULT_GID: str = "1823439984"
    
    # Cache settings - Dynamic refresh
    CACHE_TTL: int = 3600  
    STALE_DATA_HOURS: int = 24

    # Master Score 3.0 weights (total = 100%)
    POSITION_WEIGHT: float = 0.27
    VOLUME_WEIGHT: float = 0.23
    MOMENTUM_WEIGHT: float = 0.22
    ACCELERATION_WEIGHT: float = 0.00
    BREAKOUT_WEIGHT: float = 0.18
    RVOL_WEIGHT: float = 0.10
    
    # Display settings
    DEFAULT_TOP_N: int = 50
    AVAILABLE_TOP_N: List[int] = field(default_factory=lambda: [10, 20, 50, 100, 200, 500])
    
    # Critical columns (app fails without these)
    CRITICAL_COLUMNS: List[str] = field(default_factory=lambda: ['ticker', 'price', 'volume_1d'])
    
    # Important columns (degraded experience without) - ENHANCED with all return periods
    IMPORTANT_COLUMNS: List[str] = field(default_factory=lambda: [
        'category', 'sector', 'industry',
        'rvol', 'pe', 'eps_current', 'eps_change_pct',
        'sma_20d', 'sma_50d', 'sma_200d',
        'ret_1d', 'ret_3d', 'ret_7d', 'ret_30d', 'ret_3m', 'ret_6m', 'ret_1y', 'ret_3y', 'ret_5y',
        'from_low_pct', 'from_high_pct',
        'vol_ratio_1d_90d', 'vol_ratio_7d_90d', 'vol_ratio_30d_90d',
        'vol_ratio_1d_180d', 'vol_ratio_7d_180d', 'vol_ratio_30d_180d',
        'vol_ratio_90d_180d',
        'volume_30d', 'volume_90d', 'volume_180d',
        'daily_turnover', 'turnover_30d', 'turnover_90d', 'turnover_180d',
        'growth_30_to_90', 'growth_90_to_180', 'growth_momentum', 'growth_consistency',
        'growth_acceleration', 'growth_alignment', 'smart_money_flow', 'composite_growth_score',
        'growth_quality_tier', 'growth_trend'
    ])
    
    # All percentage columns for consistent handling
    PERCENTAGE_COLUMNS: List[str] = field(default_factory=lambda: [
        'from_low_pct', 'from_high_pct',
        'ret_1d', 'ret_3d', 'ret_7d', 'ret_30d', 
        'ret_3m', 'ret_6m', 'ret_1y', 'ret_3y', 'ret_5y',
        'eps_change_pct'
    ])
    
    # Volume ratio columns
    VOLUME_RATIO_COLUMNS: List[str] = field(default_factory=lambda: [
        'vol_ratio_1d_90d', 'vol_ratio_7d_90d', 'vol_ratio_30d_90d',
        'vol_ratio_1d_180d', 'vol_ratio_7d_180d', 'vol_ratio_30d_180d',
        'vol_ratio_90d_180d'
    ])
    
    # Pattern thresholds
    PATTERN_THRESHOLDS: Dict[str, float] = field(default_factory=lambda: {
        "category_leader": 80,        # Reduced from 90 - more realistic for category leaders
        "hidden_gem": 75,
        "acceleration": 80,
        "institutional": 70,
        "vol_explosion": 90,          # Keep high - explosive volume should be rare
        "market_leader": 85,          # Reduced from 95 - top 10% is more realistic
        "momentum_wave": 70,
        "liquid_leader": 80,
        "long_strength": 80,
        "52w_high_approach": 85,      # Reduced from 90 - more opportunities near highs
        "52w_low_bounce": 85,
        "golden_zone": 85,
        "vol_accumulation": 80,
        "momentum_diverge": 85,       # Reduced from 90 - divergences are valuable signals
        "range_compress": 75,
        "stealth": 70,
        "perfect_storm": 80,
        "bull_trap": 85,              # Reduced from 90 - more trap detection
        "capitulation": 90,           # Reduced from 95 - still extreme but more realistic
        "runaway_gap": 85,
        "rotation_leader": 80,
        "distribution_top": 85,
        "velocity_squeeze": 85,
        "volume_divergence": 85,      # Reduced from 90 - important early warning signal
        "golden_cross": 75,           # Reduced from 80 - classic technical pattern
        "exhaustion": 85,             # Reduced from 90 - exhaustion patterns are valuable
        "pyramid": 75,
        "vacuum": 85,
    })
    
    # Market State Filtering Configuration
    MARKET_STATE_FILTERS: Dict[str, Dict[str, Any]] = field(default_factory=lambda: {
        'MOMENTUM': {
            'allowed_states': ['STRONG_UPTREND', 'UPTREND', 'PULLBACK'],
            'description': 'Focus on stocks in uptrends and pullbacks - optimal for swing trading'
        },
        'AGGRESSIVE': {
            'allowed_states': ['STRONG_UPTREND'],
            'description': 'Only the strongest uptrending stocks - highest risk/reward'
        },
        'VALUE': {
            'allowed_states': ['PULLBACK', 'BOUNCE', 'SIDEWAYS'],
            'description': 'Stocks in correction or consolidation - value opportunities'
        },
        'DEFENSIVE': {
            'allowed_states': ['STRONG_UPTREND', 'UPTREND', 'PULLBACK', 'SIDEWAYS', 'BOUNCE'],
            'description': 'Avoid strong downtrends - conservative risk management'
        },
        'ALL': {
            'allowed_states': ['STRONG_UPTREND', 'UPTREND', 'PULLBACK', 'ROTATION', 
                             'SIDEWAYS', 'DOWNTREND', 'STRONG_DOWNTREND', 'BOUNCE'],
            'description': 'No filtering - all market states included'
        }
    })
    
    # Default filter for swing/momentum trading - Changed to ALL for better user experience
    DEFAULT_MARKET_FILTER: str = 'ALL'
    ENABLE_MARKET_STATE_FILTER: bool = False  # Disabled by default to prevent aggressive filtering
    
    # Value bounds for data validation
    VALUE_BOUNDS: Dict[str, Tuple[float, float]] = field(default_factory=lambda: {
        'price': (0.01, 1_000_000),
        'rvol': (0.01, 1_000_000.0),
        'pe': (-10000, 10000),
        'returns': (-99.99, 9999.99),
        'volume': (0, 1e12)
    })
    
    # Performance thresholds
    PERFORMANCE_TARGETS: Dict[str, float] = field(default_factory=lambda: {
        'data_processing': 2.0,
        'filtering': 0.2,
        'pattern_detection': 0.5,
        'export_generation': 1.0,
        'search': 0.05
    })
    
    # Market categories (Indian market specific)
    MARKET_CATEGORIES: List[str] = field(default_factory=lambda: [
        'Mega Cap', 'Large Cap', 'Mid Cap', 'Small Cap', 'Micro Cap'
    ])
    
    # Tier definitions with proper boundaries
    TIERS: Dict[str, Dict[str, Tuple[float, float]]] = field(default_factory=lambda: {
        "eps": {
            "Loss": (-float('inf'), 0),
            "0-5": (0, 5),
            "5-10": (5, 10),
            "10-20": (10, 20),
            "20-50": (20, 50),
            "50-100": (50, 100),
            "100+": (100, float('inf'))
        },
        "pe": {
            "Negative/NA": (-float('inf'), 0),
            "0-10": (0, 10),
            "10-15": (10, 15),
            "15-20": (15, 20),
            "20-30": (20, 30),
            "30-50": (30, 50),
            "50+": (50, float('inf'))
        },
        "price": {
            "0-100": (0, 100),
            "100-250": (100, 250),
            "250-500": (250, 500),
            "500-1000": (500, 1000),
            "1000-2500": (1000, 2500),
            "2500-5000": (2500, 5000),
            "5000+": (5000, float('inf'))
        },
        "eps_change_pct": {
            "Heavy Loss (< -50%)": (-float('inf'), -50),
            "Loss (-50% to -10%)": (-50, -10),
            "Slight Decline (-10% to 0%)": (-10, 0),
            "Low Growth (0% to 20%)": (0, 20),
            "Good Growth (20% to 50%)": (20, 50),
            "High Growth (50% to 100%)": (50, 100),
            "Explosive Growth (> 100%)": (100, float('inf'))
        },
        "position_tiers": {
            "💎 Near Lows (0-20%)": (0, 20),
            "🏗️ Lower Range (20-40%)": (20, 40),
            "🏞️ Middle Range (40-60%)": (40, 60),
            "⛰️ Upper Range (60-80%)": (60, 80),
            "🏔️ Near Highs (80-100%)": (80, 100)
        },
        "performance_tiers": {
            # Short-term momentum (Practical thresholds for Indian markets)
            "🚀 Strong Gainers (>3% 1D)": ("ret_1d", 3),
            "⚡ Power Moves (>7% 1D)": ("ret_1d", 7),
            "💥 Explosive (>15% 1D)": ("ret_1d", 15),
            "🌟 3-Day Surge (>6% 3D)": ("ret_3d", 6),
            "📈 Weekly Winners (>12% 7D)": ("ret_7d", 12),
            
            # Medium-term growth (More realistic thresholds)
            "🏆 Monthly Champions (>25% 30D)": ("ret_30d", 25),
            "🎯 Quarterly Stars (>25% 3M)": ("ret_3m", 25),
            "💎 Half-Year Heroes (>35% 6M)": ("ret_6m", 35),
            
            # Long-term performance (More practical thresholds)
            "🌙 Annual Winners (>50% 1Y)": ("ret_1y", 50),
            "👑 Multi-Year Champions (>100% 3Y)": ("ret_3y", 100),
            "🏛️ Long-Term Legends (>150% 5Y)": ("ret_5y", 150)
        },
        "volume_tiers": {
            "📈 Growing Interest (RVOL >1.5x)": ("rvol", 1.5),
            "🔥 High Activity (RVOL >2x)": ("rvol", 2.0),
            "💥 Explosive Volume (RVOL >5x)": ("rvol", 5.0),
            "🌋 Volcanic Volume (RVOL >10x)": ("rvol", 10.0),
            "😴 Low Activity (RVOL <0.5x)": ("rvol", 0.5, "below")
        },
        "vmi_tiers": {
            "🌙 Hibernating (VMI <0.3)": ("vmi", 0.3, "below"),
            "😴 Sleepy (VMI 0.3-0.6)": ("vmi", 0.3, 0.6),
            "🚶 Walking (VMI 0.6-1.0)": ("vmi", 0.6, 1.0),
            "🏃 Running (VMI 1.0-1.5)": ("vmi", 1.0, 1.5),
            "🚀 Flying (VMI 1.5-2.5)": ("vmi", 1.5, 2.5),
            "🌋 Volcanic (VMI >2.5)": ("vmi", 2.5)
        },
        "momentum_harmony_tiers": {
            "💔 Broken (Score 0)": ("momentum_harmony", 0, 0),
            "🌧️ Conflicted (Score 1)": ("momentum_harmony", 1, 1),
            "⛅ Mixed (Score 2)": ("momentum_harmony", 2, 2),
            "🌤️ Aligned (Score 3)": ("momentum_harmony", 3, 3),
            "☀️ Perfect Harmony (Score 4)": ("momentum_harmony", 4, 4)
        },
        "daily_turnover_tiers": {
            "🏜️ Very Low (<1L)": (0, 100_000),                    # < ₹1 Lakh
            "💧 Low (1L-10L)": (100_000, 1_000_000),               # ₹1 Lakh to ₹10 Lakh
            "💦 Moderate (10L-1Cr)": (1_000_000, 10_000_000),      # ₹10 Lakh to ₹1 Crore
            "🌊 High (1Cr-10Cr)": (10_000_000, 100_000_000),       # ₹1 Crore to ₹10 Crore
            "🌊🌊 Very High (>10Cr)": (100_000_000, float('inf'))  # > ₹10 Crore
        }
    })
    
    # Metric Tooltips for better UX
    METRIC_TOOLTIPS: Dict[str, str] = field(default_factory=lambda: {
        'vmi': 'Volume Momentum Index: Weighted volume trend score (higher = stronger volume momentum)',
        'position_tension': 'Range position stress: Distance from 52W low + distance from 52W high',
        'momentum_harmony': 'Multi-timeframe alignment: 0-4 score showing consistency across periods',
        'overall_market_strength': 'Composite market score: Combined momentum, acceleration, RVOL & breakout',
        'market_state': 'Market momentum regime: STRONG_UPTREND, UPTREND, PULLBACK, SIDEWAYS, DOWNTREND, etc.',
        'money_flow_mm': 'Money Flow in millions: Price × Volume × RVOL / 1M',
        'master_score': 'Overall ranking score (0-100) combining all factors',
        'acceleration_score': 'Rate of momentum change (0-100)',
        'breakout_score': 'Probability of price breakout (0-100)',
        'trend_quality': 'SMA alignment quality (0-100)',
        'liquidity_score': 'Trading liquidity measure (0-100)',
        'from_high_pct': 'Distance from 52-week high: 0% = at high, negative values = below high',
        'from_low_pct': 'Distance from 52-week low: 0% = at low, positive values = above low',
        'vmi_tier': 'Volume Momentum Index tier: Weighted volume trend classification',
        'momentum_harmony_tier': 'Multi-timeframe momentum alignment tier: 0-4 consistency score'
    })

# Global configuration instance
CONFIG = Config()

# ============================================
# PERFORMANCE MONITORING
# ============================================

class PerformanceMonitor:
    """Track and report performance metrics"""
    
    @staticmethod
    def timer(target_time: Optional[float] = None):
        """Performance timing decorator with target comparison"""
        def decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                start = time.perf_counter()
                try:
                    result = func(*args, **kwargs)
                    elapsed = time.perf_counter() - start
                    
                    # Log if exceeds target
                    if target_time and elapsed > target_time:
                        logger.warning(f"{func.__name__} took {elapsed:.2f}s (target: {target_time}s)")
                    elif elapsed > 1.0:
                        logger.info(f"{func.__name__} completed in {elapsed:.2f}s")
                    
                    # Store timing
                    if 'performance_metrics' not in st.session_state:
                        st.session_state.performance_metrics = {}
                    st.session_state.performance_metrics[func.__name__] = elapsed
                    
                    return result
                except Exception as e:
                    elapsed = time.perf_counter() - start
                    logger.error(f"{func.__name__} failed after {elapsed:.2f}s: {str(e)}")
                    raise
            return wrapper
        return decorator

# ============================================
# DATA VALIDATION AND SANITIZATION
# ============================================

class DataValidator:
    """
    Comprehensive data validation and sanitization.
    This class ensures data integrity, handles missing or invalid values gracefully,
    and reports on all correction actions taken.
    """
    
    @staticmethod
    def validate_dataframe(df: pd.DataFrame, required_cols: List[str], context: str) -> Tuple[bool, str]:
        """
        Validates the structure and basic quality of a DataFrame.

        Args:
            df (pd.DataFrame): The DataFrame to validate.
            required_cols (List[str]): A list of columns that must be present.
            context (str): A descriptive string for logging and error messages.

        Returns:
            Tuple[bool, str]: A boolean indicating validity and a message.
        """
        if df is None:
            return False, f"{context}: DataFrame is None"
        
        if df.empty:
            return False, f"{context}: DataFrame is empty"
        
        # Check for critical columns defined in CONFIG
        missing_critical = [col for col in CONFIG.CRITICAL_COLUMNS if col not in df.columns]
        if missing_critical:
            return False, f"{context}: Missing critical columns: {missing_critical}"
        
        # Check for duplicate tickers
        duplicates = df['ticker'].duplicated().sum()
        if duplicates > 0:
            logger.warning(f"{context}: Found {duplicates} duplicate tickers")
        
        # Calculate data completeness
        total_cells = len(df) * len(df.columns)
        filled_cells = df.notna().sum().sum()
        completeness = safe_percentage(filled_cells, total_cells, default=0.0)
        
        if completeness < 50:
            logger.warning(f"{context}: Low data completeness ({completeness:.1f}%)")
        
        # Update session state with data quality metrics
        if 'data_quality' not in st.session_state:
            st.session_state.data_quality = {}
        
        st.session_state.data_quality.update({
            'completeness': completeness,
            'total_rows': len(df),
            'total_columns': len(df.columns),
            'duplicate_tickers': duplicates,
            'context': context,
            'timestamp': datetime.now(timezone.utc)
        })
        
        logger.info(f"{context}: Validated {len(df)} rows, {len(df.columns)} columns, {completeness:.1f}% complete")
        return True, "Valid"

    @staticmethod
    def clean_numeric_value(value: Any, is_percentage: bool = False, bounds: Optional[Tuple[float, float]] = None) -> Optional[float]:
        """
        Cleans, converts, and validates a single numeric value.
        
        Args:
            value (Any): The value to clean.
            is_percentage (bool): Flag to handle percentage symbols.
            bounds (Optional[Tuple[float, float]]): A tuple (min, max) to clip the value.
            
        Returns:
            Optional[float]: The cleaned float value, or np.nan if invalid.
        """
        # ENHANCED INPUT VALIDATION
        if pd.isna(value) or value == '' or value is None:
            return np.nan
        
        try:
            # Convert to string for cleaning
            cleaned = str(value).strip()
            
            # COMPREHENSIVE invalid string detection
            invalid_strings = ['', '-', 'N/A', 'NA', 'NAN', 'NONE', 'NULL', 'NIL', 
                             '#VALUE!', '#ERROR!', '#DIV/0!', '#N/A', '#REF!', '#NAME?',
                             'INF', '-INF', 'INFINITY', '-INFINITY', '∞', '-∞']
            if cleaned.upper() in invalid_strings:
                return np.nan
            
            # Remove symbols and spaces - ENHANCED CLEANING
            cleaned = cleaned.replace('₹', '').replace('$', '').replace(',', '').replace(' ', '').replace('%', '')
            
            # Convert to float
            result = float(cleaned)
            
            # Apply bounds if specified
            if bounds:
                min_val, max_val = bounds
                if result < min_val or result > max_val:
                    logger.debug(f"Value {result} outside bounds [{min_val}, {max_val}]")
                    result = np.clip(result, min_val, max_val)
            
            # Check for unreasonable values
            if np.isnan(result) or np.isinf(result):
                return np.nan
            
            return result
            
        except (ValueError, TypeError, AttributeError):
            return np.nan
    
    @staticmethod
    def sanitize_string(value: Any, default: str = "Unknown") -> str:
        """
        Cleans and sanitizes a string value, returning a default if invalid.
        
        Args:
            value (Any): The value to sanitize.
            default (str): The default value to return if invalid.
            
        Returns:
            str: The sanitized string.
        """
        if pd.isna(value) or value is None:
            return default
        
        cleaned = str(value).strip()
        if cleaned.upper() in ['', 'N/A', 'NA', 'NAN', 'NONE', 'NULL', '-']:
            return default
        
        # Remove excessive whitespace
        cleaned = ' '.join(cleaned.split())
        
        return cleaned
    
    @staticmethod
    def validate_numeric_columns(df: pd.DataFrame, columns: List[str]) -> Dict[str, int]:
        """
        Validates numeric columns and returns a count of invalid values per column.
        
        Args:
            df (pd.DataFrame): The DataFrame to validate.
            columns (List[str]): List of columns to validate.
            
        Returns:
            Dict[str, int]: Dictionary mapping column names to invalid value counts.
        """
        invalid_counts = {}
        
        for col in columns:
            if col in df.columns:
                # Count non-numeric values
                try:
                    pd.to_numeric(df[col], errors='coerce')
                    invalid_count = df[col].apply(
                        lambda x: not isinstance(x, (int, float, np.number)) and pd.notna(x)
                    ).sum()
                    
                    if invalid_count > 0:
                        invalid_counts[col] = invalid_count
                        logger.warning(f"Column '{col}' has {invalid_count} non-numeric values")
                except Exception as e:
                    logger.error(f"Error validating column '{col}': {str(e)}")
        
        return invalid_counts
        
# ============================================
# SMART CACHING WITH VERSIONING
# ============================================

def extract_spreadsheet_id(url_or_id: str) -> tuple:
    """
    Handles Google Sheets URLs, Google Drive URLs, or just IDs.
    For Google Drive URLs, returns the full URL to avoid unnecessary processing.

    Args:
        url_or_id (str): A Google Sheets URL, Google Drive URL, or just the ID.

    Returns:
        tuple: (id_or_url, source_type) where source_type is 'sheets', 'drive', or 'unknown'
    """
    if not url_or_id:
        return "", "unknown"
    
    # If it's a full Google Drive URL, return it as-is
    if "drive.google.com/file/d/" in url_or_id:
        return url_or_id.strip(), "drive"
    
    # If it's already just an ID (no slashes), assume Google Sheets
    if '/' not in url_or_id:
        return url_or_id.strip(), "sheets"
    
    # Check for Google Sheets URL pattern
    sheets_pattern = r'/spreadsheets/d/([a-zA-Z0-9-_]+)'
    sheets_match = re.search(sheets_pattern, url_or_id)
    if sheets_match:
        return sheets_match.group(1), "sheets"
    
    # Check for Google Drive URL pattern - extract ID only if it's not a full URL
    drive_pattern = r'/file/d/([a-zA-Z0-9-_]+)'
    drive_match = re.search(drive_pattern, url_or_id)
    if drive_match:
        return drive_match.group(1), "drive"
    
    # Check for generic /d/ pattern (could be either)
    generic_pattern = r'/d/([a-zA-Z0-9-_]+)'
    generic_match = re.search(generic_pattern, url_or_id)
    if generic_match:
        # Try to determine type from domain
        if "drive.google.com" in url_or_id:
            return generic_match.group(1), "drive"
        else:
            return generic_match.group(1), "sheets"
    
    # If no match, return as is with unknown type
    return url_or_id.strip(), "unknown"

@st.cache_data(ttl=3600, show_spinner=False)  # 1 hour TTL to prevent stale data
def load_and_process_data(source_type: str = "sheet", file_data=None, 
                         sheet_id: str = None, gid: str = None,
                         data_version: str = "1.0", url_source_type: str = "sheets") -> Tuple[pd.DataFrame, datetime, Dict[str, Any]]:
    """
    Loads and processes data from a Google Sheet, Google Drive CSV, or uploaded CSV file with caching and versioning.

    Args:
        source_type (str): Specifies the data source, either "sheet" or "upload".
        file_data (Optional): The uploaded CSV file object if `source_type` is "upload".
        sheet_id (str): The Google Spreadsheet ID or Google Drive file ID.
        gid (str): The Google Sheet tab ID (only used for Google Sheets).
        data_version (str): A unique key to bust the cache (e.g., hash of date + sheet ID).
        url_source_type (str): The type of URL source - "sheets", "drive", or "unknown".
        data_version (str): A unique key to bust the cache (e.g., hash of date + sheet ID).

    Returns:
        Tuple[pd.DataFrame, datetime, Dict[str, Any]]: A tuple containing the processed DataFrame,
        the processing timestamp, and metadata about the process.
    
    Raises:
        ValueError: If a valid Google Sheets ID is not provided.
        Exception: If data loading or processing fails.
    """
    
    start_time = time.perf_counter()
    metadata = {
        'source_type': source_type,
        'data_version': data_version,
        'processing_start': datetime.now(timezone.utc),
        'errors': [],
        'warnings': []
    }
    
    try:
        # Load data based on source
        if source_type == "upload" and file_data is not None:
            logger.info("Loading data from uploaded CSV")
            try:
                df = pd.read_csv(file_data, low_memory=False)
                metadata['source'] = "User Upload"
            except UnicodeDecodeError:
                for encoding in ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']:
                    try:
                        file_data.seek(0)
                        df = pd.read_csv(file_data, low_memory=False, encoding=encoding)
                        metadata['warnings'].append(f"Used {encoding} encoding")
                        break
                    except (UnicodeDecodeError, pd.errors.EmptyDataError, pd.errors.ParserError) as e:
                        logger.warning(f"Failed to decode with {encoding}: {e}")
                        continue
                else:
                    raise ValueError("Unable to decode CSV file")
        else:
            # Use defaults if not provided
            if not sheet_id:
                raise ValueError("Please enter a Google Sheets ID or Google Drive file ID")
            if not gid:
                gid = CONFIG.DEFAULT_GID
            
            # Construct CSV URL based on source type
            if url_source_type == "drive":
                # If sheet_id is already a full URL, convert it to download URL
                if sheet_id.startswith("https://drive.google.com/file/d/"):
                    # Extract ID from full URL for download format
                    drive_pattern = r'/file/d/([a-zA-Z0-9-_]+)'
                    drive_match = re.search(drive_pattern, sheet_id)
                    if drive_match:
                        file_id = drive_match.group(1)
                        csv_url = f"https://drive.usercontent.google.com/download?id={file_id}&export=download&authuser=0"
                    else:
                        raise ValueError("Invalid Google Drive URL format")
                else:
                    # sheet_id is just the file ID
                    csv_url = f"https://drive.usercontent.google.com/download?id={sheet_id}&export=download&authuser=0"
                logger.info(f"🔍 DETECTED: Google Drive file | URL: {csv_url}")
                metadata['source'] = "Google Drive"
                metadata['detected_type'] = "drive"
            else:
                # Default to Google Sheets format
                csv_url = f"https://docs.google.com/spreadsheets/d/{sheet_id}/export?format=csv&gid={gid}"
                logger.info(f"🔍 DETECTED: Google Sheets | ID: {sheet_id} | GID: {gid}")
                metadata['source'] = "Google Sheets"
                metadata['detected_type'] = "sheets"
            
            logger.info(f"📂 Constructed URL: {csv_url}")
            
            try:
                df = pd.read_csv(csv_url, low_memory=False)
            except Exception as e:
                error_msg = f"Failed to load from {metadata['source']}: {str(e)}"
                logger.error(error_msg)
                metadata['errors'].append(f"Load error: {str(e)}")
                
                # Try to use cached data as fallback
                if 'last_good_data' in st.session_state:
                    logger.info("Using cached data as fallback")
                    df, timestamp, old_metadata = st.session_state.last_good_data
                    metadata['warnings'].append("Using cached data due to load failure")
                    metadata['cache_used'] = True
                    return df, timestamp, metadata
                raise
        
        # Validate loaded data
        is_valid, validation_msg = DataValidator.validate_dataframe(df, CONFIG.CRITICAL_COLUMNS, "Initial load")
        if not is_valid:
            raise ValueError(validation_msg)
        
        # Process the data
        df = DataProcessor.process_dataframe(df, metadata)
        
        # Calculate all scores and rankings
        df = RankingEngine.calculate_all_scores(df)
        
        # Corrected method call here
        df = PatternDetector.detect_all_patterns_optimized(df)
        
        # Add advanced metrics
        df = AdvancedMetrics.calculate_all_metrics(df)
        
        # Final validation
        is_valid, validation_msg = DataValidator.validate_dataframe(df, ['master_score', 'rank'], "Final processed")
        if not is_valid:
            raise ValueError(validation_msg)
        
        # Store as last good data - MEMORY OPTIMIZED
        # Only store essential columns to reduce memory footprint
        timestamp = datetime.now(timezone.utc)
        essential_cols = ['ticker', 'company_name', 'price', 'master_score', 'market_state'] + \
                        [col for col in df.columns if col.endswith(('_score', '_pct', 'ret_'))]
        essential_cols = [col for col in essential_cols if col in df.columns]
        st.session_state.last_good_data = (df[essential_cols].copy() if len(essential_cols) > 0 else df.copy(), 
                                         timestamp, metadata)
        
        # Record processing time
        processing_time = time.perf_counter() - start_time
        metadata['processing_time'] = processing_time
        metadata['processing_end'] = datetime.now(timezone.utc)
        
        logger.info(f"Data processing complete: {len(df)} stocks in {processing_time:.2f}s")
        
        # Periodic cleanup
        if 'last_cleanup' not in st.session_state:
            st.session_state.last_cleanup = datetime.now(timezone.utc)
        
        if (datetime.now(timezone.utc) - st.session_state.last_cleanup).total_seconds() > 300:
            gc.collect()
            st.session_state.last_cleanup = datetime.now(timezone.utc)
        
        return df, timestamp, metadata
        
    except Exception as e:
        logger.error(f"Failed to load and process data: {str(e)}")
        metadata['errors'].append(str(e))
        raise
        
# ============================================
# DATA PROCESSING ENGINE
# ============================================

class DataProcessor:
    """
    Handles the entire data processing pipeline, from raw data ingestion to a clean,
    ready-for-analysis DataFrame. This class is optimized for performance and robustness.
    """
    
    @staticmethod
    @PerformanceMonitor.timer(target_time=1.0)
    def process_dataframe(df: pd.DataFrame, metadata: Dict[str, Any]) -> pd.DataFrame:
        """
        Main pipeline to validate, clean, and prepare the raw DataFrame.

        Args:
            df (pd.DataFrame): The raw DataFrame to be processed.
            metadata (Dict[str, Any]): A dictionary to log warnings and changes.

        Returns:
            pd.DataFrame: A clean, processed DataFrame ready for scoring.
        """
        df = df.copy()
        initial_count = len(df)
        
        # 1. Process numeric columns with intelligent cleaning
        numeric_cols = [col for col in df.columns if col not in ['ticker', 'company_name', 'category', 'sector', 'industry', 'year', 'market_cap']]
        
        for col in numeric_cols:
            if col in df.columns:
                is_pct = col in CONFIG.PERCENTAGE_COLUMNS
                
                # Dynamically determine bounds based on column name
                bounds = None
                if 'volume' in col.lower():
                    bounds = CONFIG.VALUE_BOUNDS['volume']
                elif col == 'rvol':
                    bounds = CONFIG.VALUE_BOUNDS['rvol']
                elif col == 'pe':
                    bounds = CONFIG.VALUE_BOUNDS['pe']
                elif is_pct:
                    bounds = CONFIG.VALUE_BOUNDS['returns']
                else:
                    bounds = CONFIG.VALUE_BOUNDS.get('price', None)
                
                # Apply vectorized cleaning
                df[col] = df[col].apply(lambda x: DataValidator.clean_numeric_value(x, is_pct, bounds))
        
        # 2. Process categorical columns with robust sanitization
        string_cols = ['ticker', 'company_name', 'category', 'sector', 'industry']
        for col in string_cols:
            if col in df.columns:
                df[col] = df[col].apply(DataValidator.sanitize_string)
        
        # 3. Handle volume ratios with safety
        for col in CONFIG.VOLUME_RATIO_COLUMNS:
            if col in df.columns:
                df[col] = (100 + df[col]) / 100
                df[col] = df[col].clip(0.01, 1000.0)
                df[col] = df[col].fillna(1.0)
        
        # 4. Critical data validation and removal of duplicates
        df = df.dropna(subset=['ticker', 'price'], how='any')
        df = df[df['price'] > 0.01]
        
        before_dedup = len(df)
        df = df.drop_duplicates(subset=['ticker'], keep='first')
        if before_dedup > len(df):
            metadata['warnings'].append(f"Removed {before_dedup - len(df)} duplicate tickers")
        
        # 5. Fill missing values and add tier classifications
        df = DataProcessor._fill_missing_values(df)
        df = DataProcessor._add_tier_classifications(df)
        
        # 6. Log final data quality metrics
        removed_count = initial_count - len(df)
        if removed_count > 0:
            metadata['warnings'].append(f"Removed {removed_count} invalid rows during processing.")
        
        logger.info(f"Processed {len(df)} valid stocks from {initial_count} initial rows.")
        
        return df

    @staticmethod
    def _fill_missing_values(df: pd.DataFrame) -> pd.DataFrame:
        """
        Fills missing values in key columns with sensible defaults.
        This is a final defensive step to ensure downstream calculations don't fail due to NaNs.
        """
        # Default for position metrics
        if 'from_low_pct' in df.columns:
            df['from_low_pct'] = df['from_low_pct'].fillna(0)
        
        if 'from_high_pct' in df.columns:
            # FIXED: Calculate proper from_high_pct - positive when above 52w high, negative when below
            if all(col in df.columns for col in ['price', 'high_52w']):
                # Recalculate from_high_pct properly: (current_price / 52w_high - 1) * 100
                df['from_high_pct'] = ((df['price'] / df['high_52w']) - 1) * 100
                # Fill any remaining NaN values (division by zero cases) with 0
                df['from_high_pct'] = df['from_high_pct'].fillna(0)
            else:
                # Fallback if no price/high_52w data
                df['from_high_pct'] = df['from_high_pct'].fillna(0)
        
        # Default for Relative Volume (RVOL)
        if 'rvol' in df.columns:
            df['rvol'] = df['rvol'].fillna(1.0)
        
        # Defaults for price returns
        return_cols = [col for col in df.columns if col.startswith('ret_')]
        for col in return_cols:
            if col in df.columns:
                df[col] = df[col].fillna(0)
        
        # Defaults for volume columns
        volume_cols = [col for col in df.columns if col.startswith('volume_')]
        for col in volume_cols:
            if col in df.columns:
                df[col] = df[col].fillna(0)
        
        # Defaults for categorical columns
        for col in ['category', 'sector', 'industry']:
            if col not in df.columns:
                df[col] = 'Unknown'
            else:
                df[col] = df[col].fillna('Unknown')
        
        return df
    
    @staticmethod
    def _add_tier_classifications(df: pd.DataFrame) -> pd.DataFrame:
        """
        Applies a classification tier to numerical data (e.g., price, PE)
        based on predefined ranges in the `Config` class.
        This is a bug-fixed and robust version of the logic from earlier files.
        """
        def classify_tier(value: float, tier_dict: Dict[str, Tuple[float, float]]) -> str:
            """Helper function to map a value to its tier."""
            if pd.isna(value):
                return "Unknown"
            
            for tier_name, (min_val, max_val) in tier_dict.items():
                if min_val < value <= max_val:
                    return tier_name
                if min_val == -float('inf') and value <= max_val:
                    return tier_name
                if max_val == float('inf') and value > min_val:
                    return tier_name
            
            return "Unknown"
        
        if 'eps_current' in df.columns:
            df['eps_tier'] = df['eps_current'].apply(lambda x: classify_tier(x, CONFIG.TIERS['eps']))
        
        if 'pe' in df.columns:
            df['pe_tier'] = df['pe'].apply(
                lambda x: "Negative/NA" if pd.isna(x) or x <= 0 else classify_tier(x, CONFIG.TIERS['pe'])
            )
        
        if 'price' in df.columns:
            df['price_tier'] = df['price'].apply(lambda x: classify_tier(x, CONFIG.TIERS['price']))
        
        if 'eps_change_pct' in df.columns:
            df['eps_change_tier'] = df['eps_change_pct'].apply(
                lambda x: "Unknown" if pd.isna(x) else classify_tier(x, CONFIG.TIERS['eps_change_pct'])
            )
        
        # Position tier classification (based on from_low_pct or calculate from price data)
        if 'from_low_pct' in df.columns:
            # Use existing from_low_pct column (already 0-100%)
            df['position_pct'] = df['from_low_pct'].apply(
                lambda x: min(100, max(0, x)) if pd.notna(x) else None
            )
            df['position_tier'] = df['position_pct'].apply(
                lambda x: "Unknown" if pd.isna(x) else classify_tier(x, CONFIG.TIERS['position_tiers'])
            )
            logger.info(f"Position tiers created from from_low_pct. Sample tiers: {df['position_tier'].value_counts().head()}")
        
        # Daily turnover calculation and tier classification
        if all(col in df.columns for col in ['volume_1d', 'price']):
            # Calculate daily turnover in rupees
            df['daily_turnover'] = df['volume_1d'] * df['price']
            df['daily_turnover_tier'] = df['daily_turnover'].apply(
                lambda x: "Unknown" if pd.isna(x) else classify_tier(x, CONFIG.TIERS['daily_turnover_tiers'])
            )
            logger.info(f"Daily turnover tiers created. Sample tiers: {df['daily_turnover_tier'].value_counts().head()}")
        
        # Enhanced turnover calculations (30-day, 90-day, 180-day) with consistent pricing methodology
        # NOTE: volume_30d, volume_90d, volume_180d are AVERAGE DAILY volumes over their respective periods
        # PROFESSIONAL METHODOLOGY: Use current price for all periods to ensure consistency and accuracy
        # turnover_30d = Average Daily Turnover (calculated using 30-day avg volume × current price)
        # This represents the typical daily turnover over the 30-day period using current market valuation
        if all(col in df.columns for col in ['volume_30d', 'price']):
            df['turnover_30d'] = df['volume_30d'] * df['price']
            df['turnover_30d_tier'] = df['turnover_30d'].apply(
                lambda x: "Unknown" if pd.isna(x) else classify_tier(x, CONFIG.TIERS['daily_turnover_tiers'])
            )
            logger.info(f"30-day avg daily turnover (volume_30d × current price) calculated. Sample values: {df['turnover_30d'].describe()}")
        
        # turnover_90d = Average Daily Turnover (calculated using 90-day avg volume × current price)
        if all(col in df.columns for col in ['volume_90d', 'price']):
            df['turnover_90d'] = df['volume_90d'] * df['price']
            df['turnover_90d_tier'] = df['turnover_90d'].apply(
                lambda x: "Unknown" if pd.isna(x) else classify_tier(x, CONFIG.TIERS['daily_turnover_tiers'])
            )
            logger.info(f"90-day avg daily turnover (volume_90d × current price) calculated. Sample values: {df['turnover_90d'].describe()}")
        
        # turnover_180d = Average Daily Turnover (calculated using 180-day avg volume × current price)
        if all(col in df.columns for col in ['volume_180d', 'price']):
            df['turnover_180d'] = df['volume_180d'] * df['price']
            df['turnover_180d_tier'] = df['turnover_180d'].apply(
                lambda x: "Unknown" if pd.isna(x) else classify_tier(x, CONFIG.TIERS['daily_turnover_tiers'])
            )
            logger.info(f"180-day avg daily turnover (volume_180d × current price) calculated. Sample values: {df['turnover_180d'].describe()}")
        
        # 🌊 LIQUIDITY GROWTH ANALYTICS - Advanced Multi-Period Analysis
        # Calculate growth rates, momentum, consistency, acceleration, and alignment
        # NOTE: All turnover values represent AVERAGE DAILY turnover for their periods
        # - turnover_30d = avg daily turnover over 30 days
        # - turnover_90d = avg daily turnover over 90 days
        # - turnover_180d = avg daily turnover over 180 days
        # Comparing these shows whether recent avg daily activity is higher/lower than historical
        
        # 1. Liquidity Growth Expansion (Daily → 30d → 90d → 180d)
        
        # Growth from Daily to 30d - How much higher/lower is today's turnover vs 30d avg daily
        if all(col in df.columns for col in ['volume_1d', 'price', 'turnover_30d']):
            df['daily_turnover'] = df['volume_1d'] * df['price']
            df['growth_daily_to_30'] = df.apply(
                lambda row: ((row['daily_turnover'] - row['turnover_30d']) / row['turnover_30d'] * 100) 
                if pd.notna(row['daily_turnover']) and pd.notna(row['turnover_30d']) and row['turnover_30d'] > 0 
                else 0.0,
                axis=1
            )
            logger.info(f"Liquidity growth Daily→30d calculated. Range: [{df['growth_daily_to_30'].min():.1f}% to {df['growth_daily_to_30'].max():.1f}%]")
        
        if all(col in df.columns for col in ['turnover_30d', 'turnover_90d', 'turnover_180d']):
            # MOMENTUM METRICS: Recent Average vs Historical Average
            # Comparing average daily turnovers across periods
            # Positive = Recent 30d avg daily > Historical 90d avg daily (ACCELERATING)
            # Negative = Recent 30d avg daily < Historical 90d avg daily (DECELERATING)
            
            # 🚀 VECTORIZED Helper function (10-50x faster than apply())
            def calculate_momentum_pct_vectorized(recent, historical, min_change_pct=3):
                """
                Vectorized momentum calculation with statistical significance filter
                
                Performance: 10-50x faster than apply() method
                Quality: Filters noise by requiring minimum 3% change
                
                Args:
                    recent: pd.Series of recent period turnovers
                    historical: pd.Series of historical period turnovers
                    min_change_pct: Minimum % change to consider significant (default: 3%)
                
                Returns:
                    pd.Series of momentum percentages (capped at ±1000%)
                """
                # Initialize result series with zeros
                result = pd.Series(0.0, index=recent.index, dtype=np.float64)
                
                # Quality mask: both valid + historical > ₹1L + historical > 0
                valid_mask = (
                    recent.notna() & 
                    historical.notna() & 
                    (historical > 0) & 
                    (historical >= 100000)  # Minimum ₹1 lakh turnover threshold
                )
                
                # Vectorized calculation (process all valid rows at once)
                if valid_mask.any():
                    raw_momentum = ((recent[valid_mask] - historical[valid_mask]) / historical[valid_mask] * 100)
                    
                    # Statistical significance filter: only count changes >= min_change_pct
                    # This filters random daily variance and focuses on real trends
                    significant_change_mask = (raw_momentum.abs() >= min_change_pct)
                    
                    # Apply only statistically significant changes
                    result.loc[valid_mask & pd.Series(False, index=result.index).mask(valid_mask, significant_change_mask)] = \
                        raw_momentum[significant_change_mask]
                    
                    # Cap extreme values at ±1000% (vectorized)
                    result = result.clip(-1000, 1000).round(2)
                
                return result
            
            # 🚀 Vectorized momentum calculations (10-50x faster + noise filtering)
            # Momentum: 30d vs 90d - How much is recent 30d higher/lower than 90d average
            df['growth_30_to_90'] = calculate_momentum_pct_vectorized(
                df['turnover_30d'], 
                df['turnover_90d'],
                min_change_pct=3  # Filter changes < 3% as noise
            )
            
            # Momentum: 90d vs 180d - How much is recent 90d higher/lower than 180d average
            df['growth_90_to_180'] = calculate_momentum_pct_vectorized(
                df['turnover_90d'], 
                df['turnover_180d'],
                min_change_pct=3  # Filter changes < 3% as noise
            )
            
            logger.info(f"Liquidity momentum calculated (vectorized + noise-filtered). 30d vs 90d range: [{df['growth_30_to_90'].min():.1f}% to {df['growth_30_to_90'].max():.1f}%], 90d vs 180d range: [{df['growth_90_to_180'].min():.1f}% to {df['growth_90_to_180'].max():.1f}%]")
        
        # 2. Growth Momentum (acceleration detection)
        if all(col in df.columns for col in ['growth_30_to_90', 'growth_90_to_180']):
            # Momentum Acceleration = How much MORE is 30d accelerating vs 90d
            # Positive = 30d momentum > 90d momentum (ACCELERATING FASTER)
            # Example: 30d is +100% vs 90d, but 90d is only +50% vs 180d → momentum = +50% (accelerating!)
            df['growth_momentum'] = df['growth_30_to_90'] - df['growth_90_to_180']
            
            logger.info(f"Growth momentum acceleration calculated. Range: [{df['growth_momentum'].min():.1f}% to {df['growth_momentum'].max():.1f}%]")
        
        # 3. Growth Consistency Score (0-100)
        if all(col in df.columns for col in ['growth_30_to_90', 'growth_90_to_180']):
            def calculate_consistency(row):
                g1 = row['growth_30_to_90']   # 30d vs 90d momentum
                g2 = row['growth_90_to_180']  # 90d vs 180d momentum
                
                # Both positive = consistent acceleration (GOOD)
                if g1 > 0 and g2 > 0:
                    # Higher score for consistent strong momentum
                    if g1 > 50 and g2 > 50:
                        return 100  # Both accelerating strongly
                    elif g1 > 25 and g2 > 25:
                        return 85   # Both accelerating well
                    elif g1 > 10 and g2 > 10:
                        return 70   # Both accelerating moderately
                    else:
                        # Score based on similarity (100 = identical rates)
                        diff = abs(g1 - g2)
                        avg = (g1 + g2) / 2
                        if avg > 0:
                            score = max(0, 100 - (diff / avg * 50))
                            return min(100, score)
                        return 60
                # Both negative = consistent deceleration (BAD but consistent)
                elif g1 < 0 and g2 < 0:
                    return 25  # Consistent but decelerating
                # Mixed = inconsistent (needs investigation)
                else:
                    return 40  # Divergent signals
            
            df['growth_consistency'] = df.apply(calculate_consistency, axis=1)
            
            logger.info(f"Growth consistency calculated. Range: [0 to {df['growth_consistency'].max():.0f}]")
        
        # 4. Growth Acceleration Score (0-100)
        if 'growth_momentum' in df.columns:
            # Acceleration categories based on momentum acceleration
            # Positive momentum = recent periods accelerating MORE than older periods
            def calculate_acceleration(momentum):
                if pd.isna(momentum):
                    return 0.0
                elif momentum > 100:
                    return 100.0  # Explosive acceleration (30d growing much faster than 90d)
                elif momentum > 50:
                    return 90.0   # Very strong acceleration
                elif momentum > 25:
                    return 75.0   # Strong acceleration
                elif momentum > 10:
                    return 60.0   # Moderate acceleration
                elif momentum > 0:
                    return 50.0   # Mild acceleration
                elif momentum > -10:
                    return 40.0   # Stable
                elif momentum > -25:
                    return 25.0   # Mild deceleration
                elif momentum > -50:
                    return 10.0   # Strong deceleration
                else:
                    return 0.0    # Severe deceleration
            
            df['growth_acceleration'] = df['growth_momentum'].apply(calculate_acceleration)
            
            logger.info(f"Growth acceleration calculated. Distribution: {df['growth_acceleration'].value_counts().to_dict()}")
        
        # 5. Growth Alignment Score (0-100) - All periods accelerating together
        if all(col in df.columns for col in ['growth_30_to_90', 'growth_90_to_180']):
            def calculate_alignment(row):
                g1 = row['growth_30_to_90']   # 30d vs 90d momentum
                g2 = row['growth_90_to_180']  # 90d vs 180d momentum
                
                # Perfect alignment = both accelerating strongly (recent > historical)
                if g1 > 100 and g2 > 100:
                    return 100  # Explosive synchronized acceleration
                elif g1 > 50 and g2 > 50:
                    return 95   # Very strong synchronized acceleration
                elif g1 > 30 and g2 > 30:
                    return 85   # Strong positive alignment
                elif g1 > 15 and g2 > 15:
                    return 75   # Good positive alignment
                elif g1 > 0 and g2 > 0:
                    return 60   # Mild positive alignment (both accelerating)
                elif g1 < -30 and g2 < -30:
                    return 20   # Strong negative alignment (both decelerating)
                elif g1 < 0 and g2 < 0:
                    return 35   # Mild negative alignment
                else:
                    return 45   # Divergent (mixed signals - needs investigation)
            
            df['growth_alignment'] = df.apply(calculate_alignment, axis=1)
            
            logger.info(f"Growth alignment calculated. Range: [{df['growth_alignment'].min():.0f} to {df['growth_alignment'].max():.0f}]")
        
        # 6. Smart Money Flow Detection (Institutional Accumulation/Distribution)
        if all(col in df.columns for col in ['growth_momentum', 'growth_consistency', 'growth_alignment']):
            def detect_smart_money(row):
                momentum = row['growth_momentum']        # Acceleration difference
                consistency = row['growth_consistency']  # Pattern reliability
                alignment = row['growth_alignment']      # Period synchronization
                g1 = row.get('growth_30_to_90', 0)       # Recent momentum
                
                # Elite accumulation - All signals strong + explosive recent activity
                if g1 > 100 and momentum > 50 and consistency > 80 and alignment >= 85:
                    return "🌊💎 Elite Institutional Accumulation"
                # Strong accumulation - Consistent strong acceleration
                elif momentum > 30 and consistency > 70 and alignment >= 75:
                    return "🎯 Strong Accumulation"
                # Moderate accumulation - Good acceleration with consistency
                elif momentum > 15 and consistency > 50 and alignment >= 60:
                    return "✅ Moderate Accumulation"
                # Early accumulation - Positive momentum starting
                elif momentum > 0 and consistency > 40 and g1 > 0:
                    return "📊 Mild Interest"
                # Distribution - Negative momentum with consistency
                elif momentum < -30 and consistency > 40:
                    return "❌ Distribution"
                # Weak distribution - Declining momentum
                elif momentum < -10:
                    return "⚠️ Weak Distribution"
                else:
                    return "⚪ Neutral"
            
            df['smart_money_flow'] = df.apply(detect_smart_money, axis=1)
            
            logger.info(f"Smart money flow detected. Distribution: {df['smart_money_flow'].value_counts().to_dict()}")
        
        # 7. Composite Growth Score (0-100) - Overall liquidity health
        if all(col in df.columns for col in ['growth_consistency', 'growth_acceleration', 'growth_alignment']):
            df['composite_growth_score'] = (
                df['growth_consistency'] * 0.4 +
                df['growth_acceleration'] * 0.3 +
                df['growth_alignment'] * 0.3
            ).round(1)
            
            logger.info(f"Composite growth score calculated. Range: [{df['composite_growth_score'].min():.1f} to {df['composite_growth_score'].max():.1f}]")
        
        # 7.1. 🎯 Percentile Rankings (Market-Relative Context)
        if 'composite_growth_score' in df.columns:
            # Calculate percentile ranks (0-100) showing relative market position
            df['composite_growth_percentile'] = (df['composite_growth_score'].rank(pct=True, method='average') * 100).round(1)
            
            # Percentile tier classification
            def classify_percentile(pct):
                """Classify percentile rank into meaningful tiers"""
                if pd.isna(pct):
                    return "Unknown"
                elif pct >= 95:
                    return "🌊💎 Top 5%"
                elif pct >= 90:
                    return "🚀 Top 10%"
                elif pct >= 75:
                    return "✅ Top 25%"
                elif pct >= 50:
                    return "➡️ Above Median"
                else:
                    return "⚪ Below Median"
            
            df['composite_percentile_tier'] = df['composite_growth_percentile'].apply(classify_percentile)
            
            # Also calculate percentile for momentum (useful for sorting)
            if 'growth_momentum' in df.columns:
                df['momentum_percentile'] = (df['growth_momentum'].rank(pct=True, method='average') * 100).round(1)
            
            logger.info(f"Percentile ranks calculated. Composite percentile range: [{df['composite_growth_percentile'].min():.0f}% to {df['composite_growth_percentile'].max():.0f}%]")
        
        # 7.2. 🎯 Risk-Adjusted Composite Score (Professional Risk Awareness)
        if all(col in df.columns for col in ['composite_growth_score', 'growth_30_to_90', 'growth_90_to_180']):
            def calculate_risk_adjusted_composite(row):
                """
                Risk-adjusted composite score penalizing unstable growth patterns
                
                Penalizes:
                - Recent growth >> Historical growth (unstable explosive growth)
                - Recent positive but historical negative (reversal risk)
                
                Rewards:
                - Sustained multi-period growth with similar rates
                """
                base_composite = row['composite_growth_score']
                
                # Return 0 if base score is invalid
                if pd.isna(base_composite):
                    return 0.0
                
                g1 = row.get('growth_30_to_90', 0)
                g2 = row.get('growth_90_to_180', 0)
                
                # Calculate instability penalty
                instability_penalty = 0.0
                
                # Case 1: Both positive - check for extreme ratio (instability)
                if g2 > 0 and g1 > 0:
                    ratio = g1 / g2
                    
                    # Recent growth 3x+ higher than historical = unstable
                    if ratio > 3.0:
                        instability_penalty = min(0.15, (ratio - 3.0) * 0.03)  # Max -15%
                    # Recent growth < 50% of historical = deceleration
                    elif ratio < 0.5:
                        instability_penalty = min(0.10, (0.5 - ratio) * 0.2)  # Max -10%
                
                # Case 2: Recent positive but historical negative = reversal (high risk)
                elif g1 > 0 and g2 < 0:
                    instability_penalty = 0.20  # High risk penalty (-20%)
                
                # Apply risk adjustment
                risk_adjusted = base_composite * (1 - instability_penalty)
                
                return round(risk_adjusted, 1)
            
            df['risk_adjusted_composite'] = df.apply(calculate_risk_adjusted_composite, axis=1)
            
            # Risk-adjusted tier classification
            def classify_growth_quality_risk_adjusted(score):
                """Classify risk-adjusted composite score into tiers"""
                if pd.isna(score):
                    return "Unknown"
                elif score >= 80:
                    return "💎 Elite"
                elif score >= 65:
                    return "🌟 Excellent"
                elif score >= 50:
                    return "✅ Good"
                elif score >= 35:
                    return "⚪ Average"
                else:
                    return "⚠️ Weak"
            
            df['risk_adjusted_tier'] = df['risk_adjusted_composite'].apply(classify_growth_quality_risk_adjusted)
            
            logger.info(f"Risk-adjusted composite calculated. Range: [{df['risk_adjusted_composite'].min():.1f} to {df['risk_adjusted_composite'].max():.1f}]")
        
        # 8. Growth Quality Tiers
        if 'composite_growth_score' in df.columns:
            def classify_growth_quality(score):
                if pd.isna(score):
                    return "Unknown"
                elif score >= 80:
                    return "💎 Elite"
                elif score >= 65:
                    return "🌟 Excellent"
                elif score >= 50:
                    return "✅ Good"
                elif score >= 35:
                    return "⚪ Average"
                else:
                    return "⚠️ Weak"
            
            df['growth_quality_tier'] = df['composite_growth_score'].apply(classify_growth_quality)
            
            logger.info(f"Growth quality tiers assigned. Distribution: {df['growth_quality_tier'].value_counts().to_dict()}")
        
        # 8.1. 🏆 ADVANCED VOLUME QUALITY SCORE (VQS) - Multi-Dimensional Intelligence
        # Advanced VQS = Base_Score × Quality_Multiplier × Market_Adjustment × Risk_Factor × Confidence
        # Uses 30+ data columns for institutional-grade volume quality assessment
        if all(col in df.columns for col in ['volume_1d', 'ret_1d', 'rvol']):
            
            # ===== HELPER FUNCTIONS FOR ADVANCED VQS =====
            
            def calculate_short_term_volume_quality(row):
                """Short-term volume quality (1d): Liquidity + Activity + Consistency"""
                # Absolute Liquidity (50%)
                vol_1d = row.get('volume_1d', 0)
                if pd.isna(vol_1d) or vol_1d <= 0:
                    liquidity_score = 0
                elif vol_1d >= 10_000_000:
                    liquidity_score = 100
                elif vol_1d >= 5_000_000:
                    liquidity_score = 90
                elif vol_1d >= 1_000_000:
                    liquidity_score = 80
                elif vol_1d >= 500_000:
                    liquidity_score = 70
                elif vol_1d >= 100_000:
                    liquidity_score = 60
                else:
                    liquidity_score = 40
                
                # Relative Activity (30%)
                rvol = row.get('rvol', 1.0)
                if pd.isna(rvol):
                    activity_score = 50
                elif rvol >= 3.0:
                    activity_score = 100
                elif rvol >= 2.0:
                    activity_score = 90
                elif rvol >= 1.5:
                    activity_score = 80
                elif rvol >= 1.0:
                    activity_score = 70
                elif rvol >= 0.5:
                    activity_score = 50
                else:
                    activity_score = 30
                
                # Short-term Consistency (20%)
                vol_7d = row.get('volume_7d', vol_1d)
                if pd.notna(vol_1d) and pd.notna(vol_7d) and vol_7d > 0:
                    vol_ratio = vol_1d / vol_7d
                    deviation = abs(1.0 - vol_ratio)
                    if deviation < 0.2:
                        consistency_score = 100
                    elif deviation < 0.4:
                        consistency_score = 85
                    elif deviation < 0.6:
                        consistency_score = 70
                    elif deviation < 1.0:
                        consistency_score = 50
                    else:
                        consistency_score = 30
                else:
                    consistency_score = 50
                
                return liquidity_score * 0.50 + activity_score * 0.30 + consistency_score * 0.20
            
            def calculate_medium_term_volume_quality(row):
                """Medium-term volume quality (7-30d): Trend + Consistency + Harmony"""
                # Volume Trend (40%)
                vol_7d = row.get('volume_7d', 0)
                vol_30d = row.get('volume_30d', 0)
                if pd.notna(vol_7d) and pd.notna(vol_30d) and vol_30d > 0:
                    trend_ratio = vol_7d / vol_30d
                    if trend_ratio >= 1.5:
                        trend_score = 100
                    elif trend_ratio >= 1.2:
                        trend_score = 90
                    elif trend_ratio >= 1.0:
                        trend_score = 80
                    elif trend_ratio >= 0.8:
                        trend_score = 60
                    elif trend_ratio >= 0.6:
                        trend_score = 40
                    else:
                        trend_score = 20
                else:
                    trend_score = 50
                
                # Multi-timeframe Consistency (35%)
                vol_1d = row.get('volume_1d', 0)
                if all(pd.notna(v) and v > 0 for v in [vol_1d, vol_7d, vol_30d]):
                    volumes = np.array([vol_1d, vol_7d, vol_30d])
                    cv = np.std(volumes) / np.mean(volumes)
                    if cv < 0.3:
                        mtf_consistency_score = 100
                    elif cv < 0.6:
                        mtf_consistency_score = 85
                    elif cv < 1.0:
                        mtf_consistency_score = 65
                    elif cv < 1.5:
                        mtf_consistency_score = 45
                    else:
                        mtf_consistency_score = 25
                else:
                    mtf_consistency_score = 50
                
                # Momentum Harmony (25%)
                momentum_harmony = row.get('momentum_harmony', 2)
                if pd.notna(momentum_harmony):
                    harmony_map = {4: 100, 3: 85, 2: 65, 1: 40, 0: 20}
                    harmony_score = harmony_map.get(int(momentum_harmony), 65)
                else:
                    harmony_score = 65
                
                return trend_score * 0.40 + mtf_consistency_score * 0.35 + harmony_score * 0.25
            
            def calculate_efficiency_quality(row):
                """Efficiency quality: Current + Consistency + VMI alignment"""
                # Current Efficiency (50%)
                ret_1d = row.get('ret_1d', 0)
                rvol = row.get('rvol', 1.0)
                if pd.notna(ret_1d) and pd.notna(rvol) and rvol > 0:
                    ver_1d = abs(ret_1d) / rvol
                    if ver_1d > 5.0:
                        current_eff_score = 100
                    elif ver_1d > 2.5:
                        current_eff_score = 90
                    elif ver_1d >= 1.5:
                        current_eff_score = 80
                    elif ver_1d >= 1.0:
                        current_eff_score = 70
                    elif ver_1d >= 0.5:
                        current_eff_score = 55
                    elif ver_1d >= 0.2:
                        current_eff_score = 35
                    else:
                        current_eff_score = 20
                else:
                    current_eff_score = 50
                
                # Multi-timeframe VER Consistency (30%)
                ret_7d = row.get('ret_7d', 0)
                vol_ratio_7d = row.get('vol_ratio_7d_90d', 1.0)
                ret_30d = row.get('ret_30d', 0)
                vol_ratio_30d = row.get('vol_ratio_30d_90d', 1.0)
                
                vers = []
                if pd.notna(ret_1d) and pd.notna(rvol) and rvol > 0:
                    vers.append(abs(ret_1d) / rvol)
                if pd.notna(ret_7d) and pd.notna(vol_ratio_7d) and vol_ratio_7d > 0:
                    vers.append(abs(ret_7d) / vol_ratio_7d)
                if pd.notna(ret_30d) and pd.notna(vol_ratio_30d) and vol_ratio_30d > 0:
                    vers.append(abs(ret_30d) / vol_ratio_30d)
                
                if len(vers) >= 2:
                    ver_cv = np.std(vers) / np.mean(vers) if np.mean(vers) > 0 else 0
                    if ver_cv < 0.3:
                        ver_consistency_score = 100
                    elif ver_cv < 0.6:
                        ver_consistency_score = 85
                    elif ver_cv < 1.0:
                        ver_consistency_score = 65
                    else:
                        ver_consistency_score = 40
                else:
                    ver_consistency_score = 50
                
                # Efficiency-VMI Alignment (20%)
                vmi = row.get('vmi', 50)
                if pd.notna(vmi):
                    vmi_normalized = vmi / 100.0
                    ver_normalized = min(ver_1d / 5.0, 1.0) if ver_1d > 0 else 0
                    alignment = 1.0 - abs(vmi_normalized - ver_normalized)
                    alignment_score = alignment * 100
                else:
                    alignment_score = 50
                
                return current_eff_score * 0.50 + ver_consistency_score * 0.30 + alignment_score * 0.20
            
            def calculate_base_score(row):
                """Calculate multi-timeframe base VQS score (0-100)"""
                short_term = calculate_short_term_volume_quality(row)
                medium_term = calculate_medium_term_volume_quality(row)
                efficiency = calculate_efficiency_quality(row)
                return short_term * 0.40 + medium_term * 0.35 + efficiency * 0.25
            
            def calculate_smart_money_component(row):
                """Smart money detection: Signal + Money flow + Pattern"""
                # Smart Money Flow Signal (50%)
                smart_money_flow = row.get('smart_money_flow', 'NONE')
                if smart_money_flow == 'ACCUMULATION':
                    sm_signal_score = 100
                elif smart_money_flow == 'ABSORPTION':
                    sm_signal_score = 90
                elif smart_money_flow == 'DISTRIBUTION':
                    sm_signal_score = 40
                else:
                    sm_signal_score = 50
                
                # Money Flow Quality (30%)
                money_flow_mm = row.get('money_flow_mm', 0)
                if pd.isna(money_flow_mm):
                    mf_quality_score = 50
                elif money_flow_mm >= 1000:
                    mf_quality_score = 100
                elif money_flow_mm >= 500:
                    mf_quality_score = 90
                elif money_flow_mm >= 100:
                    mf_quality_score = 80
                elif money_flow_mm >= 10:
                    mf_quality_score = 70
                elif money_flow_mm >= 1:
                    mf_quality_score = 60
                else:
                    mf_quality_score = 40
                
                # Volume Pattern (20%)
                rvol = row.get('rvol', 1.0)
                ret_1d = row.get('ret_1d', 0)
                if pd.notna(rvol) and pd.notna(ret_1d):
                    if rvol >= 2.0 and abs(ret_1d) < 3.0:
                        pattern_score = 100
                    elif rvol >= 1.5 and ret_1d > 0:
                        pattern_score = 85
                    elif rvol < 0.7:
                        pattern_score = 30
                    else:
                        pattern_score = 60
                else:
                    pattern_score = 50
                
                return sm_signal_score * 0.50 + mf_quality_score * 0.30 + pattern_score * 0.20
            
            def calculate_volume_sustainability(row):
                """Volume sustainability: Trend strength + VMI + Liquidity depth"""
                # Volume Trend Strength (50%)
                vol_1d = row.get('volume_1d', 0)
                vol_7d = row.get('volume_7d', 0)
                vol_30d = row.get('volume_30d', 0)
                if all(pd.notna(v) and v > 0 for v in [vol_1d, vol_7d, vol_30d]):
                    trend_1d_7d = vol_1d / vol_7d if vol_7d > 0 else 1.0
                    trend_7d_30d = vol_7d / vol_30d if vol_30d > 0 else 1.0
                    if trend_1d_7d >= 1.2 and trend_7d_30d >= 1.1:
                        trend_strength_score = 100
                    elif trend_1d_7d >= 1.1 and trend_7d_30d >= 1.0:
                        trend_strength_score = 85
                    elif trend_1d_7d >= 0.9 and trend_7d_30d >= 0.9:
                        trend_strength_score = 70
                    elif trend_1d_7d < 0.8 or trend_7d_30d < 0.8:
                        trend_strength_score = 30
                    else:
                        trend_strength_score = 50
                else:
                    trend_strength_score = 50
                
                # VMI Quality (30%)
                vmi = row.get('vmi', 50)
                if pd.notna(vmi):
                    if vmi >= 70:
                        vmi_score = 100
                    elif vmi >= 60:
                        vmi_score = 85
                    elif vmi >= 50:
                        vmi_score = 70
                    elif vmi >= 40:
                        vmi_score = 50
                    else:
                        vmi_score = 30
                else:
                    vmi_score = 50
                
                # Liquidity Depth (20%)
                daily_turnover = row.get('daily_turnover', 0)
                if pd.isna(daily_turnover):
                    liquidity_depth_score = 50
                elif daily_turnover >= 10_000_000:
                    liquidity_depth_score = 100
                elif daily_turnover >= 1_000_000:
                    liquidity_depth_score = 80
                elif daily_turnover >= 100_000:
                    liquidity_depth_score = 60
                else:
                    liquidity_depth_score = 40
                
                return trend_strength_score * 0.50 + vmi_score * 0.30 + liquidity_depth_score * 0.20
            
            def calculate_trading_quality(row):
                """Trading quality: Price-volume correlation + Execution + Momentum"""
                # Price-Volume Correlation (50%)
                ret_1d = row.get('ret_1d', 0)
                rvol = row.get('rvol', 1.0)
                if pd.notna(ret_1d) and pd.notna(rvol):
                    if ret_1d > 0 and rvol > 1.2:
                        pv_corr_score = 100
                    elif ret_1d > 0 and rvol > 0.8:
                        pv_corr_score = 85
                    elif ret_1d < 0 and rvol < 0.8:
                        pv_corr_score = 70
                    elif ret_1d < 0 and rvol > 1.5:
                        pv_corr_score = 30
                    else:
                        pv_corr_score = 60
                else:
                    pv_corr_score = 50
                
                # Execution Quality (30%)
                volatility_7d = row.get('volatility_7d', 0)
                vol_7d = row.get('volume_7d', 0)
                if pd.notna(volatility_7d) and pd.notna(vol_7d):
                    if volatility_7d < 3.0 and vol_7d >= 1_000_000:
                        exec_quality_score = 100
                    elif volatility_7d < 7.0 and vol_7d >= 500_000:
                        exec_quality_score = 85
                    elif volatility_7d < 15.0:
                        exec_quality_score = 70
                    elif volatility_7d > 20.0:
                        exec_quality_score = 40
                    else:
                        exec_quality_score = 60
                else:
                    exec_quality_score = 50
                
                # Momentum Quality (20%)
                momentum_harmony = row.get('momentum_harmony', 2)
                if pd.notna(momentum_harmony):
                    harmony_map = {4: 100, 3: 85, 2: 65, 1: 45, 0: 25}
                    momentum_quality_score = harmony_map.get(int(momentum_harmony), 65)
                else:
                    momentum_quality_score = 65
                
                return pv_corr_score * 0.50 + exec_quality_score * 0.30 + momentum_quality_score * 0.20
            
            def calculate_quality_multiplier(row):
                """Calculate quality multiplier (0.5x - 1.5x)"""
                smart_money = calculate_smart_money_component(row)
                sustainability = calculate_volume_sustainability(row)
                trading_quality = calculate_trading_quality(row)
                composite_score = smart_money * 0.40 + sustainability * 0.35 + trading_quality * 0.25
                multiplier = 0.5 + (composite_score / 100.0) * 1.0
                return np.clip(multiplier, 0.5, 1.5)
            
            def calculate_market_state_impact(row):
                """Market state impact scoring"""
                market_state = row.get('market_state', 'SIDEWAYS')
                state_scores = {
                    'STRONG_UPTREND': 100, 'UPTREND': 90, 'RECOVERY': 85,
                    'PULLBACK': 70, 'SIDEWAYS': 50, 'DOWNTREND': 40, 'STRONG_DOWNTREND': 30
                }
                return state_scores.get(market_state, 50)
            
            def calculate_volatility_impact(row):
                """Volatility environment impact"""
                volatility_7d = row.get('volatility_7d', 0)
                if pd.isna(volatility_7d):
                    return 50
                if volatility_7d < 3.0:
                    return 100
                elif volatility_7d < 7.0:
                    return 80
                elif volatility_7d < 15.0:
                    return 60
                elif volatility_7d < 25.0:
                    return 40
                else:
                    return 20
            
            def calculate_market_adjustment(row):
                """Calculate market adjustment factor (0.7x - 1.3x)"""
                market_state_score = calculate_market_state_impact(row)
                volatility_score = calculate_volatility_impact(row)
                composite_score = market_state_score * 0.60 + volatility_score * 0.40
                adjustment = 0.7 + (composite_score / 100.0) * 0.6
                return np.clip(adjustment, 0.7, 1.3)
            
            def calculate_liquidity_risk(row):
                """Liquidity risk assessment (higher score = lower risk)"""
                # Absolute Liquidity (50%)
                volume_1d = row.get('volume_1d', 0)
                if pd.isna(volume_1d) or volume_1d <= 0:
                    abs_liq_score = 0
                elif volume_1d >= 10_000_000:
                    abs_liq_score = 100
                elif volume_1d >= 5_000_000:
                    abs_liq_score = 90
                elif volume_1d >= 1_000_000:
                    abs_liq_score = 80
                elif volume_1d >= 500_000:
                    abs_liq_score = 65
                elif volume_1d >= 100_000:
                    abs_liq_score = 50
                else:
                    abs_liq_score = 30
                
                # Turnover Depth (30%)
                daily_turnover = row.get('daily_turnover', 0)
                if pd.isna(daily_turnover):
                    turnover_score = 50
                elif daily_turnover >= 10_000_000:
                    turnover_score = 100
                elif daily_turnover >= 5_000_000:
                    turnover_score = 90
                elif daily_turnover >= 1_000_000:
                    turnover_score = 75
                elif daily_turnover >= 100_000:
                    turnover_score = 55
                else:
                    turnover_score = 30
                
                # Liquidity Stability (20%)
                vol_7d = row.get('volume_7d', volume_1d)
                vol_30d = row.get('volume_30d', volume_1d)
                if all(pd.notna(v) and v > 0 for v in [volume_1d, vol_7d, vol_30d]):
                    min_vol = min(volume_1d, vol_7d, vol_30d)
                    max_vol = max(volume_1d, vol_7d, vol_30d)
                    vol_range_ratio = min_vol / max_vol if max_vol > 0 else 0
                    if vol_range_ratio >= 0.7:
                        stability_score = 100
                    elif vol_range_ratio >= 0.5:
                        stability_score = 80
                    elif vol_range_ratio >= 0.3:
                        stability_score = 60
                    else:
                        stability_score = 40
                else:
                    stability_score = 50
                
                return abs_liq_score * 0.50 + turnover_score * 0.30 + stability_score * 0.20
            
            def calculate_money_flow_risk(row):
                """Money flow quality risk (higher score = lower risk)"""
                # Money Flow Magnitude (60%)
                money_flow_mm = row.get('money_flow_mm', 0)
                if pd.isna(money_flow_mm):
                    mf_mag_score = 50
                elif money_flow_mm >= 1000:
                    mf_mag_score = 100
                elif money_flow_mm >= 500:
                    mf_mag_score = 90
                elif money_flow_mm >= 100:
                    mf_mag_score = 80
                elif money_flow_mm >= 50:
                    mf_mag_score = 70
                elif money_flow_mm >= 10:
                    mf_mag_score = 60
                else:
                    mf_mag_score = 40
                
                # Smart Money Alignment (40%)
                smart_money_flow = row.get('smart_money_flow', 'NONE')
                if smart_money_flow == 'ACCUMULATION':
                    sm_align_score = 100
                elif smart_money_flow == 'ABSORPTION':
                    sm_align_score = 90
                elif smart_money_flow == 'NONE':
                    sm_align_score = 50
                elif smart_money_flow == 'DISTRIBUTION':
                    sm_align_score = 30
                else:
                    sm_align_score = 50
                
                return mf_mag_score * 0.60 + sm_align_score * 0.40
            
            def calculate_execution_risk(row):
                """Execution risk assessment (higher score = lower risk)"""
                # Volatility Risk (60%)
                volatility_7d = row.get('volatility_7d', 0)
                if pd.isna(volatility_7d):
                    vol_risk_score = 50
                elif volatility_7d < 3.0:
                    vol_risk_score = 100
                elif volatility_7d < 7.0:
                    vol_risk_score = 85
                elif volatility_7d < 15.0:
                    vol_risk_score = 60
                elif volatility_7d < 25.0:
                    vol_risk_score = 35
                else:
                    vol_risk_score = 20
                
                # Price Impact Risk (40%)
                rvol = row.get('rvol', 1.0)
                ret_1d = row.get('ret_1d', 0)
                if pd.notna(rvol) and pd.notna(ret_1d) and rvol > 0:
                    ver = abs(ret_1d) / rvol
                    if ver >= 2.0:
                        impact_risk_score = 100
                    elif ver >= 1.0:
                        impact_risk_score = 85
                    elif ver >= 0.5:
                        impact_risk_score = 65
                    elif ver >= 0.2:
                        impact_risk_score = 45
                    else:
                        impact_risk_score = 25
                else:
                    impact_risk_score = 50
                
                return vol_risk_score * 0.60 + impact_risk_score * 0.40
            
            def calculate_risk_factor(row):
                """Calculate risk factor (0.6x - 1.4x)"""
                liquidity_risk = calculate_liquidity_risk(row)
                money_flow_risk = calculate_money_flow_risk(row)
                execution_risk = calculate_execution_risk(row)
                composite_score = liquidity_risk * 0.40 + money_flow_risk * 0.35 + execution_risk * 0.25
                risk_factor = 0.6 + (composite_score / 100.0) * 0.8
                return np.clip(risk_factor, 0.6, 1.4)
            
            def calculate_confidence_score(row):
                """Calculate confidence based on data completeness (0.5 - 1.0)"""
                critical_columns = {
                    'volume_1d': 0.15, 'volume_7d': 0.10, 'volume_30d': 0.10, 'rvol': 0.05,
                    'ret_1d': 0.10, 'ret_7d': 0.08, 'ret_30d': 0.07,
                    'money_flow_mm': 0.07, 'smart_money_flow': 0.07, 'momentum_harmony': 0.06,
                    'market_state': 0.05, 'volatility_7d': 0.05, 'vmi': 0.03, 'daily_turnover': 0.02
                }
                completeness_score = sum(weight for col, weight in critical_columns.items() 
                                        if col in row.index and pd.notna(row[col]))
                confidence = 0.5 + (completeness_score * 0.5)
                return np.clip(confidence, 0.5, 1.0)
            
            def calculate_advanced_vqs(row):
                """
                Calculate Advanced VQS - ULTRA-SIMPLE 4-COMPONENT SCORING
                
                Formula: VolumeStrength (50%) + SmartMoney (20%) + Consistency (20%) + Efficiency (10%)
                - Volume Strength: Liquidity + Activity (size + surge)
                - Smart Money: WHO is buying (institutions = leading indicator)
                - Consistency: HOW reliable (pattern stability over time)
                - Efficiency: Price movement per volume unit
                """
                
                # ===== SCORE 1: VOLUME STRENGTH (0-100) - 50% weight =====
                vol_1d = row.get('volume_1d', 0)
                rvol = row.get('rvol', 1.0)
                
                # Liquidity component (0-50 points)
                if pd.isna(vol_1d) or vol_1d <= 0:
                    liquidity = 0
                elif vol_1d >= 10_000_000: liquidity = 50
                elif vol_1d >= 5_000_000: liquidity = 45
                elif vol_1d >= 1_000_000: liquidity = 40
                elif vol_1d >= 500_000: liquidity = 30
                elif vol_1d >= 100_000: liquidity = 20
                else: liquidity = 10
                
                # Activity component (0-50 points)
                if pd.isna(rvol):
                    activity = 25
                elif rvol >= 3.0: activity = 50
                elif rvol >= 2.0: activity = 45
                elif rvol >= 1.5: activity = 40
                elif rvol >= 1.0: activity = 30
                elif rvol >= 0.7: activity = 20
                else: activity = 10
                
                volume_strength = liquidity + activity  # Max 100
                
                # ===== SCORE 2: SMART MONEY (0-100) - 20% weight =====
                smart_money_flow = row.get('smart_money_flow', 'NONE')
                money_flow_mm = row.get('money_flow_mm', 0)
                
                # Smart money signal (0-60 points)
                if smart_money_flow == 'ACCUMULATION':
                    sm_signal = 60
                elif smart_money_flow == 'ABSORPTION':
                    sm_signal = 50
                elif smart_money_flow == 'DISTRIBUTION':
                    sm_signal = 20
                else:
                    sm_signal = 30
                
                # Money flow magnitude (0-40 points)
                if pd.isna(money_flow_mm):
                    mf_magnitude = 20
                elif money_flow_mm >= 500: mf_magnitude = 40
                elif money_flow_mm >= 100: mf_magnitude = 35
                elif money_flow_mm >= 50: mf_magnitude = 30
                elif money_flow_mm >= 10: mf_magnitude = 25
                elif money_flow_mm >= 1: mf_magnitude = 20
                else: mf_magnitude = 10
                
                smart_money = sm_signal + mf_magnitude  # Max 100
                
                # ===== SCORE 3: CONSISTENCY (0-100) - 20% weight =====
                # Measures volume pattern reliability across timeframes
                vol_7d = row.get('volume_7d', 0)
                vol_30d = row.get('volume_30d', 0)
                vol_90d = row.get('volume_90d', 0)
                
                if all(pd.notna(v) and v > 0 for v in [vol_1d, vol_7d, vol_30d]):
                    # Calculate coefficient of variation (lower = more consistent)
                    volumes = np.array([vol_1d, vol_7d, vol_30d])
                    mean_vol = np.mean(volumes)
                    std_vol = np.std(volumes)
                    cv = std_vol / mean_vol if mean_vol > 0 else 999
                    
                    # Score based on consistency (lower CV = higher score)
                    if cv < 0.2: consistency = 100      # Highly consistent
                    elif cv < 0.4: consistency = 85     # Very consistent
                    elif cv < 0.6: consistency = 70     # Good consistency
                    elif cv < 0.8: consistency = 55     # Moderate consistency
                    elif cv < 1.2: consistency = 40     # Low consistency
                    else: consistency = 25              # Poor consistency
                    
                    # Bonus: Upward trend in volume (1d > 7d > 30d)
                    if vol_1d > vol_7d > vol_30d:
                        consistency = min(100, consistency + 15)  # Building momentum!
                    # Penalty: Downward trend (1d < 7d < 30d)
                    elif vol_1d < vol_7d < vol_30d:
                        consistency = max(0, consistency - 15)    # Fading momentum
                else:
                    consistency = 50  # Default if data missing
                
                # ===== SCORE 4: EFFICIENCY (0-100) - 10% weight =====
                ret_1d = row.get('ret_1d', 0)
                
                if pd.notna(ret_1d) and pd.notna(rvol) and rvol > 0:
                    ver = abs(ret_1d) / rvol
                    if ver >= 3.0: efficiency = 100
                    elif ver >= 2.0: efficiency = 85
                    elif ver >= 1.5: efficiency = 75
                    elif ver >= 1.0: efficiency = 60
                    elif ver >= 0.5: efficiency = 40
                    else: efficiency = 20
                else:
                    efficiency = 50
                
                # ===== WEIGHTED COMPOSITE =====
                raw_vqs = (
                    volume_strength * 0.50 +  # 50% - Volume is king
                    smart_money * 0.20 +      # 20% - Follow the big players
                    consistency * 0.20 +      # 20% - Pattern reliability matters
                    efficiency * 0.10         # 10% - Price efficiency
                )
                
                # ===== DATA QUALITY PENALTY =====
                # Penalize if critical data is missing
                data_quality = 1.0
                if pd.isna(vol_1d) or vol_1d <= 0:
                    data_quality *= 0.7
                if pd.isna(rvol):
                    data_quality *= 0.9
                if pd.isna(ret_1d):
                    data_quality *= 0.95
                
                vqs_score = raw_vqs * data_quality
                vqs_score = np.clip(vqs_score, 0, 100)
                
                # ===== GRADING =====
                if vqs_score >= 80:
                    grade, status = "A+", "👑 Elite"
                elif vqs_score >= 70:
                    grade, status = "A", "🌟 Excellent"
                elif vqs_score >= 60:
                    grade, status = "B", "✅ Strong"
                elif vqs_score >= 45:
                    grade, status = "C", "⚪ Average"
                elif vqs_score >= 30:
                    grade, status = "D", "⚠️ Weak"
                else:
                    grade, status = "F", "❌ Poor"
                
                return pd.Series({
                    'vqs_score': round(vqs_score, 2),
                    'vqs_grade': grade,
                    'vqs_status': status,
                    'vqs_volume_strength': round(volume_strength, 2),
                    'vqs_smart_money': round(smart_money, 2),
                    'vqs_consistency': round(consistency, 2),
                    'vqs_efficiency': round(efficiency, 2),
                    'vqs_data_quality': round(data_quality, 3)
                })
            
            # ===== CALCULATE ADVANCED VQS FOR ALL STOCKS =====
            logger.info("Calculating Advanced VQS (4-Component: Volume + SmartMoney + Consistency + Efficiency)...")
            
            vqs_results = df.apply(calculate_advanced_vqs, axis=1)
            df['vqs_score'] = vqs_results['vqs_score']
            df['vqs_grade'] = vqs_results['vqs_grade']
            df['vqs_status'] = vqs_results['vqs_status']
            df['vqs_volume_strength'] = vqs_results['vqs_volume_strength']
            df['vqs_smart_money'] = vqs_results['vqs_smart_money']
            df['vqs_consistency'] = vqs_results['vqs_consistency']
            df['vqs_efficiency'] = vqs_results['vqs_efficiency']
            df['vqs_data_quality'] = vqs_results['vqs_data_quality']
            
            logger.info(f"Advanced VQS calculated. Grade distribution: {df['vqs_grade'].value_counts().to_dict()}")
            logger.info(f"Advanced VQS score range: [{df['vqs_score'].min():.2f} to {df['vqs_score'].max():.2f}]")
            logger.info(f"Advanced VQS average: {df['vqs_score'].mean():.2f}, median: {df['vqs_score'].median():.2f}")
            logger.info(f"Component averages - Volume: {df['vqs_volume_strength'].mean():.1f}, SmartMoney: {df['vqs_smart_money'].mean():.1f}, Consistency: {df['vqs_consistency'].mean():.1f}, Efficiency: {df['vqs_efficiency'].mean():.1f}")
        
        # 8.2. ⚡ ADVANCED VOLUME EFFICIENCY RATIO (Advanced VER) - Multi-Dimensional Intelligence
        # Advanced VER = Base_VER × Quality_Multiplier × Regime_Adjustment × Risk_Factor
        # Uses 30+ data columns for comprehensive volume efficiency assessment
        
        # Check required columns for Advanced VER calculation
        required_cols_advanced_ver = ['ret_1d', 'ret_7d', 'ret_30d', 'vol_ratio_1d_90d', 'vol_ratio_7d_90d', 'vol_ratio_30d_90d']
        if all(col in df.columns for col in required_cols_advanced_ver):
            
            # ===== COMPONENT 1: MULTI-TIMEFRAME BASE VER =====
            def calculate_multi_timeframe_ver(row):
                """
                Calculate VER across 3 timeframes with time-decay weighting.
                
                Formula: Base_VER = (ver_1d × 50%) + (ver_7d × 30%) + (ver_30d × 20%)
                
                Why multi-timeframe?
                - Single-day VER is noisy (gap ups, news events)
                - Weekly VER shows sustained efficiency
                - Monthly VER reveals structural quality
                
                Returns: float (0 to ~20+, typically 0-5)
                """
                # TIMEFRAME 1: Daily VER (50% weight - most recent)
                ret_1d = row.get('ret_1d', 0)
                vol_ratio_1d = row.get('vol_ratio_1d_90d', 1.0)
                
                if pd.notna(ret_1d) and pd.notna(vol_ratio_1d) and vol_ratio_1d > 0:
                    ver_1d = abs(ret_1d) / vol_ratio_1d
                else:
                    ver_1d = 0
                
                # TIMEFRAME 2: Weekly VER (30% weight - short-term trend)
                ret_7d = row.get('ret_7d', 0)
                vol_ratio_7d = row.get('vol_ratio_7d_90d', 1.0)
                
                if pd.notna(ret_7d) and pd.notna(vol_ratio_7d) and vol_ratio_7d > 0:
                    ver_7d = abs(ret_7d) / vol_ratio_7d
                else:
                    ver_7d = 0
                
                # TIMEFRAME 3: Monthly VER (20% weight - structural quality)
                ret_30d = row.get('ret_30d', 0)
                vol_ratio_30d = row.get('vol_ratio_30d_90d', 1.0)
                
                if pd.notna(ret_30d) and pd.notna(vol_ratio_30d) and vol_ratio_30d > 0:
                    ver_30d = abs(ret_30d) / vol_ratio_30d
                else:
                    ver_30d = 0
                
                # TIME-WEIGHTED AGGREGATION (Recent periods matter more)
                base_ver = (
                    ver_1d * 0.50 +   # Daily: 50%
                    ver_7d * 0.30 +   # Weekly: 30%
                    ver_30d * 0.20    # Monthly: 20%
                )
                
                return base_ver
            
            df['advanced_ver_base'] = df.apply(calculate_multi_timeframe_ver, axis=1)
            
            # ===== COMPONENT 2: QUALITY MULTIPLIER =====
            def calculate_quality_multiplier(row):
                """
                Adjust VER based on volume quality indicators.
                
                Formula: Quality = (Consistency × 40%) + (VMI × 30%) + (Momentum Harmony × 30%)
                Range: 0.5x to 1.5x
                
                Why quality multiplier?
                - Raw VER doesn't account for volume reliability
                - Consistent volume = more trustworthy efficiency
                - Volume momentum shows acceleration/deceleration
                - Momentum harmony confirms multi-timeframe alignment
                
                Returns: float (0.5 to 1.5)
                """
                # SUB-COMPONENT 1: Volume Consistency (40% of multiplier)
                vqs_consistency = row.get('vqs_consistency_score', 50)  # Default 50 (neutral)
                
                # Normalize to 0.8-1.2 range
                consistency_factor = 0.8 + (vqs_consistency / 100) * 0.4
                
                # SUB-COMPONENT 2: Volume Momentum Index (30% of multiplier)
                vmi = row.get('vmi', 1.0)  # Default 1.0 (neutral)
                
                if pd.notna(vmi):
                    if vmi > 2.0:
                        vmi_factor = 1.2  # Strong volume momentum (boost)
                    elif vmi > 1.5:
                        vmi_factor = 1.1
                    elif vmi > 0.7:
                        vmi_factor = 1.0  # Neutral
                    elif vmi > 0.5:
                        vmi_factor = 0.9
                    else:
                        vmi_factor = 0.8  # Weak volume momentum (penalty)
                else:
                    vmi_factor = 1.0
                
                # SUB-COMPONENT 3: Momentum Harmony (30% of multiplier)
                momentum_harmony = row.get('momentum_harmony', 2)  # Default 2 (mixed)
                
                if pd.notna(momentum_harmony):
                    # Perfect harmony (4) = 1.15x boost, Broken (0) = 0.85x penalty
                    harmony_factor = 0.85 + (momentum_harmony / 4) * 0.30
                else:
                    harmony_factor = 1.0
                
                # AGGREGATE QUALITY MULTIPLIER
                quality_multiplier = (
                    consistency_factor * 0.40 +
                    vmi_factor * 0.30 +
                    harmony_factor * 0.30
                )
                
                # Clip to reasonable range [0.5, 1.5]
                quality_multiplier = np.clip(quality_multiplier, 0.5, 1.5)
                
                return quality_multiplier
            
            df['advanced_ver_quality_mult'] = df.apply(calculate_quality_multiplier, axis=1)
            
            # ===== COMPONENT 3: REGIME ADJUSTMENT =====
            def calculate_regime_adjustment(row):
                """
                Adjust VER based on market regime context.
                
                Formula: Regime = (Market State × 60%) + (Volatility × 40%)
                Range: 0.7x to 1.3x
                
                Why regime adjustment?
                - Same VER means different things in different market states
                - VER=2.0 in STRONG_UPTREND is normal (expected)
                - VER=2.0 in SIDEWAYS is exceptional (rare quality)
                - High volatility increases risk → discount VER
                
                Returns: float (0.7 to 1.3)
                """
                # SUB-COMPONENT 1: Market State (60% of adjustment)
                market_state = row.get('market_state', 'SIDEWAYS')
                
                state_factors = {
                    'STRONG_UPTREND': 1.25,  # Strong trends = expect high efficiency
                    'UPTREND': 1.15,
                    'PULLBACK': 1.05,  # Pullbacks still in uptrend context
                    'SIDEWAYS': 1.00,  # Neutral
                    'ROTATION': 0.90,  # Uncertainty discount
                    'DOWNTREND': 0.85,  # Downtrends = lower efficiency acceptable
                    'STRONG_DOWNTREND': 0.80,
                    'BOUNCE': 0.95,  # Bounces are temporary
                    'UNKNOWN': 1.00
                }
                state_factor = state_factors.get(market_state, 1.00)
                
                # SUB-COMPONENT 2: Volatility Regime (40% of adjustment)
                # Calculate volatility from returns
                ret_1d = row.get('ret_1d', 0)
                ret_7d = row.get('ret_7d', 0)
                ret_30d = row.get('ret_30d', 0)
                
                # Estimate volatility as std of daily-equivalent returns
                daily_equivalent = []
                if pd.notna(ret_1d):
                    daily_equivalent.append(ret_1d)
                if pd.notna(ret_7d) and ret_7d != 0:
                    daily_equivalent.append(ret_7d / 7)
                if pd.notna(ret_30d) and ret_30d != 0:
                    daily_equivalent.append(ret_30d / 30)
                
                if len(daily_equivalent) >= 2:
                    volatility = np.std(daily_equivalent)
                else:
                    volatility = 5  # Default NORMAL volatility
                
                # Map volatility to factor
                if volatility > 15:  # HIGH
                    vol_factor = 0.85
                elif volatility > 7:  # ELEVATED
                    vol_factor = 0.92
                elif volatility < 3:  # LOW
                    vol_factor = 1.10
                else:  # NORMAL
                    vol_factor = 1.00
                
                # AGGREGATE REGIME ADJUSTMENT
                regime_adjustment = (
                    state_factor * 0.60 +
                    vol_factor * 0.40
                )
                
                # Clip to reasonable range [0.7, 1.3]
                regime_adjustment = np.clip(regime_adjustment, 0.7, 1.3)
                
                return regime_adjustment
            
            df['advanced_ver_regime_adj'] = df.apply(calculate_regime_adjustment, axis=1)
            
            # ===== COMPONENT 4: RISK FACTOR =====
            def calculate_risk_factor(row):
                """
                Adjust VER based on risk indicators.
                
                Formula: Risk = (Money Flow × 35%) + (Smart Money × 30%) + (Liquidity × 20%) + (Size × 15%)
                Range: 0.6x to 1.4x
                
                Why risk factor?
                - VER alone doesn't show institutional confidence
                - High money flow = institutional validation = lower risk
                - Smart money accumulation = bullish signal
                - Deep liquidity = lower execution risk
                
                Returns: float (0.6 to 1.4)
                """
                # SUB-COMPONENT 1: Money Flow Magnitude (35% of risk factor)
                money_flow_mm = row.get('money_flow_mm', 0)
                
                if pd.notna(money_flow_mm):
                    if money_flow_mm > 1000:  # > ₹1000Cr
                        mf_factor = 1.25  # Large institutional interest
                    elif money_flow_mm > 500:  # > ₹500Cr
                        mf_factor = 1.15
                    elif money_flow_mm > 100:  # > ₹100Cr
                        mf_factor = 1.05
                    elif money_flow_mm > 10:  # > ₹10Cr
                        mf_factor = 1.00
                    else:
                        mf_factor = 0.85  # Low money flow = higher risk
                else:
                    mf_factor = 1.00
                
                # SUB-COMPONENT 2: Smart Money Signal (30% of risk factor)
                smart_money = row.get('smart_money_flow', 'Unknown')
                
                smart_money_factors = {
                    'Strong Accumulation': 1.30,  # Institutions buying aggressively
                    'Accumulation': 1.15,
                    'Neutral': 1.00,
                    'Distribution': 0.85,
                    'Strong Distribution': 0.70,  # Institutions selling - high risk
                    'Unknown': 1.00
                }
                sm_factor = smart_money_factors.get(smart_money, 1.00)
                
                # SUB-COMPONENT 3: Liquidity Depth (20% of risk factor)
                daily_turnover = row.get('daily_turnover', 0)
                
                if pd.notna(daily_turnover):
                    if daily_turnover > 100_000_000:  # > ₹10Cr
                        liq_factor = 1.20
                    elif daily_turnover > 50_000_000:  # > ₹5Cr
                        liq_factor = 1.10
                    elif daily_turnover > 10_000_000:  # > ₹1Cr
                        liq_factor = 1.00
                    elif daily_turnover > 1_000_000:  # > ₹10L
                        liq_factor = 0.90
                    else:
                        liq_factor = 0.75  # Very illiquid
                else:
                    liq_factor = 1.00
                
                # SUB-COMPONENT 4: Size Factor (15% of risk factor)
                # Use price as proxy for market cap size
                price = row.get('price', 0)
                
                if pd.notna(price) and price > 0:
                    if price > 1000:  # Large cap proxy
                        size_factor = 1.10
                    elif price > 500:  # Mid cap
                        size_factor = 1.05
                    elif price > 100:  # Small cap
                        size_factor = 1.00
                    else:  # Micro cap (higher risk)
                        size_factor = 0.90
                else:
                    size_factor = 1.00
                
                # AGGREGATE RISK FACTOR
                risk_factor = (
                    mf_factor * 0.35 +
                    sm_factor * 0.30 +
                    liq_factor * 0.20 +
                    size_factor * 0.15
                )
                
                # Clip to reasonable range [0.6, 1.4]
                risk_factor = np.clip(risk_factor, 0.6, 1.4)
                
                return risk_factor
            
            df['advanced_ver_risk_factor'] = df.apply(calculate_risk_factor, axis=1)
            
            # ===== FINAL ADVANCED VER CALCULATION =====
            # Advanced VER = Base_VER × Quality_Multiplier × Regime_Adjustment × Risk_Factor
            df['advanced_ver'] = (
                df['advanced_ver_base'] *
                df['advanced_ver_quality_mult'] *
                df['advanced_ver_regime_adj'] *
                df['advanced_ver_risk_factor']
            ).round(3)
            
            # Calculate confidence score based on data availability
            def calculate_ver_confidence(row):
                """
                Calculate confidence score (0-100) based on data availability.
                More data available = higher confidence in Advanced VER calculation.
                """
                available_data = sum([
                    1 if pd.notna(row.get('ret_1d')) and row.get('ret_1d', 0) != 0 else 0,
                    1 if pd.notna(row.get('ret_7d')) and row.get('ret_7d', 0) != 0 else 0,
                    1 if pd.notna(row.get('ret_30d')) and row.get('ret_30d', 0) != 0 else 0,
                    1 if pd.notna(row.get('vol_ratio_1d_90d')) else 0,
                    1 if pd.notna(row.get('vmi')) else 0,
                    1 if pd.notna(row.get('momentum_harmony')) else 0,
                    1 if pd.notna(row.get('money_flow_mm')) else 0,
                    1 if pd.notna(row.get('smart_money_flow')) and row.get('smart_money_flow') != 'Unknown' else 0,
                ])
                confidence = (available_data / 8) * 100
                return confidence
            
            df['advanced_ver_confidence'] = df.apply(calculate_ver_confidence, axis=1).round(1)
            
            # ===== ADVANCED VER SCORING SYSTEM =====
            def calculate_advanced_ver_score(ver_value):
                """
                Convert Advanced VER to 0-100 score with professional thresholds.
                
                Thresholds:
                - >10.0: 100 (EXTREME - Institutional parabolic)
                - >5.0: 95 (ELITE+ - Supply shock)
                - >2.5: 85 (STRONG - Professional quality)
                - >1.5: 75 (GOOD - Above average)
                - >0.8: 60 (FAIR - Acceptable)
                - >0.5: 45 (BELOW AVG)
                - <0.5: 25 (POOR - Inefficient/risky)
                """
                if pd.isna(ver_value) or ver_value == 0:
                    return 0
                elif ver_value > 10.0:
                    return 100
                elif ver_value > 5.0:
                    return 95
                elif ver_value > 2.5:
                    return 85
                elif ver_value > 1.5:
                    return 75
                elif ver_value > 0.8:
                    return 60
                elif ver_value > 0.5:
                    return 45
                else:
                    return 25
            
            df['advanced_ver_score'] = df['advanced_ver'].apply(calculate_advanced_ver_score)
            
            # Assign letter grades with + modifiers for exceptional performance
            def assign_advanced_ver_grade(ver_value):
                """
                Assign letter grade (A+, A, B, C, D, F) based on Advanced VER value.
                
                Grades:
                - A+ (>10.0): Extreme - Institutional parabolic potential
                - A (>5.0): Elite+ - Supply shock conditions
                - B (>2.5): Strong - Professional-grade efficiency
                - C (>1.5): Good - Above-average quality
                - D (>0.8): Fair - Acceptable efficiency
                - F (<0.8): Poor - Inefficient or risky
                """
                if pd.isna(ver_value) or ver_value == 0:
                    return "N/A"
                elif ver_value > 10.0:
                    return "A+"  # Extreme
                elif ver_value > 5.0:
                    return "A"   # Elite+
                elif ver_value > 2.5:
                    return "B"   # Strong
                elif ver_value > 1.5:
                    return "C"   # Good
                elif ver_value > 0.8:
                    return "D"   # Fair
                else:
                    return "F"   # Poor
            
            df['advanced_ver_grade'] = df['advanced_ver'].apply(assign_advanced_ver_grade)
            
            # Assign status labels with emojis
            def assign_advanced_ver_status(grade):
                """Assign visual status label with emoji for Advanced VER grade."""
                grade_status = {
                    "A+": "🚀 Extreme",
                    "A": "🔥 Elite+",
                    "B": "✅ Strong",
                    "C": "⚪ Good",
                    "D": "⚠️ Fair",
                    "F": "🚨 Poor",
                    "N/A": "Unknown"
                }
                return grade_status.get(grade, "Unknown")
            
            df['advanced_ver_status'] = df['advanced_ver_grade'].apply(assign_advanced_ver_status)
            
            # Comprehensive logging
            logger.info(f"Advanced VER calculated successfully")
            logger.info(f"Advanced VER Grade distribution: {df['advanced_ver_grade'].value_counts().to_dict()}")
            logger.info(f"Advanced VER range: [{df['advanced_ver'].min():.3f} to {df['advanced_ver'].max():.3f}]")
            logger.info(f"Advanced VER mean: {df['advanced_ver'].mean():.3f}, median: {df['advanced_ver'].median():.3f}")
            logger.info(f"Advanced VER components - Base mean: {df['advanced_ver_base'].mean():.3f}, Quality mean: {df['advanced_ver_quality_mult'].mean():.3f}, Regime mean: {df['advanced_ver_regime_adj'].mean():.3f}, Risk mean: {df['advanced_ver_risk_factor'].mean():.3f}")
            logger.info(f"Advanced VER confidence scores - Mean: {df['advanced_ver_confidence'].mean():.1f}%, Median: {df['advanced_ver_confidence'].median():.1f}%")
        
        # 9. Growth Trend Classification (Based on Momentum)
        if all(col in df.columns for col in ['growth_30_to_90', 'growth_90_to_180', 'growth_momentum']):
            def classify_growth_trend(row):
                g1 = row['growth_30_to_90']       # 30d vs 90d momentum
                g2 = row['growth_90_to_180']      # 90d vs 180d momentum
                momentum = row['growth_momentum']  # Acceleration difference
                
                # Explosive momentum - Recent activity massively higher than historical
                if g1 > 200 or (g1 > 100 and g2 > 100):
                    return "🌊💎 Elite Momentum"
                elif g1 > 100 or (g1 > 50 and g2 > 50):
                    return "🚀 Explosive Growth"
                elif g1 > 50 or (g1 > 25 and g2 > 25):
                    return "📈 Strong Growth"
                elif g1 > 25 or (g1 > 10 and g2 > 10):
                    return "✅ Steady Growth"
                # Accelerating - Recent showing better momentum than older periods
                elif momentum > 50:
                    return "⚡ Rapidly Accelerating"
                elif momentum > 20:
                    return "⚡ Accelerating"
                # Stable - Both positive but similar momentum
                elif g1 > 0 and g2 > 0:
                    return "➡️ Stable Growth"
                # Declining - Recent lower than historical (deceleration)
                elif g1 < -50 or g2 < -50:
                    return "📉 Strong Decline"
                elif g1 < -25 or g2 < -25:
                    return "📉 Declining"
                elif g1 < 0 or g2 < 0:
                    return "⚠️ Weak"
                else:
                    return "⚪ Mixed"
            
            df['growth_trend'] = df.apply(classify_growth_trend, axis=1)
            
            logger.info(f"Growth trends classified. Distribution: {df['growth_trend'].value_counts().to_dict()}")
        
        # Performance tier classifications - Unified approach
        # Enhanced performance tier classification with ALL return periods
        available_return_cols = [col for col in ['ret_1d', 'ret_3d', 'ret_7d', 'ret_30d', 'ret_3m', 'ret_6m', 'ret_1y', 'ret_3y', 'ret_5y'] if col in df.columns]
        if available_return_cols:
            # Convert to percentage for classification (if needed - most are already in %)
            for col in available_return_cols:
                if col in df.columns:
                    # Check if values are in decimal format (e.g., 0.05 for 5%)
                    sample_vals = df[col].dropna().head(100)
                    if len(sample_vals) > 0 and sample_vals.abs().max() < 1.0:
                        df[f'{col}_pct'] = df[col] * 100
                    else:
                        df[f'{col}_pct'] = df[col]  # Already in percentage
            
            # Enhanced performance tier classification with ALL timeframes
            def classify_performance(row):
                # Get all return values (handle both percentage and decimal formats)
                returns = {}
                for col in available_return_cols:
                    val = row.get(col, 0) if pd.notna(row.get(col)) else 0
                    # Use percentage version if available, otherwise raw value
                    pct_col = f'{col}_pct'
                    if pct_col in row:
                        returns[col] = row.get(pct_col, 0) if pd.notna(row.get(pct_col)) else 0
                    else:
                        returns[col] = val
                
                # Priority-based classification (explosive short-term first, then longer-term)
                
                # Explosive short-term moves (highest priority)
                if 'ret_1d' in returns and returns['ret_1d'] > 15:
                    return "💥 Explosive (>15% 1D)"
                elif 'ret_1d' in returns and returns['ret_1d'] > 7:
                    return "⚡ Power Moves (>7% 1D)"
                elif 'ret_1d' in returns and returns['ret_1d'] > 3:
                    return "🚀 Strong Gainers (>3% 1D)"
                
                # Short-term momentum
                elif 'ret_3d' in returns and returns['ret_3d'] > 6:
                    return "🌟 3-Day Surge (>6% 3D)"
                elif 'ret_7d' in returns and returns['ret_7d'] > 12:
                    return "📈 Weekly Winners (>12% 7D)"
                elif 'ret_30d' in returns and returns['ret_30d'] > 25:
                    return "🏆 Monthly Champions (>25% 30D)"
                
                # Medium-term performance
                elif 'ret_3m' in returns and returns['ret_3m'] > 25:
                    return "🎯 Quarterly Stars (>25% 3M)"
                elif 'ret_6m' in returns and returns['ret_6m'] > 35:
                    return "💎 Half-Year Heroes (>35% 6M)"
                
                # Long-term performance
                elif 'ret_1y' in returns and returns['ret_1y'] > 50:
                    return "🌙 Annual Winners (>50% 1Y)"
                elif 'ret_3y' in returns and returns['ret_3y'] > 100:
                    return "👑 Multi-Year Champions (>100% 3Y)"
                elif 'ret_5y' in returns and returns['ret_5y'] > 150:
                    return "🏛️ Long-Term Legends (>150% 5Y)"
                
                else:
                    return "Standard"
            
            df['performance_tier'] = df.apply(classify_performance, axis=1)
            logger.info(f"Enhanced performance tiers created with {len(available_return_cols)} timeframes. Sample tiers: {df['performance_tier'].value_counts().head()}")
            
        # Volume tier classification
        if 'rvol' in df.columns:
            def classify_volume(row):
                rvol = row.get('rvol', 1.0) if pd.notna(row.get('rvol')) else 1.0
                
                if rvol >= 10.0:
                    return "🌋 Volcanic Volume (RVOL >10x)"
                elif rvol >= 5.0:
                    return "💥 Explosive Volume (RVOL >5x)"
                elif rvol >= 2.0:
                    return "🔥 High Activity (RVOL >2x)"
                elif rvol >= 1.5:
                    return "📈 Growing Interest (RVOL >1.5x)"
                elif rvol < 0.5:
                    return "😴 Low Activity (RVOL <0.5x)"
                else:
                    return "Standard Volume"
            
            df['volume_tier'] = df.apply(classify_volume, axis=1)
            logger.info(f"Volume tiers created. Sample tiers: {df['volume_tier'].value_counts().head()}")
            
        elif 'position_tension' in df.columns:
            # Convert position_tension to percentage from 52W low (0-100%)
            df['position_pct'] = df['position_tension'].apply(
                lambda x: min(100, max(0, x * 100)) if pd.notna(x) else None
            )
            df['position_tier'] = df['position_pct'].apply(
                lambda x: "Unknown" if pd.isna(x) else classify_tier(x, CONFIG.TIERS['position_tiers'])
            )
        elif all(col in df.columns for col in ['price', 'low_52w', 'high_52w']):
            # Calculate position percentage from price data with division by zero protection
            range_52w = df['high_52w'] - df['low_52w']
            df['position_pct'] = np.where(
                range_52w > 0,
                safe_percentage(df['price'] - df['low_52w'], range_52w, default=50.0).clip(0, 100),
                50  # Default to middle position when high equals low
            )
            df['position_tier'] = df['position_pct'].apply(
                lambda x: "Unknown" if pd.isna(x) else classify_tier(x, CONFIG.TIERS['position_tiers'])
            )
        
        return df
        
# ============================================
# ADVANCED METRICS CALCULATOR
# ============================================

class AdvancedMetrics:
    """
    Calculates advanced metrics and indicators using a combination of price,
    volume, and algorithmically derived scores. Ensures robust calculation
    by handling potential missing data (NaNs) gracefully.
    """
    
    @staticmethod
    @PerformanceMonitor.timer(target_time=0.5)
    def calculate_all_metrics(df: pd.DataFrame) -> pd.DataFrame:
        """
        Calculates a comprehensive set of advanced metrics for the DataFrame.
        All calculations are designed to be vectorized and handle missing data
        without raising errors.
        FIXED: Money flow overflow prevention added.
    
        Args:
            df (pd.DataFrame): The DataFrame with raw data and core scores.
    
        Returns:
            pd.DataFrame: The DataFrame with all calculated advanced metrics added.
        """
        if df.empty:
            return df
        
        # Money Flow (in millions) - FIXED WITH OVERFLOW PREVENTION
        if all(col in df.columns for col in ['price', 'volume_1d', 'rvol']):
            # Clip RVOL to prevent extreme multiplications - More realistic upper bound
            safe_rvol = df['rvol'].fillna(1.0).clip(0, 50)  # Reduced from 100 to 50
            
            # Calculate money flow with overflow prevention
            money_flow_raw = df['price'].fillna(0) * df['volume_1d'].fillna(0) * safe_rvol
            
            # Clip to prevent overflow (max 1 trillion)
            df['money_flow'] = np.clip(money_flow_raw, 0, 1e12)
            
            # Convert to millions for display
            df['money_flow_mm'] = df['money_flow'] / 1_000_000
            
            # Additional safety: clip the millions version too
            df['money_flow_mm'] = df['money_flow_mm'].clip(0, 1e6)  # Max 1 million millions
        else:
            df['money_flow_mm'] = pd.Series(np.nan, index=df.index)
        
        # Volume Momentum Index (VMI) - Already safe (dividing by constant 10)
        if all(col in df.columns for col in ['vol_ratio_1d_90d', 'vol_ratio_7d_90d', 'vol_ratio_30d_90d', 'vol_ratio_90d_180d']):
            df['vmi'] = (
                df['vol_ratio_1d_90d'].fillna(1.0) * 4 +
                df['vol_ratio_7d_90d'].fillna(1.0) * 3 +
                df['vol_ratio_30d_90d'].fillna(1.0) * 2 +
                df['vol_ratio_90d_180d'].fillna(1.0) * 1
            ) / 10
        else:
            df['vmi'] = pd.Series(np.nan, index=df.index)
        
        # Position Tension
        if all(col in df.columns for col in ['from_low_pct', 'from_high_pct']):
            # FIXED: NaN from_low_pct means at low (0%), NaN from_high_pct means at high (0%)
            df['position_tension'] = df['from_low_pct'].fillna(0) + abs(df['from_high_pct'].fillna(0))
        else:
            df['position_tension'] = pd.Series(np.nan, index=df.index)
        
        # Momentum Harmony
        df['momentum_harmony'] = pd.Series(0, index=df.index, dtype=int)
        
        if 'ret_1d' in df.columns:
            df['momentum_harmony'] += (df['ret_1d'].fillna(0) > 0).astype(int)
        
        if all(col in df.columns for col in ['ret_7d', 'ret_30d']):
            with np.errstate(divide='ignore', invalid='ignore'):
                # ENHANCED NaN HANDLING with explicit checks
                safe_ret_7d = df['ret_7d'].fillna(0)
                safe_ret_30d = df['ret_30d'].fillna(0)
                daily_ret_7d = safe_divide(safe_ret_7d, 7, default=0.0)
                daily_ret_7d = pd.Series(daily_ret_7d, index=df.index)
                daily_ret_30d = safe_divide(safe_ret_30d, 30, default=0.0)
                daily_ret_30d = pd.Series(daily_ret_30d, index=df.index)
            df['momentum_harmony'] += ((daily_ret_7d > daily_ret_30d)).astype(int)
        
        if all(col in df.columns for col in ['ret_30d', 'ret_3m']):
            with np.errstate(divide='ignore', invalid='ignore'):
                # ENHANCED NaN HANDLING with explicit checks
                safe_ret_30d = df['ret_30d'].fillna(0)
                safe_ret_3m = df['ret_3m'].fillna(0)
                daily_ret_30d_comp = safe_divide(safe_ret_30d, 30, default=0.0)
                daily_ret_30d_comp = pd.Series(daily_ret_30d_comp, index=df.index)
                daily_ret_3m_comp = safe_divide(safe_ret_3m, 90, default=0.0)
                daily_ret_3m_comp = pd.Series(daily_ret_3m_comp, index=df.index)
            df['momentum_harmony'] += ((daily_ret_30d_comp.fillna(-np.inf) > daily_ret_3m_comp.fillna(-np.inf))).astype(int)
        
        if 'ret_3m' in df.columns:
            df['momentum_harmony'] += (df['ret_3m'].fillna(0) > 0).astype(int)
        
        # Market State Analysis
        df['market_state'] = df.apply(AdvancedMetrics._get_market_state_from_row, axis=1)
    
        # Overall Market Strength - FOUNDATION FOCUS  
        # This measures "How solid is the setup?"
        if all(col in df.columns for col in ['position_score', 'volume_score']):
            if 'momentum_score' in df.columns:
                df['overall_market_strength'] = (
                    df['position_score'] * 0.40 +
                    df['volume_score'] * 0.35 +
                    df['momentum_score'] * 0.25
                )
            else:
                df['overall_market_strength'] = (
                    df['position_score'] * 0.55 +
                    df['volume_score'] * 0.45
                )
        else:
            df['overall_market_strength'] = np.nan
        
        # VMI and Momentum Harmony Tier Classifications
        def classify_advanced_tier(value: float, tier_config: tuple) -> str:
            """Enhanced tier classifier for VMI and Momentum Harmony with range support."""
            if pd.isna(value):
                return "Unknown"
            
            if len(tier_config) == 2:
                # Single threshold: ("column", threshold) - above threshold
                _, threshold = tier_config
                return "Above" if value > threshold else "Below"
            elif len(tier_config) == 3:
                if tier_config[2] == "below":
                    # Below threshold: ("column", threshold, "below")
                    _, threshold, _ = tier_config
                    return "Below" if value < threshold else "Above"
                else:
                    # Range: ("column", min_val, max_val)
                    _, min_val, max_val = tier_config
                    return "InRange" if min_val <= value <= max_val else "OutOfRange"
            
            return "Unknown"
        
        # VMI Tier Classification
        if 'vmi' in df.columns:
            def classify_vmi_tier(value: float) -> str:
                if pd.isna(value):
                    return "Unknown"
                if value < 0.3:
                    return "🌙 Hibernating (VMI <0.3)"
                elif 0.3 <= value < 0.6:
                    return "😴 Sleepy (VMI 0.3-0.6)"
                elif 0.6 <= value < 1.0:
                    return "🚶 Walking (VMI 0.6-1.0)"
                elif 1.0 <= value < 1.5:
                    return "🏃 Running (VMI 1.0-1.5)"
                elif 1.5 <= value < 2.5:
                    return "🚀 Flying (VMI 1.5-2.5)"
                else:  # value >= 2.5
                    return "🌋 Volcanic (VMI >2.5)"
            
            df['vmi_tier'] = df['vmi'].apply(classify_vmi_tier)
        else:
            df['vmi_tier'] = pd.Series("Unknown", index=df.index)
        
        # Momentum Harmony Tier Classification
        if 'momentum_harmony' in df.columns:
            def classify_momentum_harmony_tier(value: int) -> str:
                if pd.isna(value):
                    return "Unknown"
                harmony_map = {
                    0: "💔 Broken (Score 0)",
                    1: "🌧️ Conflicted (Score 1)", 
                    2: "⛅ Mixed (Score 2)",
                    3: "🌤️ Aligned (Score 3)",
                    4: "☀️ Perfect Harmony (Score 4)"
                }
                return harmony_map.get(int(value), "Unknown")
            
            df['momentum_harmony_tier'] = df['momentum_harmony'].apply(classify_momentum_harmony_tier)
        else:
            df['momentum_harmony_tier'] = pd.Series("Unknown", index=df.index)
        
        return df
    
    @staticmethod
    def _get_market_state_from_row(row: pd.Series) -> str:
        """
        Helper function to get market state from a pandas row.
        Converts row data to returns dict and gets market state.
        """
        returns_dict = {
            '1d': row.get('ret_1d', 0),
            '3d': row.get('ret_3d', 0),
            '7d': row.get('ret_7d', 0),
            '30d': row.get('ret_30d', 0),
            '3m': row.get('ret_3m', 0),
            '6m': row.get('ret_6m', 0)
        }
        
        market_state = AdvancedMetrics.get_market_state(returns_dict)
        return market_state['state']

    @staticmethod
    def get_market_state(returns_dict: Dict[str, float]) -> Dict[str, Any]:
        """
        Classify current market momentum state based on multi-timeframe analysis.
        FIXED: Honest momentum regime detection, not fake Elliott Waves.
        
        Core Philosophy:
        - Multi-timeframe momentum alignment
        - Identify tradeable regimes
        - Clear, actionable states
        - No pretense of Elliott Wave complexity
        
        States:
        - STRONG_UPTREND: All timeframes aligned up
        - UPTREND: Generally up with minor pullbacks
        - PULLBACK: Correction in uptrend
        - ROTATION: Trend change in progress
        - SIDEWAYS: No clear direction
        - DOWNTREND: Generally down
        - STRONG_DOWNTREND: All timeframes aligned down
        - BOUNCE: Relief rally in downtrend
        
        Returns:
            Dictionary with state, strength, and trading implications
        """
        state = {
            'state': 'UNKNOWN',
            'strength': 0,
            'confidence': 0,
            'trend_alignment': 0,
            'momentum_score': 0,
            'volatility': 'NORMAL',
            'action_bias': 'NEUTRAL',
            'description': ''
        }
        
        # Extract returns with defaults
        ret_1d = returns_dict.get('1d', 0)
        ret_3d = returns_dict.get('3d', 0)
        ret_7d = returns_dict.get('7d', 0)
        ret_30d = returns_dict.get('30d', 0)
        ret_90d = returns_dict.get('3m', 0)
        ret_180d = returns_dict.get('6m', 0)
        
        # Calculate momentum at different scales
        # Daily-equivalent rates for comparison
        very_short = ret_1d  # 1-day momentum
        # ENHANCED DIVISION PROTECTION with safe division functions
        short = safe_divide(ret_7d, 7, default=0.0)  # Daily rate over week
        medium = safe_divide(ret_30d, 30, default=0.0)  # Daily rate over month
        long = safe_divide(ret_90d, 90, default=0.0)  # Daily rate over quarter
        
        # Count positive periods (direction consistency) - ENHANCED NULL HANDLING
        periods = [ret_1d, ret_3d, ret_7d, ret_30d, ret_90d, ret_180d]
        valid_periods = [r for r in periods if pd.notna(r)]
        positive_count = sum(1 for r in valid_periods if r > 0)
        negative_count = sum(1 for r in valid_periods if r < 0)
        
        # Trend alignment score (-100 to 100) - SAFE DIVISION IMPLEMENTATION
        period_count = len(valid_periods) if valid_periods else 1  # Prevent division by zero
        trend_alignment = safe_percentage(positive_count - negative_count, period_count, default=0.0)
        state['trend_alignment'] = trend_alignment
        
        # Overall momentum score
        # Weighted average with recent periods more important
        momentum_score = (
            very_short * 0.3 +
            short * 7 * 0.25 +  # Convert back to period return
            medium * 30 * 0.25 +
            long * 90 * 0.20
        ) / 100 * 50 + 50  # Normalize to 0-100
        
        state['momentum_score'] = np.clip(momentum_score, 0, 100)
        
        # Volatility assessment
        returns_std = np.std([very_short, short * 7, medium * 30])
        if returns_std > 15:
            state['volatility'] = 'HIGH'
        elif returns_std > 7:
            state['volatility'] = 'ELEVATED'
        elif returns_std < 3:
            state['volatility'] = 'LOW'
        else:
            state['volatility'] = 'NORMAL'
        
        # STATE CLASSIFICATION LOGIC
        
        # 1. STRONG UPTREND
        if (positive_count >= 5 and ret_30d > 15 and ret_7d > 3 and ret_1d > 0):
            state['state'] = 'STRONG_UPTREND'
            state['strength'] = min(100, 70 + ret_30d)
            state['confidence'] = 85
            state['action_bias'] = 'STRONG_BUY'
            state['description'] = 'All timeframes aligned bullish. Momentum accelerating.'
        
        # 2. UPTREND
        elif (positive_count >= 4 and ret_30d > 5):
            state['state'] = 'UPTREND'
            state['strength'] = min(100, 50 + ret_30d)
            state['confidence'] = 70
            state['action_bias'] = 'BUY'
            state['description'] = 'Established uptrend. Minor corrections are buying opportunities.'
        
        # 3. PULLBACK IN UPTREND
        elif (ret_30d > 10 and ret_90d > 15 and ret_7d < 0):
            state['state'] = 'PULLBACK'
            state['strength'] = 50 - abs(ret_7d)
            state['confidence'] = 60
            state['action_bias'] = 'BUY_DIP'
            state['description'] = 'Healthy pullback in uptrend. Watch for support.'
        
        # 4. STRONG DOWNTREND
        elif (negative_count >= 5 and ret_30d < -15 and ret_7d < -3 and ret_1d < 0):
            state['state'] = 'STRONG_DOWNTREND'
            state['strength'] = min(100, 70 + abs(ret_30d))
            state['confidence'] = 85
            state['action_bias'] = 'STRONG_SELL'
            state['description'] = 'All timeframes aligned bearish. Avoid catching falling knife.'
        
        # 5. DOWNTREND
        elif (negative_count >= 4 and ret_30d < -5):
            state['state'] = 'DOWNTREND'
            state['strength'] = min(100, 50 + abs(ret_30d))
            state['confidence'] = 70
            state['action_bias'] = 'SELL'
            state['description'] = 'Established downtrend. Rallies are selling opportunities.'
        
        # 6. BOUNCE IN DOWNTREND
        elif (ret_30d < -10 and ret_90d < -15 and ret_7d > 0):
            state['state'] = 'BOUNCE'
            state['strength'] = 50 - abs(ret_30d - ret_7d)
            state['confidence'] = 50
            state['action_bias'] = 'CAUTIOUS'
            state['description'] = 'Relief rally in downtrend. Could be dead cat bounce.'
        
        # 7. ROTATION/TRANSITION
        elif (abs(ret_30d - safe_divide(ret_90d, 3, default=0)) > 10 or 
              (positive_count == 3 and negative_count == 3)):
            state['state'] = 'ROTATION'
            state['strength'] = 50
            state['confidence'] = 40
            state['action_bias'] = 'WAIT'
            state['description'] = 'Trend transition in progress. Wait for clarity.'
        
        # 8. SIDEWAYS/RANGE-BOUND
        else:
            state['state'] = 'SIDEWAYS'
            state['strength'] = 30
            state['confidence'] = 50
            state['action_bias'] = 'NEUTRAL'
            state['description'] = 'No clear trend. Range-trading environment.'
        
        # ADDITIONAL CONTEXT FLAGS
        
        # Momentum divergence detection
        if ret_30d > 10 and ret_7d < -5:
            state['warning'] = 'NEGATIVE_DIVERGENCE'
        elif ret_30d < -10 and ret_7d > 5:
            state['warning'] = 'POSITIVE_DIVERGENCE'
        
        # Extreme conditions
        if ret_30d > 30:  # Reduced from 50% to 30%
            state['extreme'] = 'OVERBOUGHT_MONTHLY'
        elif ret_30d < -25:  # Adjusted from -30% to -25%
            state['extreme'] = 'OVERSOLD_MONTHLY'
        
        if ret_7d > 20:
            state['extreme_short'] = 'OVERBOUGHT_WEEKLY'
        elif ret_7d < -15:
            state['extreme_short'] = 'OVERSOLD_WEEKLY'
        
        # Acceleration/Deceleration
        if short > medium and medium > long:
            state['momentum_phase'] = 'ACCELERATING'
        elif short < medium and medium < long:
            state['momentum_phase'] = 'DECELERATING'
        else:
            state['momentum_phase'] = 'STEADY'
        
        # TRADING RECOMMENDATIONS
        
        recommendations = []
        
        if state['state'] == 'STRONG_UPTREND':
            recommendations.append('Hold longs, add on dips')
            recommendations.append('Use trailing stops')
        elif state['state'] == 'PULLBACK':
            recommendations.append('Look for support levels')
            recommendations.append('Prepare to buy')
        elif state['state'] == 'STRONG_DOWNTREND':
            recommendations.append('Avoid longs')
            recommendations.append('Consider shorts or stay out')
        elif state['state'] == 'BOUNCE':
            recommendations.append('Take quick profits')
            recommendations.append('Don\'t chase')
        elif state['state'] == 'SIDEWAYS':
            recommendations.append('Range trade')
            recommendations.append('Buy support, sell resistance')
        elif state['state'] == 'ROTATION':
            recommendations.append('Reduce position size')
            recommendations.append('Wait for trend confirmation')
        
        state['recommendations'] = recommendations
        
        # CONFIDENCE ADJUSTMENTS
        
        # Higher confidence if volatility is normal
        if state['volatility'] == 'LOW':
            state['confidence'] *= 1.1
        elif state['volatility'] == 'HIGH':
            state['confidence'] *= 0.8
        
        # Ensure confidence is 0-100
        state['confidence'] = np.clip(state['confidence'], 0, 100)
        
        # FINAL SCORING
        
        # Overall state score (0-100)
        # Combines state strength with confidence
        state['overall_score'] = (state['strength'] * 0.7 + state['confidence'] * 0.3)
        
        return state

    @staticmethod
    def get_market_regime_summary(df: pd.DataFrame) -> Dict[str, Any]:
        """
        Analyze overall market regime from multiple stocks.
        
        Args:
            df: DataFrame with return columns
        
        Returns:
            Market regime summary
        """
        if df.empty or 'ret_30d' not in df.columns:
            return {'regime': 'UNKNOWN', 'confidence': 0}
        
        # Get valid stocks - ENHANCED VALIDATION
        valid_mask = df['ret_30d'].notna()
        if not valid_mask.any():
            return {'regime': 'UNKNOWN', 'confidence': 0, 'description': 'No valid data available'}
        
        valid_df = df[valid_mask]
        valid_count = len(valid_df)
        
        # Prevent division by zero
        if valid_count == 0:
            return {'regime': 'UNKNOWN', 'confidence': 0, 'description': 'No valid stocks found'}
        
        # Calculate market statistics
        median_return_30d = valid_df['ret_30d'].median()
        mean_return_30d = valid_df['ret_30d'].mean()
        
        # Percentage of stocks in different states - SAFE DIVISION IMPLEMENTATION
        strong_up = safe_percentage((valid_df['ret_30d'] > 20).sum(), valid_count, default=0.0)
        up = safe_percentage((valid_df['ret_30d'] > 5).sum(), valid_count, default=0.0)
        down = safe_percentage((valid_df['ret_30d'] < -5).sum(), valid_count, default=0.0)
        strong_down = safe_percentage((valid_df['ret_30d'] < -20).sum(), valid_count, default=0.0)
        
        # Determine regime
        regime = {
            'regime': 'UNKNOWN',
            'strength': 0,
            'breadth': up - down,
            'median_return': median_return_30d,
            'mean_return': mean_return_30d,
            'confidence': 0,
            'description': ''
        }
        
        # Classification logic
        if up > 70 and median_return_30d > 10:
            regime['regime'] = 'BULL_MARKET'
            regime['strength'] = min(100, up)
            regime['confidence'] = 85
            regime['description'] = f'{up:.0f}% of stocks in uptrend. Strong bull market.'
        
        elif up > 55 and median_return_30d > 5:
            regime['regime'] = 'MILD_BULL'
            regime['strength'] = 60 + (up - 55)
            regime['confidence'] = 70
            regime['description'] = f'{up:.0f}% of stocks up. Moderate bullish bias.'
        
        elif down > 70 and median_return_30d < -10:
            regime['regime'] = 'BEAR_MARKET'
            regime['strength'] = min(100, down)
            regime['confidence'] = 85
            regime['description'] = f'{down:.0f}% of stocks in downtrend. Bear market conditions.'
        
        elif down > 55 and median_return_30d < -5:
            regime['regime'] = 'MILD_BEAR'
            regime['strength'] = 60 + (down - 55)
            regime['confidence'] = 70
            regime['description'] = f'{down:.0f}% of stocks down. Moderate bearish bias.'
        
        elif abs(median_return_30d) < 5 and abs(up - down) < 20:
            regime['regime'] = 'SIDEWAYS'
            regime['strength'] = 50
            regime['confidence'] = 60
            regime['description'] = 'Market range-bound. No clear direction.'
        
        else:
            regime['regime'] = 'MIXED'
            regime['strength'] = 50
            regime['confidence'] = 40
            regime['description'] = 'Mixed signals. Market in transition.'
        
        # Add distribution details
        regime['distribution'] = {
            'strong_up_pct': strong_up,
            'up_pct': up,
            'down_pct': down,
            'strong_down_pct': strong_down,
            'sideways_pct': 100 - up - down
        }
        
        return regime
        
# ============================================
# RANKING ENGINE
# Only 3 critical improvements to your already perfect system
# ============================================

class RankingEngine:
    """
    Core ranking calculations using a multi-factor model.
    FIXED VERSION: Smoothed acceleration, better thresholds, distribution detection.
    Maintains all your original genius logic and weights.
    """

    @staticmethod
    @PerformanceMonitor.timer(target_time=0.5)
    def calculate_all_scores(df: pd.DataFrame) -> pd.DataFrame:
        """
        Master orchestration function for all score calculations.
        BULLETPROOF: Guaranteed to complete and return valid DataFrame.
        
        Architecture:
        - Modular score calculation with error recovery
        - Dynamic weight normalization (no fake values)
        - Comprehensive data quality tracking
        - Full market context analysis
        - Performance optimized with vectorization
        
        Guarantees:
        - Always returns DataFrame with all expected columns
        - Never crashes (full error handling)
        - Preserves original data integrity
        - Complete audit trail via logging
        """
        # CRITICAL: Work on copy to preserve original
        df = df.copy()
        
        if df.empty:
            logger.warning("Empty dataframe provided to calculate_all_scores")
            return df
        
        # Initialize tracking
        start_time = time.time()
        timing_breakdown = {}
        calculation_metadata = {
            'start_time': start_time,
            'initial_rows': len(df),
            'initial_columns': len(df.columns)
        }
        
        logger.info(f"="*60)
        logger.info(f"Starting comprehensive ranking for {len(df)} stocks")
        logger.info(f"="*60)
        
        # ============================================================
        # PHASE 1: COMPONENT SCORE CALCULATION
        # ============================================================
        component_start = time.time()
        
        # Define all score calculators with metadata
        score_definitions = {
            'primary': {
                'position_score': {
                    'func': RankingEngine._calculate_position_score,
                    'weight': getattr(CONFIG, 'POSITION_WEIGHT', 0.27),
                    'required': True
                },
                'volume_score': {
                    'func': RankingEngine._calculate_volume_score,
                    'weight': getattr(CONFIG, 'VOLUME_WEIGHT', 0.23),
                    'required': True
                },
                'momentum_score': {
                    'func': RankingEngine._calculate_momentum_score,
                    'weight': getattr(CONFIG, 'MOMENTUM_WEIGHT', 0.22),
                    'required': True
                },
                'acceleration_score': {
                    'func': RankingEngine._calculate_acceleration_score,
                    'weight': getattr(CONFIG, 'ACCELERATION_WEIGHT', 0.00),  # Neutralized - too noisy, redistributed to Momentum
                    'required': True
                },
                'breakout_score': {
                    'func': RankingEngine._calculate_breakout_score,
                    'weight': getattr(CONFIG, 'BREAKOUT_WEIGHT', 0.18),
                    'required': True
                },
                'rvol_score': {
                    'func': RankingEngine._calculate_rvol_score,
                    'weight': getattr(CONFIG, 'RVOL_WEIGHT', 0.10),
                    'required': True
                }
            },
            'auxiliary': {
                'trend_quality': {
                    'func': RankingEngine._calculate_trend_quality,
                    'required': False
                },
                'long_term_strength': {
                    'func': RankingEngine._calculate_long_term_strength,
                    'required': False
                },
                'liquidity_score': {
                    'func': RankingEngine._calculate_liquidity_score,
                    'required': False
                }
            }
        }
        
        # Calculate all scores with error handling
        calculation_results = {}
        
        for score_type, scores in score_definitions.items():
            for score_name, score_config in scores.items():
                try:
                    # Execute score calculation
                    result = score_config['func'](df)
                    
                    # Validate result
                    if result is None:
                        logger.warning(f"{score_name}: Returned None, creating NaN column")
                        df[score_name] = np.nan
                        calculation_results[score_name] = 'failed_none'
                        
                    elif not isinstance(result, pd.Series):
                        logger.warning(f"{score_name}: Invalid type {type(result)}, creating NaN column")
                        df[score_name] = np.nan
                        calculation_results[score_name] = 'failed_type'
                        
                    elif len(result) != len(df):
                        logger.warning(f"{score_name}: Length mismatch ({len(result)} vs {len(df)})")
                        df[score_name] = np.nan
                        calculation_results[score_name] = 'failed_length'
                        
                    else:
                        # Valid result
                        df[score_name] = result
                        valid_count = result.notna().sum()
                        calculation_results[score_name] = f'success_{valid_count}/{len(df)}'
                        
                except Exception as e:
                    logger.error(f"{score_name}: Exception - {str(e)[:100]}")
                    df[score_name] = np.nan
                    calculation_results[score_name] = f'exception_{type(e).__name__}'
        
        timing_breakdown['component_calculation'] = time.time() - component_start
        
        # ============================================================
        # PHASE 2: DATA QUALITY ASSESSMENT
        # ============================================================
        quality_start = time.time()
        
        # Extract primary scores and weights
        primary_score_names = list(score_definitions['primary'].keys())
        primary_weights = np.array([score_definitions['primary'][s]['weight'] for s in primary_score_names])
        
        # Ensure weights sum to 1 - SAFE DIVISION IMPLEMENTATION
        if primary_weights.sum() != 1.0:
            logger.warning(f"Weights sum to {primary_weights.sum():.3f}, normalizing...")
            default_weight = 1.0/len(primary_weights) if len(primary_weights) > 0 else 1.0
            primary_weights = safe_normalize(primary_weights, primary_weights.sum(), default=default_weight)
        
        # Calculate data availability matrix
        scores_matrix = df[primary_score_names].values
        data_available = ~np.isnan(scores_matrix)
        components_per_stock = data_available.sum(axis=1)
        
        # Data quality metrics
        df['components_available'] = components_per_stock
        df['data_completeness'] = (components_per_stock / len(primary_score_names)) * 100
        df['has_minimum_data'] = components_per_stock >= 4  # Minimum threshold
        
        # Quality categories
        df['data_quality_category'] = pd.cut(
            df['components_available'],
            bins=[-0.1, 2.5, 3.5, 4.5, 5.5, 6.1],
            labels=['Very Poor', 'Poor', 'Fair', 'Good', 'Excellent']
        )
        
        timing_breakdown['quality_assessment'] = time.time() - quality_start
        
        # ============================================================
        # PHASE 3: MASTER SCORE CALCULATION
        # ============================================================
        calculation_start = time.time()
        
        # Constants
        MIN_REQUIRED_COMPONENTS = 4
        
        # Initialize master scores
        master_scores = np.full(len(df), np.nan, dtype=float)
        weights_used = np.full((len(df), len(primary_score_names)), np.nan, dtype=float)
        
        # Vectorized calculation for all stocks at once
        for idx in range(len(df)):
            if components_per_stock[idx] >= MIN_REQUIRED_COMPONENTS:
                # Get valid components for this stock
                valid_mask = data_available[idx]
                
                if valid_mask.any():
                    # Extract valid scores and weights
                    valid_scores = scores_matrix[idx][valid_mask]
                    valid_weights = primary_weights[valid_mask]
                    
                    # Normalize weights - SAFE DIVISION IMPLEMENTATION
                    if valid_weights.sum() > 0:
                        default_weight = 1.0/len(valid_weights) if len(valid_weights) > 0 else 1.0
                        normalized_weights = safe_normalize(valid_weights, valid_weights.sum(), default=default_weight)
                        # Calculate weighted average
                        master_scores[idx] = np.dot(valid_scores, normalized_weights)
                        # Store weights for transparency
                        weights_used[idx][valid_mask] = normalized_weights
        
        # Assign raw scores
        df['master_score_raw'] = master_scores
        
        # Calculate quality multiplier (no linear penalty, use curve)
        # 6/6 = 1.00, 5/6 = 0.88, 4/6 = 0.72
        MAX_COMPONENTS = 6  # Make configurable instead of hard-coded
        df['quality_multiplier'] = np.where(
            df['components_available'] < MIN_REQUIRED_COMPONENTS,
            np.nan,  # No score if insufficient data
            0.5 + 0.5 * safe_divide(df['components_available'], MAX_COMPONENTS, default=1.0) ** 1.5  # Exponential curve
        )
        
        # Apply quality adjustment
        df['master_score_before_bonus'] = df['master_score_raw'] * df['quality_multiplier']
        df['master_score'] = df['master_score_before_bonus'].clip(0, 100)
        
        timing_breakdown['master_calculation'] = time.time() - calculation_start
        
        # ============================================================
        # PHASE 3.5: MARKET REGIME ADJUSTMENTS
        # ============================================================
        regime_start = time.time()
        
        # Detect overall market regime and apply adjustments
        try:
            # Get market regime summary
            market_regime = AdvancedMetrics.get_market_regime_summary(df)
            logger.info(f"Market regime detected: {market_regime['regime']} "
                       f"(confidence: {market_regime['confidence']:.0f}%)")
            logger.info(f"Market regime description: {market_regime['description']}")
            
            # Apply regime-based score adjustments
            valid_scores = df['master_score'].notna()
            adjustments_applied = 0
            
            if valid_scores.any():
                # BULL MARKET: Boost momentum and breakout scores by 5%
                if market_regime['regime'] in ['BULL_MARKET', 'MILD_BULL']:
                    bull_boost_mask = valid_scores & (
                        (df['momentum_score'].fillna(0) > 50) | 
                        (df['breakout_score'].fillna(0) > 50)
                    )
                    if bull_boost_mask.any():
                        # Boost component scores as specified, not master score
                        if 'momentum_score' in df.columns:
                            df.loc[bull_boost_mask, 'momentum_score'] *= 1.05
                        if 'breakout_score' in df.columns:
                            df.loc[bull_boost_mask, 'breakout_score'] *= 1.05
                        # Clip to ensure scores stay within bounds
                        if 'momentum_score' in df.columns:
                            df['momentum_score'] = df['momentum_score'].clip(0, 100)
                        if 'breakout_score' in df.columns:
                            df['breakout_score'] = df['breakout_score'].clip(0, 100)
                        adjustments_applied += bull_boost_mask.sum()
                        logger.info(f"Applied bull market component score boost to {bull_boost_mask.sum()} stocks")
                
                # BEAR MARKET: Boost position scores for stocks near 52w lows by 10%
                elif market_regime['regime'] in ['BEAR_MARKET', 'MILD_BEAR']:
                    if 'from_low_pct' in df.columns and 'position_score' in df.columns:
                        # FIXED: Use 999 to exclude NaN values from boost (they're at the low)
                        bear_boost_mask = valid_scores & (df['from_low_pct'].fillna(999) < 20)
                        if bear_boost_mask.any():
                            # Boost position score as specified, not master score
                            df.loc[bear_boost_mask, 'position_score'] *= 1.10
                            df['position_score'] = df['position_score'].clip(0, 100)
                            adjustments_applied += bear_boost_mask.sum()
                            logger.info(f"Applied bear market position score boost to {bear_boost_mask.sum()} stocks")
                
                # SIDEWAYS: No adjustments (as specified)
                elif market_regime['regime'] == 'SIDEWAYS':
                    logger.info("Sideways market detected - no regime adjustments applied")
            
            # Store regime info for later use
            df.attrs['market_regime'] = market_regime
            
            if adjustments_applied > 0:
                logger.info(f"Market regime adjustments applied to {adjustments_applied} stocks")
                # Recalculate master score since component scores changed
                logger.info("Recalculating master scores after regime adjustments...")
                df = RankingEngine._recalculate_master_score_after_adjustments(df)
            else:
                logger.info("No market regime adjustments applied")
                
        except Exception as e:
            logger.error(f"Market regime detection failed: {e}")
            df.attrs['market_regime'] = {'regime': 'UNKNOWN', 'confidence': 0}
        
        # Ensure master score stays in bounds after regime adjustments
        df['master_score'] = df['master_score'].clip(0, 100)
        
        timing_breakdown['regime_adjustments'] = time.time() - regime_start
        
        # ============================================================
        # PHASE 3.6: MARKET STATE FILTERING
        # ============================================================
        filter_start = time.time()
        
        try:
            # Apply market state filtering based on trading strategy
            if getattr(CONFIG, 'ENABLE_MARKET_STATE_FILTER', True):
                filter_name = getattr(CONFIG, 'DEFAULT_MARKET_FILTER', 'MOMENTUM')
                logger.info(f"Applying {filter_name} market state filter...")
                df = RankingEngine.apply_market_state_filter(df, filter_name)
            else:
                logger.info("Market state filtering disabled - keeping all stocks")
                
        except Exception as e:
            logger.error(f"Market state filtering failed: {e}")
            logger.error("Continuing with unfiltered dataset")
        
        timing_breakdown['market_state_filtering'] = time.time() - filter_start
        
        # ============================================================
        # PHASE 4: SMART BONUSES
        # ============================================================
        bonus_start = time.time()
        
        try:
            df = RankingEngine._apply_smart_bonuses(df)
            df['bonuses_applied'] = True
        except Exception as e:
            logger.error(f"Smart bonuses failed: {e}")
            df['bonuses_applied'] = False
            df['bonus_points'] = 0
            df['bonus_reasons'] = ''
        
        # Ensure master score stays in bounds
        df['master_score'] = df['master_score'].clip(0, 100)
        
        timing_breakdown['bonuses'] = time.time() - bonus_start
        
        # ============================================================
        # PHASE 5: RANKING CALCULATIONS
        # ============================================================
        ranking_start = time.time()
        
        # Overall rankings
        valid_scores_exist = df['master_score'].notna().any()
        
        if valid_scores_exist:
            # Standard rankings
            df['rank'] = df['master_score'].rank(
                method='first', 
                ascending=False, 
                na_option='bottom'
            )
            
            df['percentile'] = df['master_score'].rank(
                method='average',
                ascending=True,
                pct=True,
                na_option='bottom'
            ) * 100
            
            # Decile groups
            df['decile'] = pd.qcut(
                df['master_score'].dropna(),
                q=10,
                labels=range(10, 0, -1),
                duplicates='drop'
            ).reindex(df.index)
            
        else:
            df['rank'] = np.nan
            df['percentile'] = 0
            df['decile'] = np.nan
        
        # Fill NaN ranks
        df['rank'] = df['rank'].fillna(len(df) + 1)
        df['percentile'] = df['percentile'].fillna(0)
        
        # Score grades
        if valid_scores_exist:
            df['score_grade'] = pd.cut(
                df['master_score'],
                bins=[0, 25, 40, 50, 60, 70, 80, 90, 100],
                labels=['F', 'E', 'D', 'C', 'B', 'A', 'AA', 'AAA']
            )
        else:
            df['score_grade'] = pd.Categorical([np.nan] * len(df))
        
        # Category rankings
        try:
            df = RankingEngine._calculate_category_ranks(df)
        except Exception as e:
            logger.error(f"Category ranking failed: {e}")
            df['category_rank'] = np.nan
            df['category_percentile'] = np.nan
        
        timing_breakdown['rankings'] = time.time() - ranking_start
        
        # ============================================================
        # PHASE 6: CONFIDENCE METRICS
        # ============================================================
        confidence_start = time.time()
        
        # Multi-factor confidence score
        df['confidence_score'] = np.where(
            df['has_minimum_data'],
            (
                df['data_completeness'] * 0.40 +  # Data availability
                np.minimum(df['liquidity_score'].fillna(0), 100) * 0.30 +  # FIXED: Use 0 for missing liquidity
                (100 - np.abs(df['percentile'] - 50)) * 0.30  # Rank stability
            ) / 100,
            0
        ).clip(0, 100)
        
        # Confidence categories
        df['confidence_level'] = pd.cut(
            df['confidence_score'],
            bins=[0, 30, 50, 70, 85, 100],
            labels=['Very Low', 'Low', 'Medium', 'High', 'Very High']
        )
        
        timing_breakdown['confidence'] = time.time() - confidence_start
        
        # ============================================================
        # PHASE 7: MARKET CONTEXT
        # ============================================================
        context_start = time.time()
        
        # Calculate market statistics
        scored_stocks = df[df['master_score'].notna()]
        
        market_stats = {
            'total_stocks': len(df),
            'scored_stocks': len(scored_stocks),
            'coverage_pct': (len(scored_stocks) / len(df) * 100) if len(df) > 0 else 0
        }
        
        if len(scored_stocks) > 0:
            # Score statistics
            market_stats.update({
                'mean_score': scored_stocks['master_score'].mean(),
                'median_score': scored_stocks['master_score'].median(),
                'std_score': scored_stocks['master_score'].std(),
                'q1_score': scored_stocks['master_score'].quantile(0.25),
                'q3_score': scored_stocks['master_score'].quantile(0.75)
            })
            
            # Component statistics
            for component in primary_score_names:
                if component in df.columns:
                    market_stats[f'mean_{component}'] = df[component].mean()
            
            # Market regime
            momentum_avg = df['momentum_score'].mean() if 'momentum_score' in df.columns else np.nan
            if pd.notna(momentum_avg):
                if momentum_avg > 65:
                    market_regime = 'strong_bull'
                elif momentum_avg > 55:
                    market_regime = 'bull'
                elif momentum_avg > 45:
                    market_regime = 'neutral'
                elif momentum_avg > 35:
                    market_regime = 'bear'
                else:
                    market_regime = 'strong_bear'
            else:
                market_regime = 'unknown'
            
            # Z-scores
            df['z_score'] = safe_divide(
                df['master_score'] - market_stats['mean_score'],
                market_stats['std_score'],
                default=0.0
            ).clip(-3, 3)
                
        else:
            market_stats.update({
                'mean_score': np.nan,
                'median_score': np.nan,
                'std_score': np.nan,
                'q1_score': np.nan,
                'q3_score': np.nan
            })
            market_regime = 'no_data'
            df['z_score'] = np.nan
        
        df['market_regime'] = market_regime
        
        timing_breakdown['context'] = time.time() - context_start
        
        # ============================================================
        # PHASE 8: FINAL VALIDATION
        # ============================================================
        validation_start = time.time()
        
        # Ensure all expected columns exist
        required_columns = {
            # Scores
            **{name: np.nan for name in primary_score_names},
            **{name: np.nan for name in score_definitions['auxiliary'].keys()},
            # Master scores
            'master_score': np.nan,
            'master_score_raw': np.nan,
            'master_score_before_bonus': np.nan,
            'quality_multiplier': np.nan,
            # Rankings
            'rank': len(df) + 1,
            'percentile': 0,
            'decile': np.nan,
            'score_grade': np.nan,
            # Quality metrics
            'components_available': 0,
            'data_completeness': 0,
            'has_minimum_data': False,
            'data_quality_category': 'Unknown',
            # Confidence
            'confidence_score': 0,
            'confidence_level': 'Unknown',
            # Market context
            'market_regime': 'unknown',
            'z_score': 0,
            # Bonuses
            'bonus_points': 0,
            'bonus_reasons': '',
            'bonuses_applied': False
        }
        
        for col, default_value in required_columns.items():
            if col not in df.columns:
                logger.debug(f"Adding missing column: {col}")
                df[col] = default_value
        
        timing_breakdown['validation'] = time.time() - validation_start
        
        # ============================================================
        # FINAL REPORTING
        # ============================================================
        total_time = time.time() - start_time
        
        # Comprehensive logging
        logger.info("="*60)
        logger.info("RANKING CALCULATION COMPLETE")
        logger.info("="*60)
        logger.info(f"Total execution time: {total_time:.3f}s")
        logger.info(f"Stocks processed: {market_stats['scored_stocks']}/{market_stats['total_stocks']} "
                   f"({market_stats['coverage_pct']:.1f}% coverage)")
        
        if market_stats['scored_stocks'] > 0:
            logger.info(f"Score distribution: Mean={market_stats['mean_score']:.1f}, "
                       f"Median={market_stats['median_score']:.1f}, "
                       f"Std={market_stats['std_score']:.1f}")
            logger.info(f"Market regime: {market_regime.upper()}")
        
        # Performance breakdown
        logger.info("Performance breakdown:")
        for phase, duration in timing_breakdown.items():
            pct = (duration / total_time * 100) if total_time > 0 else 0
            status = "⚠️" if duration > 0.1 else "✓"
            logger.info(f"  {status} {phase}: {duration:.3f}s ({pct:.1f}%)")
        
        # Data quality report
        quality_dist = df['data_quality_category'].value_counts()
        if len(quality_dist) > 0:
            logger.info("Data quality distribution:")
            for category, count in quality_dist.items():
                logger.info(f"  {category}: {count} stocks")
        
        # Top performers
        if 'ticker' in df.columns and market_stats['scored_stocks'] >= 5:
            top_5 = df.nlargest(5, 'master_score')
            logger.info("Top 5 stocks:")
            for rank, (_, row) in enumerate(top_5.iterrows(), 1):
                logger.info(f"  {rank}. {row['ticker']}: {row['master_score']:.1f} "
                           f"({row['components_available']}/6 components)")
        
        # Warnings
        if total_time > 0.5:
            logger.warning(f"⚠️ Exceeded target time of 0.5s by {total_time - 0.5:.3f}s")
        
        if market_stats['coverage_pct'] < 50:
            logger.warning(f"⚠️ Low data coverage: {market_stats['coverage_pct']:.1f}%")
        
        # Component calculation summary
        logger.debug("Component calculation results:")
        for component, status in calculation_results.items():
            logger.debug(f"  {component}: {status}")
        
        logger.info("="*60)
        
        # GUARANTEE: Return valid DataFrame
        return df
        
    @staticmethod
    def _recalculate_master_score_after_adjustments(df: pd.DataFrame) -> pd.DataFrame:
        """
        Recalculate master score after regime adjustments to component scores.
        This ensures the master score reflects the adjusted component scores.
        """
        # Get available score columns and their weights
        score_cols = {
            'position_score': getattr(CONFIG, 'POSITION_WEIGHT', 0.27),
            'volume_score': getattr(CONFIG, 'VOLUME_WEIGHT', 0.23),
            'momentum_score': getattr(CONFIG, 'MOMENTUM_WEIGHT', 0.22),
            'acceleration_score': getattr(CONFIG, 'ACCELERATION_WEIGHT', 0.00),  # Neutralized
            'breakout_score': getattr(CONFIG, 'BREAKOUT_WEIGHT', 0.18),
            'rvol_score': getattr(CONFIG, 'RVOL_WEIGHT', 0.10)
        }
        
        # Initialize new master scores
        new_master_scores = pd.Series(np.nan, index=df.index)
        
        for idx in df.index:
            row = df.loc[idx]
            
            # Get available scores
            available_scores = []
            available_weights = []
            
            for col, weight in score_cols.items():
                if col in df.columns and pd.notna(row[col]):
                    available_scores.append(row[col])
                    available_weights.append(weight)
            
            # Calculate weighted score if we have minimum components
            if len(available_scores) >= 3:  # Minimum required components
                # Normalize weights to sum to 1
                available_weights = np.array(available_weights)
                available_weights = available_weights / available_weights.sum()
                
                # Calculate weighted score
                weighted_score = np.dot(available_scores, available_weights)
                new_master_scores[idx] = weighted_score
        
        # Update master score with new calculation
        # Apply the same quality multiplier that was used before
        df['master_score_raw'] = new_master_scores
        df['master_score_before_bonus'] = df['master_score_raw'] * df['quality_multiplier']
        df['master_score'] = df['master_score_before_bonus'].clip(0, 100)
        
        return df
        
    @staticmethod
    def apply_market_state_filter(df: pd.DataFrame, filter_name: str = None) -> pd.DataFrame:
        """
        Apply market state filtering based on trading strategy preferences.
        
        Args:
            df: DataFrame with stocks and their market states
            filter_name: Filter to apply (MOMENTUM, AGGRESSIVE, VALUE, DEFENSIVE, ALL)
                        If None, uses CONFIG.DEFAULT_MARKET_FILTER
        
        Returns:
            Filtered DataFrame optimized for the specified trading strategy
        """
        # Check if filtering is enabled
        if not getattr(CONFIG, 'ENABLE_MARKET_STATE_FILTER', True):
            logger.info("Market state filtering is disabled in configuration")
            return df
        
        # Use default filter if none specified
        if filter_name is None:
            filter_name = getattr(CONFIG, 'DEFAULT_MARKET_FILTER', 'MOMENTUM')
        
        # Get the filter configuration
        filter_config = getattr(CONFIG, 'MARKET_STATE_FILTERS', {}).get(filter_name)
        if not filter_config:
            logger.warning(f"Unknown market state filter: {filter_name}, using ALL")
            filter_name = 'ALL'
            filter_config = getattr(CONFIG, 'MARKET_STATE_FILTERS', {}).get('ALL', {
                'allowed_states': ['STRONG_UPTREND', 'UPTREND', 'PULLBACK', 'ROTATION', 
                                 'SIDEWAYS', 'DOWNTREND', 'STRONG_DOWNTREND', 'BOUNCE'],
                'description': 'No filtering - all market states included'
            })
        
        # MEMORY OPTIMIZATION: Work on view first, only copy if changes needed
        original_count = len(df)
        
        logger.info(f"="*50)
        logger.info(f"Applying {filter_name} market state filter")
        logger.info(f"Original dataset: {original_count} stocks")
        
        # Check if we have market state data
        if 'market_state' not in df.columns:
            logger.warning("No market_state column found - filter cannot be applied")
            logger.info(f"="*50)
            return df
        
        # Get allowed states for this filter
        allowed_states = filter_config.get('allowed_states', [])
        if not allowed_states:
            logger.warning(f"No allowed states defined for {filter_name} filter")
            logger.info(f"="*50)
            return df
        
        # Apply the filter
        try:
            # Create filter mask
            state_mask = df['market_state'].isin(allowed_states)
            
            # MEMORY OPTIMIZATION: Only copy if filtering is actually needed
            if state_mask.all():
                # No filtering needed, return original
                filtered_df = df
                logger.info("No filtering applied - all stocks match criteria")
            else:
                filtered_df = df[state_mask].copy()
            
            # Log filtering results
            filtered_count = len(filtered_df)
            filtered_pct = (filtered_count / original_count * 100) if original_count > 0 else 0
            
            logger.info(f"Filter allowed states: {', '.join(allowed_states)}")
            logger.info(f"Filtered dataset: {filtered_count} stocks ({filtered_pct:.1f}% retained)")
            
            # Show state breakdown before filtering - DIVISION PROTECTED
            state_counts = df['market_state'].value_counts()
            logger.info("Market state distribution before filtering:")
            for state, count in state_counts.items():
                pct = (count / original_count * 100) if original_count > 0 else 0
                status = "✓ ALLOWED" if state in allowed_states else "✗ FILTERED"
                logger.info(f"  {state}: {count} stocks ({pct:.1f}%) - {status}")
            
            # Store filter information for reference
            filtered_df.attrs['market_filter_applied'] = filter_name
            filtered_df.attrs['market_filter_allowed_states'] = allowed_states
            filtered_df.attrs['original_count'] = original_count
            filtered_df.attrs['filtered_count'] = filtered_count
            filtered_df.attrs['filter_retention_pct'] = filtered_pct
            
            # Emergency backup: If filter is too aggressive (less than 5% retention), keep top stocks
            if filtered_count < max(1, original_count * 0.05):
                logger.warning(f"Filter too aggressive ({filtered_pct:.1f}% retention)")
                logger.warning("Falling back to top 20% of stocks by master_score")
                
                # Sort by master_score and keep top 20%
                if 'master_score' in df.columns:
                    df_sorted = df.dropna(subset=['master_score']).sort_values('master_score', ascending=False)
                    backup_count = max(1, int(len(df_sorted) * 0.20))
                    backup_df = df_sorted.head(backup_count).copy()
                    
                    backup_df.attrs['market_filter_applied'] = f"{filter_name}_BACKUP"
                    backup_df.attrs['backup_reason'] = "Filter too aggressive"
                    backup_df.attrs['backup_retention_pct'] = (backup_count / original_count * 100)
                    
                    logger.info(f"Backup filter: keeping top {backup_count} stocks ({backup_df.attrs['backup_retention_pct']:.1f}%)")
                    logger.info(f"="*50)
                    
                    return backup_df
            
            logger.info(f"Market state filtering completed successfully")
            logger.info(f"="*50)
            
            return filtered_df
            
        except Exception as e:
            logger.error(f"Market state filtering failed: {e}")
            logger.error("Returning unfiltered dataset")
            logger.info(f"="*50)
            return df
    
    @staticmethod
    def _calculate_position_score(df: pd.DataFrame) -> pd.Series:
        """
        Calculate position score based on actual market dynamics.
        FIXED: No arbitrary sweet spots, clear breakout/reversal logic.
        
        Core Philosophy:
        - Extremes matter, middle doesn't
        - Near 52w high = Breakout potential (momentum play)
        - Near 52w low = Reversal potential (value play)
        - Middle range = Dead zone (no edge)
        
        Scoring Strategy:
        - This implementation favors MOMENTUM (high position = high score)
        - For value strategy, simply invert the final score
        
        Score Range:
        - 85-100: Breaking out or above 52w high
        - 70-85: Near 52w high (strength zone)
        - 30-70: Middle range (neutral/dead zone)
        - 15-30: Near 52w low (weakness)
        - 0-15: At/below 52w low (extreme weakness)
        """
        # Initialize with NaN
        position_score = pd.Series(np.nan, index=df.index, dtype=float)
        
        # Check required data
        if 'from_low_pct' not in df.columns or 'from_high_pct' not in df.columns:
            logger.warning("Missing position data (from_low_pct/from_high_pct)")
            return position_score
        
        from_low = df['from_low_pct']
        from_high = df['from_high_pct']
        
        # Only calculate where both values exist
        valid_mask = from_low.notna() & from_high.notna()
        
        if not valid_mask.any():
            logger.warning("No valid position data available")
            return position_score
        
        # CORE CALCULATION: Simple position in range
        # This is the true position (0-100% of 52w range)
        position_in_range = pd.Series(np.nan, index=df.index)
        
        # For stocks below 52w high (normal case)
        below_high = valid_mask & (from_high <= 0)
        if below_high.any():
            # Direct position calculation
            position_in_range[below_high] = from_low[below_high].clip(0, 100)
        
        # For stocks above 52w high (breakout)
        above_high = valid_mask & (from_high > 0)
        if above_high.any():
            # Above high = 100+ (can go to 110 for 10% above high)
            position_in_range[above_high] = 100 + from_high[above_high].clip(0, 20)
        
        # SCORING PHILOSOPHY: Favor extremes, penalize middle
        # Research shows stocks at extremes have edge, middle range doesn't
        
        if valid_mask.any():
            # Non-linear scoring that emphasizes extremes
            for idx in df[valid_mask].index:
                pos = position_in_range[idx]
                
                if pd.isna(pos):
                    continue
                    
                # BREAKOUT ZONE (>90%): High scores
                if pos >= 90:
                    if pos > 100:  # Above 52w high
                        # Diminishing returns above high
                        position_score[idx] = 90 + np.minimum(10, (pos - 100) * 0.5)
                    else:  # 90-100%
                        position_score[idx] = 80 + (pos - 90) * 1.0
                
                # STRENGTH ZONE (70-90%): Good scores
                elif pos >= 70:
                    position_score[idx] = 60 + (pos - 70) * 1.0
                
                # DEAD ZONE (30-70%): Reduced scores
                # This is where your "sweet spot" was, but it's actually dead money
                elif pos >= 30:
                    position_score[idx] = 30 + (pos - 30) * 0.75  # Compressed scoring
                
                # VALUE ZONE (10-30%): Lower scores (but valuable for mean reversion)
                elif pos >= 10:
                    position_score[idx] = 15 + (pos - 10) * 0.75
                
                # EXTREME LOW (<10%): Minimum scores
                else:
                    position_score[idx] = pos * 1.5
        
        # CONTEXT ADJUSTMENTS
        
        # 1. MOMENTUM CONFIRMATION
        # High position needs momentum to confirm
        if 'ret_30d' in df.columns and position_score.notna().any():
            ret_30d = df['ret_30d']
            
            # Breakout with momentum = confirmed
            breakout_confirmed = (
                (position_in_range > 85) & 
                (ret_30d > 10) & 
                position_score.notna()
            )
            if breakout_confirmed.any():
                position_score[breakout_confirmed] = np.minimum(
                    position_score[breakout_confirmed] * 1.1,
                    100
                )
            
            # High position but negative momentum = false breakout
            false_breakout = (
                (position_in_range > 85) & 
                (ret_30d < -5) & 
                position_score.notna()
            )
            if false_breakout.any():
                position_score[false_breakout] *= 0.8
            
            # Low position with positive momentum = potential reversal
            reversal_starting = (
                (position_in_range < 20) & 
                (ret_30d > 5) & 
                position_score.notna()
            )
            if reversal_starting.any():
                position_score[reversal_starting] += 10
        
        # 2. VOLUME CONFIRMATION
        # Breakouts need volume
        if 'rvol' in df.columns and position_score.notna().any():
            rvol = df['rvol']
            
            # High position with volume = strong
            high_with_volume = (
                (position_in_range > 80) & 
                (rvol > 1.5) & 
                position_score.notna()
            )
            if high_with_volume.any():
                position_score[high_with_volume] = np.minimum(
                    position_score[high_with_volume] + 5,
                    100
                )
            
            # High position without volume = weak
            high_no_volume = (
                (position_in_range > 80) & 
                (rvol < 0.8) & 
                position_score.notna()
            )
            if high_no_volume.any():
                position_score[high_no_volume] -= 10
        
        # 3. TIME AT EXTREME
        # How long has it been near high/low?
        if 'sma_20d' in df.columns and 'price' in df.columns:
            price = df['price']
            sma_20 = df['sma_20d']
            
            valid_time = price.notna() & sma_20.notna() & position_score.notna()
            
            if valid_time.any():
                # If near high and price > SMA20 = sustained strength
                sustained_high = (
                    valid_time & 
                    (position_in_range > 80) & 
                    (price > sma_20)
                )
                if sustained_high.any():
                    position_score[sustained_high] += 3
                
                # If near low and price < SMA20 = sustained weakness
                sustained_low = (
                    valid_time & 
                    (position_in_range < 20) & 
                    (price < sma_20)
                )
                if sustained_low.any():
                    position_score[sustained_low] -= 3
        
        # 4. MARKET CAP CONTEXT
        if 'category' in df.columns and position_score.notna().any():
            category = df['category']
            
            # Large caps at extremes are more significant
            is_large = category.isin(['Large Cap', 'Mega Cap'])
            large_extreme = (
                is_large & 
                ((position_in_range > 90) | (position_in_range < 10)) & 
                position_score.notna()
            )
            if large_extreme.any():
                position_score[large_extreme] = (
                    50 + (position_score[large_extreme] - 50) * 1.2
                ).clip(0, 100)
            
            # Small caps above high need caution
            is_small = category.isin(['Micro Cap', 'Small Cap'])
            small_extended = (
                is_small & 
                (position_in_range > 110) & 
                position_score.notna()
            )
            if small_extended.any():
                position_score[small_extended] = np.minimum(
                    position_score[small_extended],
                    85
                )
        
        # 5. MARKET REGIME
        # In bull markets, high position better. In bear, low position better.
        if position_score.notna().any() and len(df[valid_mask]) > 100:
            # Calculate market regime from average position
            avg_position = position_in_range[valid_mask].median()
            
            if pd.notna(avg_position):
                if avg_position > 65:
                    # Bull market: Boost high positions
                    bull_high = (position_in_range > 70) & position_score.notna()
                    if bull_high.any():
                        position_score[bull_high] += 5
                        
                elif avg_position < 35:
                    # Bear market: Value in low positions
                    bear_low = (position_in_range < 30) & position_score.notna()
                    if bear_low.any():
                        position_score[bear_low] += 5
        
        # Final clipping
        position_score = position_score.clip(0, 100)
        
        # NEVER FILL NaN!
        
        # COMPREHENSIVE LOGGING
        valid_scores = position_score.notna().sum()
        total_stocks = len(df)
        
        logger.info(f"Position scores: {valid_scores}/{total_stocks} calculated")
        
        if valid_scores > 0:
            score_dist = position_score[position_score.notna()]
            logger.info(f"Distribution - Mean: {score_dist.mean():.1f}, "
                       f"Median: {score_dist.median():.1f}, "
                       f"Std: {score_dist.std():.1f}")
            
            # Position distribution
            if valid_mask.any():
                pos_dist = position_in_range[valid_mask]
                at_highs = (pos_dist > 90).sum()
                at_lows = (pos_dist < 10).sum()
                in_middle = ((pos_dist >= 30) & (pos_dist <= 70)).sum()
                
                logger.info(f"Position distribution: {at_highs} near highs, "
                           f"{at_lows} near lows, {in_middle} in dead zone")
                
                if in_middle > len(pos_dist) * 0.5:
                    logger.warning(f"Warning: {in_middle} stocks in dead zone (30-70%)")
        
        return position_score
    
    @staticmethod
    def _calculate_volume_score(df: pd.DataFrame) -> pd.Series:
        """
        Calculate volume score focusing on what actually matters.
        ENHANCED: Volume confirms, doesn't predict. Focus on liquidity, relative volume, and consistency.
        
        Core Philosophy:
        - Volume is a CONFIRMING indicator, not predictive
        - Relative volume matters more than absolute
        - Liquidity (turnover) matters for tradability
        - Consistency separates quality from manipulation
        - Extreme volume often signals problems
        
        Components:
        - 45% Relative volume (vs historical average)
        - 30% Liquidity (can you actually trade it?)
        - 15% Price-volume harmony (does volume confirm price action?)
        - 10% Volume consistency (stable vs erratic patterns)
        
        Score Range:
        - 70-100: High relative volume with price confirmation and consistency
        - 50-70: Above average activity
        - 30-50: Normal activity
        - 10-30: Below average (concerning)
        - 0-10: Dead (no liquidity)
        """
        # Initialize with NaN
        volume_score = pd.Series(np.nan, index=df.index, dtype=float)
        
        # Check minimum data
        has_rvol = 'rvol' in df.columns
        has_volume = 'volume_1d' in df.columns
        has_price = 'price' in df.columns
        
        if not has_rvol and not has_volume:
            logger.warning("No volume data available")
            return volume_score
        
        # COMPONENT 1: RELATIVE VOLUME (45% weight)
        # This is the most important - is volume unusual today?
        relative_component = pd.Series(np.nan, index=df.index, dtype=float)
        
        if has_rvol:
            rvol = df['rvol']
            valid_rvol = rvol.notna() & (rvol >= 0)
            
            if valid_rvol.any():
                # Simple but effective scoring
                # < 0.5x = dead (20)
                # 0.5-1x = below normal (20-50)
                # 1-2x = normal to elevated (50-70)
                # 2-5x = high interest (70-85)
                # > 5x = extreme (85-90, capped due to potential manipulation)
                
                relative_component[valid_rvol] = np.where(
                    rvol[valid_rvol] < 0.5,
                    rvol[valid_rvol] * 40,  # 0-20
                    np.where(
                        rvol[valid_rvol] < 1,
                        20 + (rvol[valid_rvol] - 0.5) * 60,  # 20-50
                        np.where(
                            rvol[valid_rvol] < 2,
                            50 + (rvol[valid_rvol] - 1) * 20,  # 50-70
                            np.where(
                                rvol[valid_rvol] < 5,
                                70 + np.log(rvol[valid_rvol]) * 10,  # 70-85 (log scale)
                                np.minimum(90, 85 + np.log(rvol[valid_rvol] / 5) * 5)  # 85-90 cap
                            )
                        )
                    )
                )
        
        # COMPONENT 2: LIQUIDITY (30% weight)
        # Can you actually trade this stock?
        liquidity_component = pd.Series(np.nan, index=df.index, dtype=float)
        
        if has_volume and has_price:
            volume = df['volume_1d']
            price = df['price']
            
            valid_liquidity = volume.notna() & price.notna() & (volume >= 0) & (price > 0)
            
            if valid_liquidity.any():
                # Calculate turnover in lakhs (100,000s)
                turnover_lakhs = (volume[valid_liquidity] * price[valid_liquidity]) / 100000
                
                # Scoring based on Indian market standards
                # < 10 lakhs = illiquid (0-30)
                # 10-100 lakhs = low liquidity (30-50)
                # 100-1000 lakhs = decent (50-70)
                # 1000-10000 lakhs = good (70-85)
                # > 10000 lakhs = excellent (85-100)
                
                log_turnover = np.log10(turnover_lakhs + 1)  # Add 1 to handle zero
                
                liquidity_component[valid_liquidity] = np.where(
                    log_turnover < 1,  # < 10 lakhs
                    log_turnover * 30,
                    np.where(
                        log_turnover < 2,  # 10-100 lakhs
                        30 + (log_turnover - 1) * 20,
                        np.where(
                            log_turnover < 3,  # 100-1000 lakhs
                            50 + (log_turnover - 2) * 20,
                            np.where(
                                log_turnover < 4,  # 1000-10000 lakhs
                                70 + (log_turnover - 3) * 15,
                                np.minimum(100, 85 + (log_turnover - 4) * 10)
                            )
                        )
                    )
                )
        
        # COMPONENT 3: PRICE-VOLUME HARMONY (15% weight)
        # Does volume confirm price action?
        harmony_component = pd.Series(50, index=df.index, dtype=float)  # Default neutral
        
        if has_rvol and 'ret_1d' in df.columns:
            rvol = df['rvol']
            ret_1d = df['ret_1d']
            
            valid_harmony = rvol.notna() & ret_1d.notna()
            
            if valid_harmony.any():
                # Best: High volume + strong price move (either direction)
                strong_move_confirmed = valid_harmony & (rvol > 1.5) & (np.abs(ret_1d) > 3)
                harmony_component[strong_move_confirmed] = 80
                
                # Good: Above average volume + price move
                normal_confirmed = valid_harmony & (rvol > 1) & (np.abs(ret_1d) > 1)
                harmony_component[normal_confirmed] = 65
                
                # Bad: High volume + no price move (distribution/accumulation)
                high_vol_no_move = valid_harmony & (rvol > 2) & (np.abs(ret_1d) < 0.5)
                harmony_component[high_vol_no_move] = 30
                
                # Suspicious: Big price move + no volume
                move_no_volume = valid_harmony & (np.abs(ret_1d) > 5) & (rvol < 0.7)
                harmony_component[move_no_volume] = 20
                
                # Neutral: Everything else stays at 50
        
        # COMPONENT 4: VOLUME CONSISTENCY (10% weight)
        # Temporal stability - separates quality stocks from pump & dump schemes
        consistency_component = pd.Series(np.nan, index=df.index, dtype=float)
        
        vol_columns = ['volume_1d', 'volume_7d', 'volume_30d', 'volume_90d']
        available_vols = [col for col in vol_columns if col in df.columns]
        
        if len(available_vols) >= 2:
            # Collect volume data (all are already average daily volumes)
            vol_data = pd.DataFrame()
            for col in available_vols:
                vol_data[col] = pd.Series(df[col].values, index=df.index)
            
            # Calculate coefficient of variation for each stock
            valid_rows = vol_data.notna().all(axis=1)
            
            if valid_rows.any():
                # volume_7d, volume_30d, volume_90d are ALREADY average daily volumes
                # No normalization needed - they're already in daily equivalent form
                vol_mean = vol_data[valid_rows].mean(axis=1)
                vol_std = vol_data[valid_rows].std(axis=1)
                
                # Calculate CV (coefficient of variation)
                vol_cv = pd.Series(np.nan, index=df.index)
                non_zero = valid_rows & (vol_mean > 0)
                
                if non_zero.any():
                    vol_cv[non_zero] = vol_std[non_zero] / vol_mean[non_zero]
                    
                    # Score based on consistency (lower CV = higher score)
                    # CV < 0.2 = 85-100 (excellent - institutional quality)
                    # CV 0.2-0.4 = 70-85 (good - stable retail)
                    # CV 0.4-0.7 = 50-70 (moderate - some volatility)
                    # CV 0.7-1.2 = 30-50 (poor - erratic)
                    # CV > 1.2 = 10-30 (terrible - pump/dump pattern)
                    
                    consistency_component[non_zero] = np.where(
                        vol_cv[non_zero] < 0.2,
                        85 + (0.2 - vol_cv[non_zero]) * 75,  # 85-100
                        np.where(
                            vol_cv[non_zero] < 0.4,
                            70 + (0.4 - vol_cv[non_zero]) * 75,  # 70-85
                            np.where(
                                vol_cv[non_zero] < 0.7,
                                50 + (0.7 - vol_cv[non_zero]) * 66.67,  # 50-70
                                np.where(
                                    vol_cv[non_zero] < 1.2,
                                    30 + (1.2 - vol_cv[non_zero]) * 40,  # 30-50
                                    np.maximum(10, 30 - (vol_cv[non_zero] - 1.2) * 20)  # 10-30
                                )
                            )
                        )
                    )
        
        # COMBINE COMPONENTS
        components = [
            (relative_component, 0.45),  # Reduced from 0.50
            (liquidity_component, 0.30),  # Unchanged
            (harmony_component, 0.15),    # Reduced from 0.20
            (consistency_component, 0.10) # NEW
        ]
        
        for idx in df.index:
            valid_components = []
            valid_weights = []
            
            for component, weight in components:
                if pd.notna(component[idx]):
                    valid_components.append(component[idx])
                    valid_weights.append(weight)
            
            if valid_components:
                total_weight = sum(valid_weights)
                if total_weight > 0:
                    normalized_weights = [w/total_weight for w in valid_weights]
                    volume_score[idx] = sum(c * w for c, w in zip(valid_components, normalized_weights))
        
        # CONTEXT ADJUSTMENTS (Simplified)
        
        # Market cap context
        if 'category' in df.columns and volume_score.notna().any():
            category = df['category']
            
            # Penny stocks: High volume more common, reduce score
            is_penny = category.isin(['Micro Cap', 'Small Cap'])
            if has_rvol:
                rvol = df['rvol']
                penny_high = is_penny & (rvol > 3) & volume_score.notna()
                if penny_high.any():
                    volume_score[penny_high] *= 0.85
            
            # Large caps: High volume more significant
            is_large = category.isin(['Large Cap', 'Mega Cap'])
            if has_rvol:
                large_high = is_large & (rvol > 2) & volume_score.notna()
                if large_high.any():
                    volume_score[large_high] = np.minimum(
                        volume_score[large_high] * 1.1,
                        100
                    )
        
        # Extreme volume investigation
        if has_rvol:
            rvol = df['rvol']
            
            # Very extreme volume (>10x) is usually bad news
            extreme = (rvol > 10) & volume_score.notna()
            if extreme.any():
                # Check if it's with bad price action
                if 'ret_1d' in df.columns:
                    ret_1d = df['ret_1d']
                    extreme_negative = extreme & (ret_1d < -5)
                    if extreme_negative.any():
                        volume_score[extreme_negative] = np.minimum(volume_score[extreme_negative], 60)
                        logger.warning(f"Extreme volume with negative price action in {extreme_negative.sum()} stocks")
        
        # Final clipping
        volume_score = volume_score.clip(0, 100)
        
        # NEVER FILL NaN!
        
        # COMPREHENSIVE LOGGING
        valid_scores = volume_score.notna().sum()
        total_stocks = len(df)
        
        logger.info(f"Volume scores: {valid_scores}/{total_stocks} calculated")
        
        if valid_scores > 0:
            score_dist = volume_score[volume_score.notna()]
            logger.info(f"Distribution - Mean: {score_dist.mean():.1f}, "
                       f"Median: {score_dist.median():.1f}, "
                       f"Std: {score_dist.std():.1f}")
            
            # Volume categories
            if has_rvol:
                rvol = df['rvol']
                dead = (rvol < 0.5).sum()
                normal = ((rvol >= 0.5) & (rvol < 2)).sum()
                elevated = ((rvol >= 2) & (rvol < 5)).sum()
                extreme = (rvol >= 5).sum()
                
                logger.debug(f"Volume distribution: Dead={dead}, Normal={normal}, "
                            f"Elevated={elevated}, Extreme={extreme}")
            
            # Volume consistency statistics
            if consistency_component.notna().any():
                consistency_dist = consistency_component[consistency_component.notna()]
                logger.info(f"Volume Consistency - Mean: {consistency_dist.mean():.1f}, "
                           f"Median: {consistency_dist.median():.1f}")
                
                # Consistency quality distribution
                excellent = (consistency_component >= 85).sum()
                good = ((consistency_component >= 70) & (consistency_component < 85)).sum()
                moderate = ((consistency_component >= 50) & (consistency_component < 70)).sum()
                poor = ((consistency_component >= 30) & (consistency_component < 50)).sum()
                terrible = (consistency_component < 30).sum()
                
                logger.debug(f"Consistency quality: Excellent={excellent}, Good={good}, "
                            f"Moderate={moderate}, Poor={poor}, Terrible={terrible}")
        
        return volume_score
        
    @staticmethod
    def _calculate_momentum_score(df: pd.DataFrame) -> pd.Series:
        """
        Calculate momentum score with refined scaling and true volatility adjustment.
        REFINED: Better scaling calibration, proper volatility, cleaner logic.
        
        Core Philosophy:
        - Momentum = Recent price performance with quality adjustments
        - Scale based on actual market return distributions
        - True volatility measurement when possible
        - Multi-component for robustness
        
        Components:
        - 60% Raw momentum (market-calibrated scaling)
        - 20% Consistency (direction and magnitude)
        - 20% Quality (true Sharpe-like ratio)
        
        Score Range:
        - 80-100: Exceptional momentum
        - 60-80: Strong momentum
        - 40-60: Neutral momentum
        - 20-40: Weak momentum
        - 0-20: Severe negative momentum
        """
        # Initialize with NaN - NEVER fill with defaults
        momentum_score = pd.Series(np.nan, index=df.index, dtype=float)
        
        # Check data availability
        has_30d = 'ret_30d' in df.columns
        has_7d = 'ret_7d' in df.columns
        has_1d = 'ret_1d' in df.columns
        
        if not has_30d and not has_7d:
            logger.warning("No return data for momentum calculation")
            return momentum_score
        
        # COMPONENT 1: RAW MOMENTUM (60% weight)
        # Calibrated based on actual market distributions
        raw_momentum = pd.Series(np.nan, index=df.index, dtype=float)
        
        if has_30d:
            ret_30d = df['ret_30d']
            valid_30d = ret_30d.notna()
            
            if valid_30d.any():
                # REFINED SCALING: Based on empirical market data
                # Indian market monthly returns typically:
                # 95th percentile: ~25%
                # 75th percentile: ~10%
                # Median: ~1%
                # 25th percentile: ~-5%
                # 5th percentile: ~-15%
                
                # Use sigmoid with market-calibrated center and scale
                # Center at 1% (typical median), scale for proper spread
                raw_momentum[valid_30d] = 50 / (1 + np.exp(-(ret_30d[valid_30d] - 1) / 10))
                
                # Enhance extremes (non-linear at tails)
                very_strong = valid_30d & (ret_30d > 30)
                if very_strong.any():
                    raw_momentum[very_strong] = 90 + np.tanh((ret_30d[very_strong] - 30) / 20) * 10
                
                very_weak = valid_30d & (ret_30d < -20)
                if very_weak.any():
                    raw_momentum[very_weak] = 10 * (1 + ret_30d[very_weak] / 20)
        
        # Fallback to 7-day (properly scaled)
        needs_7d = raw_momentum.isna() & has_7d
        if needs_7d.any() and has_7d:
            ret_7d = df['ret_7d']
            valid_7d = ret_7d.notna() & needs_7d
            
            if valid_7d.any():
                # Convert to monthly equivalent using compound rate
                # (1 + r_monthly) = (1 + r_weekly)^4.3
                monthly_equivalent = np.sign(ret_7d[valid_7d]) * (
                    (np.abs(1 + ret_7d[valid_7d]/100) ** 4.3 - 1) * 100
                )
                
                # Apply same sigmoid scaling
                raw_momentum[valid_7d] = 50 / (1 + np.exp(-(monthly_equivalent - 1) / 10))
        
        # COMPONENT 2: CONSISTENCY FACTOR (20% weight)
        # Improved measurement of momentum consistency
        consistency_factor = pd.Series(50, index=df.index, dtype=float)
        
        if all([has_1d, has_7d, has_30d]):
            ret_1d = df['ret_1d']
            ret_7d = df['ret_7d']
            ret_30d = df['ret_30d']
            
            valid_all = ret_1d.notna() & ret_7d.notna() & ret_30d.notna()
            
            if valid_all.any():
                # Calculate momentum alignment score
                # All positive with increasing strength = best
                # Mixed signals = neutral
                # All negative with worsening = worst
                
                # Sign alignment (direction consistency)
                signs = np.sign(np.column_stack([ret_1d[valid_all], 
                                                ret_7d[valid_all], 
                                                ret_30d[valid_all]]))
                sign_consistency = np.abs(signs.sum(axis=1)) / 3  # 0 to 1
                
                # Magnitude progression (is momentum building?)
                daily_equiv_7d = ret_7d[valid_all] / 7
                daily_equiv_30d = ret_30d[valid_all] / 30
                
                building = (ret_1d[valid_all] > daily_equiv_7d) & (daily_equiv_7d > daily_equiv_30d)
                steady = np.abs(ret_1d[valid_all] - daily_equiv_7d) < 2
                fading = (ret_1d[valid_all] < daily_equiv_7d) & (daily_equiv_7d < daily_equiv_30d)
                
                # Combine sign and magnitude
                consistency_factor[valid_all] = 50  # Base
                consistency_factor[valid_all] += sign_consistency * 20  # ±20 for direction
                consistency_factor[building] += 20  # Bonus for building
                consistency_factor[fading] -= 20  # Penalty for fading
                consistency_factor[steady] += 10  # Small bonus for steady
        
        # COMPONENT 3: QUALITY FACTOR (20% weight)
        # True Sharpe-like ratio using actual volatility
        quality_factor = pd.Series(50, index=df.index, dtype=float)
        
        # Try to use actual daily data if available
        if 'daily_returns' in df.columns:
            # If we have actual daily returns series
            daily_returns = df['daily_returns']
            valid_daily = daily_returns.notna()
            
            if valid_daily.any():
                # Calculate true volatility
                volatility = daily_returns[valid_daily].apply(lambda x: np.std(x) if len(x) > 1 else np.nan)
                returns = df.loc[valid_daily, 'ret_30d'] if has_30d else df.loc[valid_daily, 'ret_7d']
                
                # Sharpe-like ratio
                sharpe = returns / (volatility * np.sqrt(30) + 1)  # Annualized
                quality_factor[valid_daily] = 50 + np.tanh(sharpe / 2) * 30
        else:
            # Fallback: Estimate volatility from available returns
            if has_30d and has_7d and has_1d:
                ret_30d = df['ret_30d']
                ret_7d = df['ret_7d']
                ret_1d = df['ret_1d']
                
                valid_vol = ret_30d.notna() & ret_7d.notna() & ret_1d.notna()
                
                if valid_vol.any():
                    # Estimate volatility from return dispersion
                    # Better than std of averages
                    expected_1d = ret_30d[valid_vol] / 30
                    expected_7d = ret_30d[valid_vol] * 7 / 30
                    
                    deviation_1d = np.abs(ret_1d[valid_vol] - expected_1d)
                    deviation_7d = np.abs(ret_7d[valid_vol] - expected_7d)
                    
                    # Average deviation as volatility proxy
                    avg_deviation = (deviation_1d + deviation_7d / 7) / 2
                    
                    # Quality score based on return/risk
                    risk_adjusted = ret_30d[valid_vol] / (avg_deviation + 1)
                    quality_factor[valid_vol] = 50 + np.tanh(risk_adjusted / 10) * 30
        
        # COMBINE COMPONENTS
        has_components = raw_momentum.notna()
        
        if has_components.any():
            # Start with raw momentum
            momentum_score[has_components] = raw_momentum[has_components] * 0.6
            
            # Add consistency if available
            if consistency_factor.notna().any():
                valid_both = has_components & consistency_factor.notna()
                momentum_score[valid_both] += consistency_factor[valid_both] * 0.2
            else:
                # If no consistency data, give more weight to raw
                momentum_score[has_components] = raw_momentum[has_components] * 0.8
            
            # Add quality if available
            if quality_factor.notna().any():
                valid_all = has_components & quality_factor.notna()
                momentum_score[valid_all] += quality_factor[valid_all] * 0.2
        
        # CONTEXT ADJUSTMENTS
        
        # Market cap adjustments
        if 'category' in df.columns and momentum_score.notna().any():
            category = df['category']
            
            # Small cap pump & dump detection
            is_small = category.isin(['Micro Cap', 'Small Cap'])
            
            if has_30d:
                ret_30d = df['ret_30d']
                
                # Progressive penalty for extreme returns - More realistic threshold
                extreme_small = is_small & momentum_score.notna() & (ret_30d > 35)  # Reduced from 50%
                if extreme_small.any():
                    penalty_factor = np.exp(-(ret_30d[extreme_small] - 35) / 35)  # Adjusted calculation
                    momentum_score[extreme_small] *= penalty_factor
                    logger.debug(f"Applied pump penalty to {extreme_small.sum()} small caps")
            
            # Large cap momentum significance
            is_large = category.isin(['Large Cap', 'Mega Cap'])
            strong_large = is_large & momentum_score.notna() & (momentum_score > 70)
            if strong_large.any():
                momentum_score[strong_large] = np.minimum(
                    momentum_score[strong_large] * 1.08,
                    100
                )
        
        # Volume confirmation
        if 'rvol' in df.columns and momentum_score.notna().any():
            rvol = df['rvol']
            
            # Momentum-volume harmony
            high_momentum = momentum_score > 70
            
            # Strong momentum with volume = confirmed
            with_volume = high_momentum & (rvol > 1.5) & momentum_score.notna()
            if with_volume.any():
                momentum_score[with_volume] = np.minimum(
                    momentum_score[with_volume] + 5,
                    100
                )
            
            # Strong momentum without volume = suspicious
            no_volume = high_momentum & (rvol < 0.7) & momentum_score.notna()
            if no_volume.any():
                momentum_score[no_volume] -= 10
        
        # Final clipping
        momentum_score = momentum_score.clip(0, 100)
        
        # NEVER FILL NaN!
        
        # COMPREHENSIVE LOGGING
        valid_scores = momentum_score.notna().sum()
        total_stocks = len(df)
        
        logger.info(f"Momentum scores: {valid_scores}/{total_stocks} calculated")
        
        if valid_scores > 0:
            score_dist = momentum_score[momentum_score.notna()]
            logger.info(f"Distribution - Mean: {score_dist.mean():.1f}, "
                       f"Median: {score_dist.median():.1f}, "
                       f"Std: {score_dist.std():.1f}")
            
            # Check market momentum
            if score_dist.mean() > 65:
                logger.info("Strong market-wide momentum detected")
            elif score_dist.mean() < 35:
                logger.warning("Weak market-wide momentum detected")
        
        return momentum_score
        
    @staticmethod
    def _calculate_acceleration_score(df: pd.DataFrame) -> pd.Series:
        """
        Calculate momentum acceleration using proper rate of change analysis.
        FIXED: Correct math, no division issues, clear scoring logic.
        
        Core Concept:
        - Acceleration = Change in momentum over time
        - Compare recent vs historical momentum rates
        - Use log returns for proper compounding
        - Smooth transitions, no arbitrary bins
        
        Method:
        - Calculate momentum over different periods
        - Compare short vs long momentum slopes
        - Score based on momentum improvement/deterioration
        
        Score Range:
        - 80-100: Strong positive acceleration (momentum building rapidly)
        - 60-80: Moderate acceleration (steady improvement)
        - 40-60: Neutral (constant momentum)
        - 20-40: Deceleration (momentum fading)
        - 0-20: Strong deceleration (momentum collapsing)
        """
        # Initialize with NaN
        acceleration_score = pd.Series(np.nan, index=df.index, dtype=float)
        
        # Check minimum required columns
        required_cols = ['ret_1d', 'ret_7d', 'ret_30d']
        available_cols = [col for col in required_cols if col in df.columns]
        
        if len(available_cols) < 2:
            logger.warning(f"Insufficient data for acceleration: only {len(available_cols)} return columns")
            return acceleration_score
        
        # METHOD 1: Momentum Slope Comparison
        # Calculate momentum at different time points and compare slopes
        
        momentum_scores = pd.DataFrame(index=df.index)
        
        # Current momentum (most recent)
        if 'ret_7d' in df.columns:
            # Weekly momentum as baseline
            momentum_scores['current'] = df['ret_7d']
        
        # Historical momentum (for comparison)
        if 'ret_30d' in df.columns:
            # Monthly momentum
            momentum_scores['historical'] = df['ret_30d']
        
        # Very short-term momentum
        if 'ret_1d' in df.columns:
            momentum_scores['immediate'] = df['ret_1d']
        
        # Calculate acceleration as momentum improvement
        valid_data = pd.Series(False, index=df.index)
        
        # Primary calculation: 7d vs 30d momentum
        if 'ret_7d' in df.columns and 'ret_30d' in df.columns:
            ret_7d = df['ret_7d']
            ret_30d = df['ret_30d']
            
            valid = ret_7d.notna() & ret_30d.notna()
            valid_data |= valid
            
            if valid.any():
                # Calculate average daily momentum for each period
                # Use compound rate for accuracy: (1 + r)^(1/n) - 1
                
                # Safe calculation avoiding negative final values
                safe_7d = np.where(ret_7d > -99, ret_7d, -99)
                safe_30d = np.where(ret_30d > -99, ret_30d, -99)
                
                # Daily equivalent rates
                daily_7d = np.sign(safe_7d) * (np.abs(1 + safe_7d/100) ** (1/7) - 1) * 100
                daily_30d = np.sign(safe_30d) * (np.abs(1 + safe_30d/100) ** (1/30) - 1) * 100
                
                # Acceleration factor: recent momentum vs historical
                # Positive = accelerating, Negative = decelerating
                momentum_change = daily_7d - daily_30d
                
                # Convert to 0-100 score using sigmoid-like function
                # Center at 0 (no acceleration) = 50 score
                # Use tanh for smooth transitions
                acceleration_score[valid] = 50 + 30 * np.tanh(momentum_change[valid] / 5)
        
        # Enhanced calculation with 1-day data
        if 'ret_1d' in df.columns and 'ret_7d' in df.columns:
            ret_1d = df['ret_1d']
            ret_7d = df['ret_7d']
            
            valid_short = ret_1d.notna() & ret_7d.notna()
            
            if valid_short.any():
                # Very short-term acceleration
                daily_7d_rate = np.sign(ret_7d) * (np.abs(1 + ret_7d/100) ** (1/7) - 1) * 100
                
                # Compare today vs weekly average
                short_acceleration = ret_1d - daily_7d_rate
                
                # Blend with main score if available
                if acceleration_score.notna().any():
                    valid_blend = valid_short & acceleration_score.notna()
                    # 70% weight on primary, 30% on short-term
                    acceleration_score[valid_blend] = (
                        acceleration_score[valid_blend] * 0.7 +
                        (50 + 30 * np.tanh(short_acceleration[valid_blend] / 3)) * 0.3
                    )
                else:
                    # Use short-term as primary if no other data
                    acceleration_score[valid_short] = 50 + 30 * np.tanh(short_acceleration[valid_short] / 3)
                    valid_data |= valid_short
        
        # METHOD 2: Momentum Consistency Bonus
        # Reward consistent acceleration across timeframes
        if all(col in df.columns for col in ['ret_1d', 'ret_7d', 'ret_30d']):
            ret_1d = df['ret_1d']
            ret_7d = df['ret_7d']  
            ret_30d = df['ret_30d']
            
            valid_all = ret_1d.notna() & ret_7d.notna() & ret_30d.notna() & acceleration_score.notna()
            
            if valid_all.any():
                # Check for consistent improvement pattern
                # Each period better than the longer one (normalized)
                improving = (
                    (ret_1d > ret_7d / 7) & 
                    (ret_7d / 7 > ret_30d / 30) &
                    (ret_1d > 0)
                )
                
                # Add bonus for consistent acceleration
                consistent_accel = valid_all & improving
                if consistent_accel.any():
                    acceleration_score[consistent_accel] = np.minimum(
                        acceleration_score[consistent_accel] + 10,
                        95
                    )
                
                # Penalty for consistent deceleration
                declining = (
                    (ret_1d < ret_7d / 7) & 
                    (ret_7d / 7 < ret_30d / 30) &
                    (ret_1d < 0)
                )
                
                consistent_decel = valid_all & declining
                if consistent_decel.any():
                    acceleration_score[consistent_decel] = np.maximum(
                        acceleration_score[consistent_decel] - 10,
                        5
                    )
        
        # CONTEXT ADJUSTMENTS
        
        # Volume confirmation
        if 'rvol' in df.columns and acceleration_score.notna().any():
            rvol = df['rvol']
            
            # Strong acceleration needs volume
            strong_accel = (acceleration_score > 70) & rvol.notna()
            
            # With volume = confirmed
            with_volume = strong_accel & (rvol > 1.5)
            if with_volume.any():
                acceleration_score[with_volume] = np.minimum(
                    acceleration_score[with_volume] * 1.05,
                    100
                )
            
            # Without volume = suspicious
            no_volume = strong_accel & (rvol < 0.8)
            if no_volume.any():
                acceleration_score[no_volume] *= 0.90
        
        # Market cap adjustment
        if 'category' in df.columns and acceleration_score.notna().any():
            category = df['category']
            
            # Small caps: More volatile, normalize extremes
            is_small = category.isin(['Micro Cap', 'Small Cap'])
            small_mask = is_small & acceleration_score.notna()
            
            if small_mask.any():
                # Pull extremes toward center
                acceleration_score[small_mask] = 50 + (acceleration_score[small_mask] - 50) * 0.8
            
            # Large caps: Acceleration more significant
            is_large = category.isin(['Large Cap', 'Mega Cap'])
            large_accel = is_large & (acceleration_score > 65) & acceleration_score.notna()
            
            if large_accel.any():
                acceleration_score[large_accel] = np.minimum(
                    acceleration_score[large_accel] * 1.05,
                    100
                )
        
        # Handle edge cases
        # Extreme positive returns might break calculations
        if 'ret_30d' in df.columns:
            extreme_returns = (df['ret_30d'] > 200) & acceleration_score.notna()
            if extreme_returns.any():
                # Cap acceleration score for extreme movers
                acceleration_score[extreme_returns] = np.minimum(
                    acceleration_score[extreme_returns],
                    85
                )
        
        # Final clipping
        acceleration_score = acceleration_score.clip(0, 100)
        
        # COMPREHENSIVE LOGGING
        valid_scores = acceleration_score.notna().sum()
        total_stocks = len(df)
        
        logger.info(f"Acceleration scores: {valid_scores}/{total_stocks} calculated")
        
        if valid_scores > 0:
            score_dist = acceleration_score[acceleration_score.notna()]
            logger.info(f"Distribution - Mean: {score_dist.mean():.1f}, "
                       f"Median: {score_dist.median():.1f}, "
                       f"Std: {score_dist.std():.1f}")
            
            # Categories
            strong_accel = (acceleration_score > 80).sum()
            moderate_accel = ((acceleration_score > 60) & (acceleration_score <= 80)).sum()
            neutral = ((acceleration_score >= 40) & (acceleration_score <= 60)).sum()
            moderate_decel = ((acceleration_score >= 20) & (acceleration_score < 40)).sum()
            strong_decel = (acceleration_score < 20).sum()
            
            logger.debug(f"Acceleration breakdown: Strong Accel={strong_accel}, "
                        f"Moderate Accel={moderate_accel}, Neutral={neutral}, "
                        f"Moderate Decel={moderate_decel}, Strong Decel={strong_decel}")
            
            # Check for market-wide acceleration
            if score_dist.mean() > 65:
                logger.info("Market-wide positive acceleration detected")
            elif score_dist.mean() < 35:
                logger.warning("Market-wide deceleration detected")
        
        return acceleration_score
        
    @staticmethod
    def _calculate_breakout_score(df: pd.DataFrame) -> pd.Series:
        """
        Calculate breakout probability using FULLY VECTORIZED operations.
        FIXED: No loops, consistent weighting, proper breakout patterns.
        
        Breakout Methodology:
        - Distance from 52w high with exponential weighting
        - Volume accumulation patterns (not just surge)
        - Trend strength with proper SMA hierarchy
        - Breakout pattern recognition (cup & handle, flag, consolidation)
        - Volume-price confirmation
        
        Score Components:
        - 35% Distance from high (exponential decay)
        - 30% Volume pattern (accumulation vs distribution)
        - 20% Trend alignment (SMA structure)
        - 15% Price consolidation (volatility compression)
        
        Bonuses Applied:
        - Cup & Handle: +15 points
        - Flag Pattern: +12 points
        - Ascending Triangle: +10 points
        - Volume Dry-up: +8 points (pre-breakout signal)
        """
        breakout_score = pd.Series(np.nan, index=df.index, dtype=float)
        
        # Component 1: DISTANCE FROM HIGH (35% weight)
        # Uses exponential decay - being at high is exponentially better than being far
        distance_component = pd.Series(50, index=df.index, dtype=float)
        
        if 'from_high_pct' in df.columns:
            from_high = pd.Series(df['from_high_pct'].values, index=df.index)
            valid_high = from_high.notna()
            
            if valid_high.any():
                # from_high is negative (e.g., -5 means 5% below high)
                # Convert to positive distance for calculation
                distance = -from_high[valid_high]
                
                # Exponential scoring: Closer to high = exponentially better
                # Using tanh for smooth S-curve that handles all ranges well
                # Distance 0% = 100, 5% = 85, 10% = 70, 20% = 45, 50% = 10
                distance_component[valid_high] = 100 * (1 - np.tanh(distance / 20))
                
                # Special handling for stocks ABOVE 52w high (breakout already happened)
                above_high = valid_high & (from_high > 0)
                if above_high.any():
                    # Recent breakout (0-10% above) = Good
                    recent_breakout = above_high & (from_high <= 10)
                    distance_component[recent_breakout] = 90 - from_high[recent_breakout] * 2  # 90 to 70
                    
                    # Extended breakout (10-30% above) = Careful
                    extended = above_high & (from_high > 10) & (from_high <= 30)
                    distance_component[extended] = 70 - (from_high[extended] - 10) * 1.5  # 70 to 40
                    
                    # Overextended (>30% above) = Dangerous
                    overextended = above_high & (from_high > 30)
                    distance_component[overextended] = 30  # Flat low score
        
        # Component 2: VOLUME PATTERN (30% weight)
        # Not just current volume, but pattern over time
        volume_component = pd.Series(50, index=df.index, dtype=float)
        
        # Sub-component 2a: Volume trend (is volume building?)
        if all(col in df.columns for col in ['vol_ratio_1d_90d', 'vol_ratio_7d_90d', 'vol_ratio_30d_90d']):
            vol_1d = pd.Series(df['vol_ratio_1d_90d'].values, index=df.index)
            vol_7d = pd.Series(df['vol_ratio_7d_90d'].values, index=df.index)
            vol_30d = pd.Series(df['vol_ratio_30d_90d'].values, index=df.index)
            
            valid_vol = vol_1d.notna() & vol_7d.notna() & vol_30d.notna()
            
            if valid_vol.any():
                # Calculate volume progression score
                # Best: Steady increase (30d < 7d < 1d)
                volume_progression = pd.Series(50, index=df.index, dtype=float)
                
                # Accumulation pattern: Volume building over time
                accumulation = valid_vol & (vol_1d > vol_7d) & (vol_7d > vol_30d)
                volume_progression[accumulation] = 70 + (vol_1d[accumulation] - 1) * 15  # 70-100
                
                # Steady volume: Consistent across timeframes
                steady = valid_vol & (np.abs(vol_1d - vol_7d) < 0.3) & (np.abs(vol_7d - vol_30d) < 0.3)
                volume_progression[steady] = 60
                
                # Distribution pattern: Volume decreasing (bearish for breakout)
                distribution = valid_vol & (vol_1d < vol_7d) & (vol_7d < vol_30d)
                volume_progression[distribution] = 30
                
                # Spike only: Just 1-day spike, no build-up (unreliable)
                spike_only = valid_vol & (vol_1d > 2) & (vol_7d < 1.2) & (vol_30d < 1.1)
                volume_progression[spike_only] = 40
                
                volume_component[valid_vol] = volume_progression[valid_vol]
        
        # Sub-component 2b: Volume dry-up detection (often precedes breakout)
        if 'rvol' in df.columns and 'from_high_pct' in df.columns:
            rvol = pd.Series(df['rvol'].values, index=df.index)
            from_high = pd.Series(df['from_high_pct'].values, index=df.index)
            
            # Volume dry-up near resistance = spring loading
            volume_dryup = (
                rvol.notna() & 
                from_high.notna() &
                (rvol < 0.7) &  # Low current volume
                (from_high > -10) & (from_high <= 0)  # Near high
            )
            if volume_dryup.any():
                # This is actually bullish - like a coiled spring
                volume_component[volume_dryup] = 65
        
        # Component 3: TREND ALIGNMENT (20% weight)
        # SMA structure with proper weighting
        trend_component = pd.Series(50, index=df.index, dtype=float)
        
        if 'price' in df.columns:
            price = pd.Series(df['price'].values, index=df.index)
            price_valid = price.notna() & (price > 0)
            
            if price_valid.any():
                trend_score = pd.Series(0, index=df.index, dtype=float)
                max_possible = pd.Series(0, index=df.index, dtype=float)
                
                # SMA hierarchy: 200 > 50 > 20 in importance for breakouts
                sma_weights = {
                    'sma_200d': 50,  # Most important - long-term trend
                    'sma_50d': 30,   # Medium-term trend
                    'sma_20d': 20    # Short-term trend
                }
                
                for sma_col, weight in sma_weights.items():
                    if sma_col in df.columns:
                        sma_values = pd.Series(df[sma_col].values, index=df.index)
                        valid_sma = price_valid & sma_values.notna() & (sma_values > 0)
                        
                        # Price above SMA
                        above_sma = valid_sma & (price > sma_values)
                        trend_score[above_sma] += weight
                        max_possible[valid_sma] += weight
                
                # Normalize to 0-100
                has_sma = max_possible > 0
                trend_component[has_sma] = (trend_score[has_sma] / max_possible[has_sma]) * 100
                
                # Golden alignment bonus: Price > SMA20 > SMA50 > SMA200
                if all(col in df.columns for col in ['sma_20d', 'sma_50d', 'sma_200d']):
                    sma_20 = pd.Series(df['sma_20d'].values, index=df.index)
                    sma_50 = pd.Series(df['sma_50d'].values, index=df.index)
                    sma_200 = pd.Series(df['sma_200d'].values, index=df.index)
                    
                    golden_alignment = (
                        price_valid & 
                        sma_20.notna() & sma_50.notna() & sma_200.notna() &
                        (price > sma_20) & (sma_20 > sma_50) & (sma_50 > sma_200)
                    )
                    if golden_alignment.any():
                        trend_component[golden_alignment] = 100  # Perfect score
        
        # Component 4: PRICE CONSOLIDATION (15% weight)
        # Tight consolidation near high = higher breakout probability
        consolidation_component = pd.Series(50, index=df.index, dtype=float)
        
        if all(col in df.columns for col in ['ret_7d', 'ret_30d', 'from_high_pct']):
            ret_7d = pd.Series(df['ret_7d'].values, index=df.index)
            ret_30d = pd.Series(df['ret_30d'].values, index=df.index)
            from_high = pd.Series(df['from_high_pct'].values, index=df.index)
            
            valid_consol = ret_7d.notna() & ret_30d.notna() & from_high.notna()
            
            if valid_consol.any():
                # Calculate volatility/range
                volatility = np.abs(ret_7d)
                
                # Tight consolidation near high (best setup)
                tight_consol = valid_consol & (volatility < 3) & (from_high > -10) & (from_high <= 0)
                consolidation_component[tight_consol] = 85
                
                # Normal consolidation
                normal_consol = valid_consol & (volatility >= 3) & (volatility < 7) & (from_high > -20)
                consolidation_component[normal_consol] = 60
                
                # Wide/volatile consolidation (less reliable)
                wide_consol = valid_consol & (volatility >= 7)
                consolidation_component[wide_consol] = 35
        
        # COMBINE ALL COMPONENTS (with fixed weights)
        # Calculate weighted average only where we have data
        components = {
            'distance': (distance_component, 0.35),
            'volume': (volume_component, 0.30),
            'trend': (trend_component, 0.20),
            'consolidation': (consolidation_component, 0.15)
        }
        
        # Vectorized combination
        weighted_sum = pd.Series(0, index=df.index, dtype=float)
        weight_sum = pd.Series(0, index=df.index, dtype=float)
        
        for name, (component, weight) in components.items():
            valid = component.notna() & (component != 50)  # 50 is default, means no data
            weighted_sum[valid] += component[valid] * weight
            weight_sum[valid] += weight
        
        # Calculate final score
        has_data = weight_sum > 0
        breakout_score[has_data] = weighted_sum[has_data] / weight_sum[has_data]
        
        # PATTERN RECOGNITION BONUSES
        # These are additive bonuses for specific bullish patterns
        
        # Pattern 1: Cup & Handle
        if all(col in df.columns for col in ['ret_3m', 'ret_1y', 'ret_30d', 'from_high_pct']):
            ret_3m = pd.Series(df['ret_3m'].values, index=df.index)
            ret_1y = pd.Series(df['ret_1y'].values, index=df.index)
            ret_30d = pd.Series(df['ret_30d'].values, index=df.index)
            from_high = pd.Series(df['from_high_pct'].values, index=df.index)
            
            cup_handle = (
                breakout_score.notna() &
                ret_1y.notna() & ret_3m.notna() & ret_30d.notna() & from_high.notna() &
                (ret_1y > 20) &  # Good yearly performance (left side of cup)
                (ret_3m < 10) &  # Consolidation (bottom of cup)
                (ret_30d > -5) & (ret_30d < 5) &  # Handle formation
                (from_high > -15) & (from_high <= 0)  # Near resistance
            )
            if cup_handle.any():
                breakout_score[cup_handle] += 15
                logger.debug(f"Cup & Handle pattern detected in {cup_handle.sum()} stocks")
        
        # Pattern 2: Flag Pattern (consolidation after strong move)
        if all(col in df.columns for col in ['ret_30d', 'ret_7d', 'from_high_pct', 'rvol']):
            ret_30d = pd.Series(df['ret_30d'].values, index=df.index)
            ret_7d = pd.Series(df['ret_7d'].values, index=df.index)
            from_high = pd.Series(df['from_high_pct'].values, index=df.index)
            rvol = pd.Series(df['rvol'].values, index=df.index)
            
            flag_pattern = (
                breakout_score.notna() &
                ret_30d.notna() & ret_7d.notna() & from_high.notna() & rvol.notna() &
                (ret_30d > 15) &  # Strong prior move (pole)
                (ret_7d > -3) & (ret_7d < 3) &  # Tight consolidation (flag)
                (from_high > -8) & (from_high <= 0) &  # Near high
                (rvol < 1.5)  # Volume contraction during flag
            )
            if flag_pattern.any():
                breakout_score[flag_pattern] += 12
                logger.debug(f"Flag pattern detected in {flag_pattern.sum()} stocks")
        
        # Pattern 3: Ascending Triangle (higher lows, same highs)
        if all(col in df.columns for col in ['low_52w', 'high_52w', 'price', 'from_high_pct']):
            low_52w = pd.Series(df['low_52w'].values, index=df.index)
            high_52w = pd.Series(df['high_52w'].values, index=df.index)
            price = pd.Series(df['price'].values, index=df.index)
            from_high = pd.Series(df['from_high_pct'].values, index=df.index)
            
            # Check if lows are rising while testing same high
            valid_triangle = (
                low_52w.notna() & high_52w.notna() & 
                price.notna() & from_high.notna() & 
                (low_52w > 0) & (high_52w > 0)
            )
            
            if valid_triangle.any():
                # Price well above 52w low but struggling at high
                from_low_ratio = (price - low_52w) / low_52w * 100
                ascending_triangle = (
                    valid_triangle &
                    (from_low_ratio > 30) &  # Well above low
                    (from_high > -5) & (from_high <= 0) &  # Testing high repeatedly
                    breakout_score.notna()
                )
                if ascending_triangle.any():
                    breakout_score[ascending_triangle] += 10
                    logger.debug(f"Ascending triangle detected in {ascending_triangle.sum()} stocks")
        
        # Pattern 4: Volatility Contraction Pattern (VCP)
        if 'ret_3d' in df.columns and 'ret_7d' in df.columns and 'ret_30d' in df.columns:
            ret_3d = pd.Series(df['ret_3d'].values, index=df.index)
            ret_7d = pd.Series(df['ret_7d'].values, index=df.index)
            ret_30d = pd.Series(df['ret_30d'].values, index=df.index)
            
            # Volatility decreasing over time
            vol_3d = np.abs(ret_3d)
            vol_7d = np.abs(ret_7d)
            vol_30d = np.abs(ret_30d)
            
            vcp_pattern = (
                breakout_score.notna() &
                vol_3d.notna() & vol_7d.notna() & vol_30d.notna() &
                (vol_3d < vol_7d * 0.7) &  # Recent volatility much lower
                (vol_7d < vol_30d * 0.8) &  # Progressive contraction
                (vol_3d < 3)  # Very tight recent range
            )
            if vcp_pattern.any():
                breakout_score[vcp_pattern] += 8
                logger.debug(f"VCP pattern detected in {vcp_pattern.sum()} stocks")
        
        # CONTEXT ADJUSTMENTS
        
        # Market cap adjustment
        if 'category' in df.columns and breakout_score.notna().any():
            category = pd.Series(df['category'].values, index=df.index)
            
            # Large cap breakouts are more reliable
            large_cap = category.isin(['Large Cap', 'Mega Cap'])
            large_breakout = large_cap & breakout_score.notna() & (breakout_score > 70)
            if large_breakout.any():
                breakout_score[large_breakout] *= 1.05
                logger.debug(f"Large cap breakout bonus applied to {large_breakout.sum()} stocks")
            
            # Small cap breakouts need stronger confirmation
            small_cap = category.isin(['Small Cap', 'Micro Cap'])
            small_breakout = small_cap & breakout_score.notna() & (breakout_score > 60)
            if small_breakout.any():
                # Require volume confirmation for small caps
                if 'rvol' in df.columns:
                    rvol = pd.Series(df['rvol'].values, index=df.index)
                    needs_volume = small_breakout & (rvol < 1.5)
                    breakout_score[needs_volume] *= 0.85  # Penalty without volume
        
        # Sector momentum adjustment
        if 'sector' in df.columns and 'ret_30d' in df.columns:
            # If sector is strong, individual breakouts more likely
            sector_returns = df.groupby('sector')['ret_30d'].transform('mean')
            strong_sector = sector_returns > 10  # Sector up >10% in month
            
            sector_boost = strong_sector & breakout_score.notna() & (breakout_score > 60)
            if sector_boost.any():
                breakout_score[sector_boost] *= 1.03
                logger.debug(f"Strong sector bonus applied to {sector_boost.sum()} stocks")
        
        # Final clipping
        breakout_score = breakout_score.clip(0, 100)
        
        # Fill remaining NaN with default
        still_nan = breakout_score.isna()
        if still_nan.any():
            # Check if they have any price data at all
            has_price = 'price' in df.columns and df['price'].notna()
            breakout_score[still_nan & has_price] = 40  # Below average default
            breakout_score[still_nan & ~has_price] = np.nan  # Keep NaN if no data
        
        # COMPREHENSIVE LOGGING
        valid_scores = breakout_score.notna().sum()
        if valid_scores > 0:
            logger.info(f"Breakout scores calculated: {valid_scores} valid out of {len(df)} stocks")
            
            # Distribution
            score_dist = breakout_score[breakout_score.notna()]
            logger.info(f"Score distribution - Min: {score_dist.min():.1f}, "
                       f"Max: {score_dist.max():.1f}, "
                       f"Mean: {score_dist.mean():.1f}, "
                       f"Median: {score_dist.median():.1f}")
            
            # Breakout categories
            imminent = (breakout_score > 80).sum()
            probable = ((breakout_score > 65) & (breakout_score <= 80)).sum()
            possible = ((breakout_score > 50) & (breakout_score <= 65)).sum()
            unlikely = (breakout_score <= 50).sum()
            
            logger.debug(f"Breakout probability: Imminent={imminent}, Probable={probable}, "
                        f"Possible={possible}, Unlikely={unlikely}")
            
            # Pattern detection summary
            if 'from_high_pct' in df.columns:
                near_high = ((df['from_high_pct'] > -5) & (df['from_high_pct'] <= 0)).sum()
                above_high = (df['from_high_pct'] > 0).sum()
                logger.debug(f"Position: {near_high} near 52w high, {above_high} above high")
        
        return breakout_score
    
    @staticmethod
    def _calculate_rvol_score(df: pd.DataFrame) -> pd.Series:
        """
        Calculate RVOL score using smooth logarithmic scaling.
        FIXED: No category overrides, proper logarithmic curve, context-aware.
        
        Core Philosophy:
        - RVOL is exponential by nature - needs log scaling
        - NO discrete categories that create jumps
        - Higher volume should never score lower (no inversions!)
        - Context determines if high volume is good or bad
        
        Score Formula:
        - Below normal (0-1x): Linear scaling 0-50
        - Above normal (1x+): Logarithmic scaling 50-100
        - Context adjustments for price action and market cap
        
        Score Range:
        - 85-100: Extreme volume (major event/news)
        - 70-85: High volume (strong interest)
        - 50-70: Elevated volume (above average)
        - 30-50: Normal range (typical)
        - 10-30: Low volume (lack of interest)
        - 0-10: Dead volume (no activity)
        """
        # Initialize with NaN
        rvol_score = pd.Series(np.nan, index=df.index, dtype=float)
        
        if 'rvol' not in df.columns:
            logger.warning("RVOL data not available")
            return rvol_score  # Return NaN, not default!
        
        rvol = df['rvol']
        valid_rvol = rvol.notna() & (rvol >= 0)
        
        if not valid_rvol.any():
            logger.warning("No valid RVOL data found")
            return rvol_score
        
        # BASE SCORE: Smooth continuous function (NO CATEGORIES!)
        
        # Handle zero volume separately
        zero_vol = valid_rvol & (rvol == 0)
        if zero_vol.any():
            rvol_score[zero_vol] = 0  # Dead stocks
        
        # Below normal volume (0 < RVOL < 1)
        below_normal = valid_rvol & (rvol > 0) & (rvol < 1)
        if below_normal.any():
            # Linear scaling: 0.1x = 5, 0.5x = 25, 0.9x = 45
            rvol_score[below_normal] = rvol[below_normal] * 50
        
        # Normal to elevated (RVOL >= 1)
        above_normal = valid_rvol & (rvol >= 1)
        if above_normal.any():
            # Logarithmic scaling for smooth progression
            # log(1) = 0 → 50
            # log(2) = 0.69 → 65
            # log(5) = 1.61 → 75
            # log(10) = 2.30 → 82
            # log(20) = 3.00 → 87
            # log(50) = 3.91 → 92
            
            # Using natural log with adjusted multiplier
            log_rvol = np.log(rvol[above_normal])
            
            # Smooth formula that never decreases
            rvol_score[above_normal] = 50 + 30 * (1 - np.exp(-log_rvol * 0.7))
            
            # Cap at 95 for extreme values
            rvol_score[above_normal] = rvol_score[above_normal].clip(50, 95)
        
        # CONTEXT LAYER 1: Price Action Harmony
        # Volume means different things with different price action
        if 'ret_1d' in df.columns and rvol_score.notna().any():
            ret_1d = df['ret_1d']
            valid_context = valid_rvol & ret_1d.notna() & rvol_score.notna()
            
            if valid_context.any():
                # High volume + big move = confirmation (good)
                strong_move = valid_context & (rvol > 2) & (ret_1d.abs() > 5)
                if strong_move.any():
                    rvol_score[strong_move] = np.minimum(rvol_score[strong_move] * 1.1, 100)
                
                # High volume + no move = distribution (bad)
                no_move = valid_context & (rvol > 2) & (ret_1d.abs() < 1)
                if no_move.any():
                    rvol_score[no_move] *= 0.8
                
                # Low volume + big move = suspicious
                suspicious = valid_context & (rvol < 0.5) & (ret_1d.abs() > 10)
                if suspicious.any():
                    rvol_score[suspicious] *= 0.7
        
        # CONTEXT LAYER 2: Sustained vs Spike
        if all(col in df.columns for col in ['vol_ratio_7d_90d', 'vol_ratio_30d_90d']):
            vol_7d = df['vol_ratio_7d_90d']
            vol_30d = df['vol_ratio_30d_90d']
            
            valid_sustain = valid_rvol & vol_7d.notna() & vol_30d.notna() & rvol_score.notna()
            
            if valid_sustain.any():
                # Sustained elevation (building volume) = better
                sustained = valid_sustain & (rvol > 2) & (vol_7d > 1.5) & (vol_30d > 1.2)
                if sustained.any():
                    rvol_score[sustained] = np.minimum(rvol_score[sustained] * 1.05, 100)
                    logger.debug(f"Sustained volume bonus for {sustained.sum()} stocks")
                
                # Spike only (no build-up) = suspicious
                spike_only = valid_sustain & (rvol > 3) & (vol_7d < 1.2) & (vol_30d < 1.1)
                if spike_only.any():
                    rvol_score[spike_only] *= 0.85
                    logger.debug(f"Spike-only penalty for {spike_only.sum()} stocks")
        
        # CONTEXT LAYER 3: Market Cap Adjustments
        if 'category' in df.columns and rvol_score.notna().any():
            category = df['category']
            
            # Small/Micro caps: High RVOL is more common
            is_penny = category.isin(['Micro Cap', 'Small Cap'])
            penny_valid = is_penny & valid_rvol & rvol_score.notna()
            
            if penny_valid.any():
                # Progressive adjustment based on RVOL level
                # 2-3x is normal for penny stocks
                penny_moderate = penny_valid & (rvol > 2) & (rvol <= 3)
                if penny_moderate.any():
                    rvol_score[penny_moderate] *= 0.9
                
                # 3-5x is elevated but not unusual
                penny_elevated = penny_valid & (rvol > 3) & (rvol <= 5)
                if penny_elevated.any():
                    rvol_score[penny_elevated] *= 0.85
                
                # 5-10x needs scrutiny
                penny_high = penny_valid & (rvol > 5) & (rvol <= 10)
                if penny_high.any():
                    rvol_score[penny_high] *= 0.75
                
                # >10x is likely manipulation
                penny_extreme = penny_valid & (rvol > 10)
                if penny_extreme.any():
                    rvol_score[penny_extreme] = np.minimum(rvol_score[penny_extreme] * 0.6, 70)
                    logger.warning(f"Extreme RVOL in {penny_extreme.sum()} penny stocks")
            
            # Large/Mega caps: High RVOL is more significant
            is_large = category.isin(['Large Cap', 'Mega Cap'])
            large_valid = is_large & valid_rvol & rvol_score.notna()
            
            if large_valid.any():
                # Even moderate elevation is significant
                large_elevated = large_valid & (rvol > 1.5)
                if large_elevated.any():
                    rvol_score[large_elevated] = np.minimum(rvol_score[large_elevated] * 1.1, 100)
                
                # High RVOL in large caps = major event
                large_high = large_valid & (rvol > 3)
                if large_high.any():
                    rvol_score[large_high] = np.minimum(rvol_score[large_high] * 1.15, 100)
                    logger.info(f"Significant RVOL in {large_high.sum()} large cap stocks")
        
        # CONTEXT LAYER 4: Position in Range
        # Volume at extremes has different meaning
        if 'from_high_pct' in df.columns and rvol_score.notna().any():
            from_high = df['from_high_pct']
            valid_position = valid_rvol & from_high.notna() & rvol_score.notna()
            
            if valid_position.any():
                # High volume near 52w high = breakout attempt
                near_high = valid_position & (from_high > -5) & (from_high <= 0) & (rvol > 2)
                if near_high.any():
                    rvol_score[near_high] = np.minimum(rvol_score[near_high] + 5, 100)
                
                # High volume near 52w low = potential reversal
                if 'from_low_pct' in df.columns:
                    from_low = df['from_low_pct']
                    near_low = valid_position & from_low.notna() & (from_low < 10) & (rvol > 2)
                    if near_low.any():
                        rvol_score[near_low] = np.minimum(rvol_score[near_low] + 3, 100)
        
        # Final clipping
        rvol_score = rvol_score.clip(0, 100)
        
        # DO NOT FILL NaN!
        
        # COMPREHENSIVE LOGGING
        valid_scores = rvol_score.notna().sum()
        total_stocks = len(df)
        
        logger.info(f"RVOL scores: {valid_scores}/{total_stocks} calculated")
        
        if valid_scores > 0:
            score_dist = rvol_score[rvol_score.notna()]
            logger.info(f"Distribution - Mean: {score_dist.mean():.1f}, "
                       f"Median: {score_dist.median():.1f}, "
                       f"Std: {score_dist.std():.1f}")
            
            # RVOL distribution
            if valid_rvol.any():
                rvol_values = rvol[valid_rvol]
                dead = (rvol_values == 0).sum()
                low = ((rvol_values > 0) & (rvol_values < 0.5)).sum()
                below_avg = ((rvol_values >= 0.5) & (rvol_values < 1)).sum()
                normal = ((rvol_values >= 1) & (rvol_values < 2)).sum()
                elevated = ((rvol_values >= 2) & (rvol_values < 5)).sum()
                high = ((rvol_values >= 5) & (rvol_values < 10)).sum()
                extreme = (rvol_values >= 10).sum()
                
                logger.debug(f"RVOL ranges: Dead={dead}, Low={low}, Below={below_avg}, "
                            f"Normal={normal}, Elevated={elevated}, High={high}, Extreme={extreme}")
                
                if rvol_values.median() > 1.5:
                    logger.info(f"Market-wide elevated volume (median RVOL: {rvol_values.median():.2f})")
        
        return rvol_score
        
    @staticmethod
    def _calculate_trend_quality(df: pd.DataFrame) -> pd.Series:
        """
        Calculate trend quality based on proper SMA hierarchy and alignment.
        FIXED: Correct SMA weights, no overrides, proper cross detection.
        
        Core Philosophy:
        - SMA200 > SMA50 > SMA20 in importance (not reversed!)
        - Trend quality = alignment + consistency + strength
        - No arbitrary overrides that defeat calculations
        - Golden/Death cross needs actual crossover, not just proximity
        
        Components:
        - 50% SMA Alignment (200=50%, 50=30%, 20=20% of this)
        - 25% Trend Consistency (how well SMAs are ordered)
        - 15% Trend Strength (separation between SMAs)
        - 10% Special Patterns (golden cross, squeeze, etc.)
        
        Score Range:
        - 85-100: Perfect bullish alignment
        - 70-85: Strong bullish trend
        - 50-70: Mild bullish/neutral
        - 30-50: Weak/mixed trend
        - 15-30: Bearish trend
        - 0-15: Strong bearish alignment
        """
        # Initialize with NaN
        trend_quality = pd.Series(np.nan, index=df.index, dtype=float)
        
        # Check minimum requirements
        if 'price' not in df.columns:
            logger.warning("No price data for trend quality calculation")
            return trend_quality  # Return NaN, not default!
        
        price = df['price']
        price_valid = price.notna() & (price > 0)
        
        if not price_valid.any():
            logger.warning("No valid price data for trend quality")
            return trend_quality
        
        # COMPONENT 1: SMA ALIGNMENT (50% weight)
        # FIXED: Proper hierarchy - SMA200 most important!
        alignment_score = pd.Series(0, index=df.index, dtype=float)
        alignment_max = pd.Series(0, index=df.index, dtype=float)
        
        # SMA200 - PRIMARY TREND (50% of alignment score)
        if 'sma_200d' in df.columns:
            sma_200 = df['sma_200d']
            valid_200 = price_valid & sma_200.notna() & (sma_200 > 0)
            
            if valid_200.any():
                # Above SMA200 = bullish primary trend
                above_200 = valid_200 & (price > sma_200)
                alignment_score[above_200] += 50
                alignment_max[valid_200] += 50
                
                # Distance bonus/penalty (up to ±10 points)
                distance_200 = ((price - sma_200) / sma_200 * 100).clip(-20, 20)
                alignment_score[valid_200] += distance_200[valid_200] * 0.5
        
        # SMA50 - SECONDARY TREND (30% of alignment score)
        if 'sma_50d' in df.columns:
            sma_50 = df['sma_50d']
            valid_50 = price_valid & sma_50.notna() & (sma_50 > 0)
            
            if valid_50.any():
                above_50 = valid_50 & (price > sma_50)
                alignment_score[above_50] += 30
                alignment_max[valid_50] += 30
                
                # Distance bonus/penalty
                distance_50 = ((price - sma_50) / sma_50 * 100).clip(-15, 15)
                alignment_score[valid_50] += distance_50[valid_50] * 0.3
        
        # SMA20 - MINOR TREND (20% of alignment score)
        if 'sma_20d' in df.columns:
            sma_20 = df['sma_20d']
            valid_20 = price_valid & sma_20.notna() & (sma_20 > 0)
            
            if valid_20.any():
                above_20 = valid_20 & (price > sma_20)
                alignment_score[above_20] += 20
                alignment_max[valid_20] += 20
                
                # Distance bonus/penalty
                distance_20 = ((price - sma_20) / sma_20 * 100).clip(-10, 10)
                alignment_score[valid_20] += distance_20[valid_20] * 0.2
        
        # Normalize alignment score
        has_alignment = alignment_max > 0
        alignment_component = pd.Series(50, index=df.index, dtype=float)
        if has_alignment.any():
            # Convert to 0-100 scale
            alignment_component[has_alignment] = (
                (alignment_score[has_alignment] / alignment_max[has_alignment]) * 100
            ).clip(0, 100)
        
        # COMPONENT 2: TREND CONSISTENCY (25% weight)
        # How well are SMAs ordered?
        consistency_component = pd.Series(50, index=df.index, dtype=float)
        
        # Check if we have multiple SMAs
        sma_count = sum([
            'sma_20d' in df.columns,
            'sma_50d' in df.columns,
            'sma_200d' in df.columns
        ])
        
        if sma_count >= 2:
            # Perfect bullish: Price > SMA20 > SMA50 > SMA200
            if all(col in df.columns for col in ['sma_20d', 'sma_50d', 'sma_200d']):
                sma_20 = df['sma_20d']
                sma_50 = df['sma_50d']
                sma_200 = df['sma_200d']
                
                all_valid = (
                    price_valid & 
                    sma_20.notna() & sma_50.notna() & sma_200.notna() &
                    (sma_20 > 0) & (sma_50 > 0) & (sma_200 > 0)
                )
                
                if all_valid.any():
                    # Perfect bullish alignment
                    perfect_bull = all_valid & (price > sma_20) & (sma_20 > sma_50) & (sma_50 > sma_200)
                    consistency_component[perfect_bull] = 100
                    
                    # Good bullish (price above all, but SMAs mixed)
                    good_bull = all_valid & (price > sma_20) & (price > sma_50) & (price > sma_200) & ~perfect_bull
                    consistency_component[good_bull] = 75
                    
                    # Mixed (price between SMAs)
                    mixed = all_valid & ~perfect_bull & ~good_bull
                    price_above_count = (
                        (price > sma_20).astype(int) +
                        (price > sma_50).astype(int) +
                        (price > sma_200).astype(int)
                    )
                    consistency_component[mixed] = 25 + (price_above_count[mixed] * 16.67)
                    
                    # Perfect bearish alignment
                    perfect_bear = all_valid & (price < sma_20) & (sma_20 < sma_50) & (sma_50 < sma_200)
                    consistency_component[perfect_bear] = 0
            
            # Two SMAs available
            elif sma_count == 2:
                if 'sma_50d' in df.columns and 'sma_200d' in df.columns:
                    sma_50 = df['sma_50d']
                    sma_200 = df['sma_200d']
                    
                    valid_two = (
                        price_valid & 
                        sma_50.notna() & sma_200.notna() &
                        (sma_50 > 0) & (sma_200 > 0)
                    )
                    
                    if valid_two.any():
                        bullish_two = valid_two & (price > sma_50) & (sma_50 > sma_200)
                        consistency_component[bullish_two] = 80
                        
                        bearish_two = valid_two & (price < sma_50) & (sma_50 < sma_200)
                        consistency_component[bearish_two] = 20
        
        # COMPONENT 3: TREND STRENGTH (15% weight)
        # Separation between SMAs indicates trend strength
        strength_component = pd.Series(50, index=df.index, dtype=float)
        
        if sma_count >= 2:
            separation_scores = []
            
            if 'sma_50d' in df.columns and 'sma_200d' in df.columns:
                sma_50 = df['sma_50d']
                sma_200 = df['sma_200d']
                
                valid = sma_50.notna() & sma_200.notna() & (sma_200 > 0)
                if valid.any():
                    # Calculate % separation
                    separation = ((sma_50 - sma_200) / sma_200 * 100).clip(-30, 30)
                    
                    # Positive separation (50 > 200) = bullish strength
                    # Negative separation (50 < 200) = bearish strength
                    # Convert to 0-100 score
                    sep_score = pd.Series(50, index=df.index)
                    sep_score[valid] = 50 + separation[valid] * 1.67
                    separation_scores.append(sep_score)
            
            if separation_scores:
                strength_component = pd.concat(separation_scores, axis=1).mean(axis=1)
        
        # COMPONENT 4: SPECIAL PATTERNS (10% weight)
        pattern_component = pd.Series(50, index=df.index, dtype=float)
        
        # Golden/Death Cross Detection (FIXED)
        if 'sma_50d' in df.columns and 'sma_200d' in df.columns:
            sma_50 = df['sma_50d']
            sma_200 = df['sma_200d']
            
            valid_cross = sma_50.notna() & sma_200.notna() & (sma_50 > 0) & (sma_200 > 0)
            
            if valid_cross.any():
                # Golden cross: SMA50 above SMA200 and close together (recent cross)
                sma_50_above = sma_50 > sma_200
                proximity = (np.abs(sma_50 - sma_200) / sma_200) < 0.03  # Within 3%
                
                # Need momentum confirmation for actual cross
                if 'ret_30d' in df.columns:
                    ret_30d = df['ret_30d']
                    
                    # Golden cross: 50 > 200, close together, positive momentum
                    golden_cross = valid_cross & sma_50_above & proximity & (ret_30d > 5)
                    pattern_component[golden_cross] = 80
                    
                    # Death cross: 50 < 200, close together, negative momentum
                    death_cross = valid_cross & ~sma_50_above & proximity & (ret_30d < -5)
                    pattern_component[death_cross] = 20
                else:
                    # Without momentum, just use position
                    potential_golden = valid_cross & sma_50_above & proximity
                    pattern_component[potential_golden] = 70
                    
                    potential_death = valid_cross & ~sma_50_above & proximity
                    pattern_component[potential_death] = 30
        
        # SMA Squeeze Pattern (volatility contraction)
        if all(col in df.columns for col in ['sma_20d', 'sma_50d']):
            sma_20 = df['sma_20d']
            sma_50 = df['sma_50d']
            
            valid_squeeze = sma_20.notna() & sma_50.notna() & (sma_50 > 0)
            if valid_squeeze.any():
                # SMAs converging = potential breakout
                convergence = (np.abs(sma_20 - sma_50) / sma_50) < 0.02  # Within 2%
                pattern_component[convergence] = 60  # Neutral with potential
        
        # COMBINE COMPONENTS
        components = [
            (alignment_component, 0.50),
            (consistency_component, 0.25),
            (strength_component, 0.15),
            (pattern_component, 0.10)
        ]
        
        # Calculate weighted average
        for idx in df.index:
            valid_components = []
            valid_weights = []
            
            for component, weight in components:
                if pd.notna(component[idx]) and component[idx] != 50:  # 50 is default
                    valid_components.append(component[idx])
                    valid_weights.append(weight)
            
            if valid_components:
                total_weight = sum(valid_weights)
                if total_weight > 0:
                    normalized_weights = [w/total_weight for w in valid_weights]
                    trend_quality[idx] = sum(c * w for c, w in zip(valid_components, normalized_weights))
        
        # NO OVERRIDES! Let the calculation stand
        
        # Final clipping
        trend_quality = trend_quality.clip(0, 100)
        
        # COMPREHENSIVE LOGGING
        valid_scores = trend_quality.notna().sum()
        total_stocks = len(df)
        
        logger.info(f"Trend quality scores: {valid_scores}/{total_stocks} calculated")
        
        if valid_scores > 0:
            score_dist = trend_quality[trend_quality.notna()]
            logger.info(f"Distribution - Mean: {score_dist.mean():.1f}, "
                       f"Median: {score_dist.median():.1f}, "
                       f"Std: {score_dist.std():.1f}")
            
            # Trend categories
            strong_bull = (trend_quality > 85).sum()
            bull = ((trend_quality > 70) & (trend_quality <= 85)).sum()
            neutral = ((trend_quality >= 30) & (trend_quality <= 70)).sum()
            bear = ((trend_quality >= 15) & (trend_quality < 30)).sum()
            strong_bear = (trend_quality < 15).sum()
            
            logger.debug(f"Trend breakdown: Strong Bull={strong_bull}, Bull={bull}, "
                        f"Neutral={neutral}, Bear={bear}, Strong Bear={strong_bear}")
        
        return trend_quality
    
    @staticmethod
    def _calculate_long_term_strength(df: pd.DataFrame) -> pd.Series:
        """
        Calculate long-term strength based on multi-year performance.
        GUARANTEED to return a Series of same length as input DataFrame.
        
        Core Philosophy:
        - ALWAYS returns pd.Series with same index as df
        - Fully vectorized (no loops)
        - Only scores what can be measured (NaN for insufficient data)
        - Long-term = 1y, 3y, 5y ONLY
        - No penalties, just honest assessment
        
        Score Components:
        - 1-year: 20% weight (when available)
        - 3-year: 35% weight (when available)
        - 5-year: 45% weight (when available)
        - Weights auto-normalize based on available data
        """
        # CRITICAL: Initialize with correct index and length
        long_term_strength = pd.Series(np.nan, index=df.index, dtype=float)
        
        # Early return if empty DataFrame
        if df.empty:
            logger.warning("Empty DataFrame provided to calculate_long_term_strength")
            return long_term_strength
        
        # Define periods and weights
        periods = {
            'ret_1y': 0.20,
            'ret_3y': 0.35,
            'ret_5y': 0.45
        }
        
        # Check which columns exist
        available_periods = {}
        for period, weight in periods.items():
            if period in df.columns:
                available_periods[period] = weight
        
        # If no long-term data columns exist at all
        if not available_periods:
            logger.warning("No long-term return columns (ret_1y, ret_3y, ret_5y) found in DataFrame")
            # Return Series of NaN (not error, just no data)
            return long_term_strength
        
        # VECTORIZED CALCULATION
        # Collect scores for each period
        period_scores = pd.DataFrame(index=df.index)
        period_weights = pd.DataFrame(index=df.index)
        
        for period, base_weight in available_periods.items():
            # Get return data
            returns = pd.Series(df[period].values, index=df.index)
            valid = returns.notna()
            
            if not valid.any():
                continue
            
            # Calculate annualized return based on period
            years = float(period.replace('ret_', '').replace('y', ''))
            
            # Initialize score for this period
            score = pd.Series(np.nan, index=df.index)
            
            # Calculate CAGR for valid returns
            # Handle negative returns properly
            positive_final = valid & (returns > -100)
            if positive_final.any():
                # CAGR = ((1 + return/100)^(1/years) - 1) * 100
                cagr = ((1 + returns[positive_final]/100) ** (1/years) - 1) * 100
                
                # Convert CAGR to score (0-100)
                # Indian market context:
                # <0% → 20-50
                # 0-15% → 50-70  
                # 15-25% → 70-85
                # >25% → 85-100
                score[positive_final] = np.where(
                    cagr < 0,
                    np.maximum(20, 50 + cagr * 1.5),  # Negative: scale from 20 to 50
                    np.where(
                        cagr <= 15,
                        50 + (cagr / 15) * 20,  # 0-15%: scale from 50 to 70
                        np.where(
                            cagr <= 25,
                            70 + ((cagr - 15) / 10) * 15,  # 15-25%: scale from 70 to 85
                            np.minimum(100, 85 + (cagr - 25) * 0.5)  # >25%: scale from 85 to 100
                        )
                    )
                )
            
            # Handle total loss cases
            total_loss = valid & (returns <= -100)
            if total_loss.any():
                score[total_loss] = 0
            
            # Store score and weight for this period
            period_scores[period] = score
            period_weights[period] = pd.Series(
                np.where(score.notna(), base_weight, 0),
                index=df.index
            )
        
        # Calculate weighted average where we have data
        if not period_scores.empty:
            # Sum weighted scores
            weighted_sum = pd.Series(0, index=df.index, dtype=float)
            weight_sum = pd.Series(0, index=df.index, dtype=float)
            
            for period in period_scores.columns:
                valid = period_scores[period].notna()
                weighted_sum[valid] += period_scores[period][valid] * period_weights[period][valid]
                weight_sum[valid] += period_weights[period][valid]
            
            # Calculate final score where weights > 0
            has_score = weight_sum > 0
            long_term_strength[has_score] = weighted_sum[has_score] / weight_sum[has_score]
        
        # ADD CONSISTENCY BONUS (if multiple periods available)
        if len(period_scores.columns) >= 2:
            # Check for consistency across periods
            scores_array = period_scores.values
            valid_rows = ~np.isnan(scores_array).all(axis=1)
            
            if valid_rows.any():
                # Count how many periods have data for each stock
                periods_with_data = (~np.isnan(scores_array)).sum(axis=1)
                
                # Calculate standard deviation across periods
                score_std = np.nanstd(scores_array, axis=1)
                score_mean = np.nanmean(scores_array, axis=1)
                
                # Low std relative to mean = consistent
                # Add small bonus for consistency
                consistent = valid_rows & (periods_with_data >= 2) & (score_std < 10) & (score_mean > 50)
                if consistent.any():
                    long_term_strength[consistent] = np.minimum(
                        long_term_strength[consistent] * 1.05, 
                        100
                    )
                
                # High std = inconsistent (volatile performance)
                inconsistent = valid_rows & (periods_with_data >= 2) & (score_std > 20)
                if inconsistent.any():
                    long_term_strength[inconsistent] *= 0.95
        
        # SPECIAL CASES
        # Check for improvement pattern if all 3 periods available
        if all(period in period_scores.columns for period in ['ret_1y', 'ret_3y', 'ret_5y']):
            ret_1y = pd.Series(df['ret_1y'].values, index=df.index)
            ret_3y = pd.Series(df['ret_3y'].values, index=df.index)
            ret_5y = pd.Series(df['ret_5y'].values, index=df.index)
            
            all_valid = ret_1y.notna() & ret_3y.notna() & ret_5y.notna()
            
            if all_valid.any():
                # Annualized returns
                ann_1y = ret_1y[all_valid]
                ann_3y = ret_3y[all_valid] / 3
                ann_5y = ret_5y[all_valid] / 5
                
                # Accelerating growth pattern
                accelerating = all_valid & (ann_1y > ann_3y * 1.1) & (ann_3y > ann_5y * 1.1)
                if accelerating.any():
                    long_term_strength[accelerating] = np.minimum(
                        long_term_strength[accelerating] + 5, 
                        100
                    )
                
                # Decelerating pattern
                decelerating = all_valid & (ann_1y < ann_3y * 0.9) & (ann_3y < ann_5y * 0.9)
                if decelerating.any():
                    long_term_strength[decelerating] = np.maximum(
                        long_term_strength[decelerating] - 5,
                        0
                    )
        
        # MINIMUM DATA REQUIREMENT
        # Require at least 1 period with data for a score
        # This is already handled by the weight_sum > 0 check above
        
        # Final clipping to ensure valid range
        long_term_strength = long_term_strength.clip(0, 100)
        
        # DO NOT FILL NaN!
        # Stocks without sufficient data remain NaN
        # This is intentional and correct
        
        # VERIFICATION - Critical for debugging
        assert len(long_term_strength) == len(df), f"Length mismatch: {len(long_term_strength)} vs {len(df)}"
        assert long_term_strength.index.equals(df.index), "Index mismatch!"
        
        # LOGGING
        total_stocks = len(df)
        scored_stocks = long_term_strength.notna().sum()
        
        logger.info(f"Long-term strength: {scored_stocks}/{total_stocks} stocks scored")
        
        if scored_stocks > 0:
            score_dist = long_term_strength[long_term_strength.notna()]
            logger.info(f"Score distribution - Mean: {score_dist.mean():.1f}, "
                       f"Median: {score_dist.median():.1f}, "
                       f"Std: {score_dist.std():.1f}")
            
            # Check data availability
            for period in ['ret_1y', 'ret_3y', 'ret_5y']:
                if period in df.columns:
                    available = df[period].notna().sum()
                    pct = (available / total_stocks) * 100
                    logger.debug(f"{period}: {available}/{total_stocks} ({pct:.1f}%) stocks have data")
            
            # Distribution categories
            excellent = (long_term_strength > 80).sum()
            good = ((long_term_strength > 65) & (long_term_strength <= 80)).sum()
            average = ((long_term_strength > 50) & (long_term_strength <= 65)).sum()
            poor = (long_term_strength <= 50).sum()
            
            if excellent > 0 or poor > 0:
                logger.debug(f"Performance breakdown: Excellent={excellent}, Good={good}, "
                            f"Average={average}, Poor={poor}")
        else:
            logger.warning("No stocks had sufficient data for long-term strength scoring")
        
        # GUARANTEED RETURN
        # Always return Series of same length as input
        return long_term_strength
    
    @staticmethod
    def _calculate_liquidity_score(df: pd.DataFrame) -> pd.Series:
        """
        Calculate true liquidity score based on turnover value and tradability.
        FIXED: Uses turnover (volume × price), float ratios, proper scaling.
        
        Liquidity Philosophy:
        - Liquidity = Ability to trade without impacting price
        - Turnover value matters more than share volume
        - Consistency across timeframes is crucial
        - Float-adjusted metrics show true liquidity
        - Market cap relative thresholds
        
        Score Components:
        - 40% Turnover value (volume × price)
        - 25% Turnover ratio (turnover / market cap)
        - 20% Volume consistency
        - 15% Trading frequency (active days)
        
        Score Interpretation:
        - 85-100: Highly liquid (institutional grade)
        - 70-85: Good liquidity (easily tradable)
        - 50-70: Moderate liquidity (retail friendly)
        - 30-50: Low liquidity (caution needed)
        - 0-30: Illiquid (difficult to trade)
        """
        liquidity_score = pd.Series(np.nan, index=df.index, dtype=float)
        
        # Component 1: TURNOVER VALUE (40% weight)
        # Actual money traded, not just shares
        turnover_component = pd.Series(np.nan, index=df.index, dtype=float)
        
        if 'volume_1d' in df.columns and 'price' in df.columns:
            volume_1d = pd.Series(df['volume_1d'].values, index=df.index)
            price = pd.Series(df['price'].values, index=df.index)
            
            valid_turnover = volume_1d.notna() & price.notna() & (volume_1d >= 0) & (price > 0)
            
            if valid_turnover.any():
                # Calculate daily turnover in currency (millions for scaling)
                # Assuming price is in rupees and volume in shares
                turnover = (volume_1d[valid_turnover] * price[valid_turnover]) / 1_000_000  # In millions
                
                # Log scale for turnover (spans many orders of magnitude)
                # Avoid log(0) with small addition
                log_turnover = np.log10(turnover + 0.001)
                
                # Dynamic scaling based on actual data range
                # Don't assume arbitrary ranges
                log_min = log_turnover.min()
                log_max = log_turnover.max()
                log_range = log_max - log_min
                
                if log_range > 0:
                    # Normalize to 0-100 based on actual range
                    turnover_component[valid_turnover] = ((log_turnover - log_min) / log_range) * 100
                else:
                    # All same value
                    turnover_component[valid_turnover] = 50
                
                # Apply market context thresholds
                # Indian market context (in millions INR daily turnover)
                # <0.1M = 20, 0.1-1M = 20-40, 1-10M = 40-60, 10-100M = 60-80, >100M = 80-100
                turnover_component[valid_turnover] = np.where(
                    turnover < 0.1, 20,
                    np.where(
                        turnover < 1, 20 + (np.log10(turnover + 0.1) + 1) * 20,
                        np.where(
                            turnover < 10, 40 + np.log10(turnover) * 20,
                            np.where(
                                turnover < 100, 60 + np.log10(turnover/10) * 20,
                                np.where(
                                    turnover < 1000, 80 + np.log10(turnover/100) * 10,
                                    95  # Cap very high turnover
                                )
                            )
                        )
                    )
                )
        
        # Component 2: TURNOVER RATIO (25% weight)
        # Turnover as % of market cap (velocity)
        velocity_component = pd.Series(np.nan, index=df.index, dtype=float)
        
        if all(col in df.columns for col in ['volume_1d', 'price', 'market_cap']):
            volume_1d = pd.Series(df['volume_1d'].values, index=df.index)
            price = pd.Series(df['price'].values, index=df.index)
            market_cap = pd.Series(df['market_cap'].values, index=df.index)
            
            # Parse market cap (could be string like "1.5B")
            market_cap_numeric = pd.Series(np.nan, index=df.index)
            for idx in df.index:
                mc_val = market_cap[idx]
                if pd.notna(mc_val):
                    if isinstance(mc_val, str):
                        # Parse strings like "1.5B", "500M", "75K"
                        try:
                            if 'T' in str(mc_val):
                                market_cap_numeric[idx] = float(mc_val.replace('T', '').replace(',', '')) * 1_000_000
                            elif 'B' in str(mc_val):
                                market_cap_numeric[idx] = float(mc_val.replace('B', '').replace(',', '')) * 1_000
                            elif 'M' in str(mc_val):
                                market_cap_numeric[idx] = float(mc_val.replace('M', '').replace(',', ''))
                            elif 'K' in str(mc_val):
                                market_cap_numeric[idx] = float(mc_val.replace('K', '').replace(',', '')) / 1_000
                            else:
                                market_cap_numeric[idx] = float(str(mc_val).replace(',', ''))
                        except (ValueError, TypeError, AttributeError) as e:
                            logger.debug(f"Failed to parse market cap value {mc_val}: {e}")
                            market_cap_numeric[idx] = np.nan
                    else:
                        market_cap_numeric[idx] = float(mc_val)
            
            valid_velocity = (
                volume_1d.notna() & price.notna() & market_cap_numeric.notna() &
                (volume_1d >= 0) & (price > 0) & (market_cap_numeric > 0)
            )
            
            if valid_velocity.any():
                # Daily turnover as % of market cap
                daily_turnover = (volume_1d[valid_velocity] * price[valid_velocity]) / 1_000_000
                turnover_ratio = (daily_turnover / market_cap_numeric[valid_velocity]) * 100
                
                # Score based on turnover ratio
                # 0-0.1% = 30, 0.1-0.5% = 30-60, 0.5-2% = 60-80, >2% = 80-100
                velocity_component[valid_velocity] = np.where(
                    turnover_ratio < 0.1, 30 + turnover_ratio * 300,
                    np.where(
                        turnover_ratio < 0.5, 30 + (turnover_ratio - 0.1) * 75,
                        np.where(
                            turnover_ratio < 2, 60 + (turnover_ratio - 0.5) * 13.33,
                            np.where(
                                turnover_ratio < 5, 80 + (turnover_ratio - 2) * 5,
                                95  # Cap at very high ratios (might be manipulation)
                            )
                        )
                    )
                )
        
        # Component 3: VOLUME CONSISTENCY (20% weight)
        # How stable is the liquidity across time
        consistency_component = pd.Series(np.nan, index=df.index, dtype=float)
        
        vol_columns = ['volume_1d', 'volume_7d', 'volume_30d', 'volume_90d']
        available_vols = [col for col in vol_columns if col in df.columns]
        
        if len(available_vols) >= 2:
            # Collect volume data
            vol_data = pd.DataFrame()
            for col in available_vols:
                vol_data[col] = pd.Series(df[col].values, index=df.index)
            
            # Calculate coefficient of variation for each stock
            valid_rows = vol_data.notna().all(axis=1)
            
            if valid_rows.any():
                # volume_7d, volume_30d, volume_90d are ALREADY average daily volumes
                # No normalization needed - they're already in daily equivalent form
                vol_normalized = vol_data.copy()
                
                # Calculate consistency
                vol_mean = vol_normalized[valid_rows].mean(axis=1)
                vol_std = vol_normalized[valid_rows].std(axis=1)
                
                # Avoid division by zero
                vol_cv = pd.Series(np.nan, index=df.index)
                non_zero = valid_rows & (vol_mean > 0)
                vol_cv[non_zero] = vol_std[non_zero] / vol_mean[non_zero]
                
                # Score based on consistency (lower CV = higher score)
                # CV < 0.3 = 80, 0.3-0.6 = 60-80, 0.6-1.0 = 40-60, >1.0 = 20-40
                consistency_component[non_zero] = np.where(
                    vol_cv[non_zero] < 0.3, 80,
                    np.where(
                        vol_cv[non_zero] < 0.6, 80 - (vol_cv[non_zero] - 0.3) * 66.67,
                        np.where(
                            vol_cv[non_zero] < 1.0, 60 - (vol_cv[non_zero] - 0.6) * 50,
                            np.maximum(20, 40 - (vol_cv[non_zero] - 1.0) * 10)
                        )
                    )
                )
        
        # Component 4: TRADING FREQUENCY (15% weight)
        # How often does it trade (vs halted/inactive days)
        frequency_component = pd.Series(np.nan, index=df.index, dtype=float)
        
        if 'volume_30d' in df.columns and 'volume_1d' in df.columns:
            volume_30d = pd.Series(df['volume_30d'].values, index=df.index)
            volume_1d = pd.Series(df['volume_1d'].values, index=df.index)
            
            valid_freq = volume_30d.notna() & volume_1d.notna() & (volume_30d >= 0)
            
            if valid_freq.any():
                # volume_30d is ALREADY average daily volume over 30 days
                # Compare today's volume against the 30-day average
                avg_daily = volume_30d[valid_freq]
                
                # Avoid division by zero
                safe_avg = avg_daily.copy()
                safe_avg[safe_avg == 0] = 1
                
                # Ratio indicates consistency
                # ratio = today's volume / 30-day average daily volume
                # ratio > 1 = above average, ratio < 1 = below average
                ratio = volume_1d[valid_freq] / safe_avg
                
                # Score based on trading regularity
                # Ratio near 1 = regular (good)
                # Ratio >> 1 = sporadic (bad)
                # Ratio << 1 = declining (bad)
                frequency_component[valid_freq] = np.where(
                    ratio < 0.5, 40,  # Much lower than average
                    np.where(
                        ratio < 0.8, 50,  # Somewhat lower
                        np.where(
                            ratio < 1.5, 70,  # Normal range
                            np.where(
                                ratio < 3, 60,  # Somewhat sporadic
                                40  # Very sporadic
                            )
                        )
                    )
                )
                
                # Bonus for very consistent trading
                very_consistent = valid_freq & (ratio > 0.8) & (ratio < 1.2)
                frequency_component[very_consistent] = 85
        
        # COMBINE COMPONENTS
        components = {
            'turnover': (turnover_component, 0.40),
            'velocity': (velocity_component, 0.25),
            'consistency': (consistency_component, 0.20),
            'frequency': (frequency_component, 0.15)
        }
        
        # Calculate weighted average
        for idx in df.index:
            component_scores = []
            component_weights = []
            
            for name, (score_series, weight) in components.items():
                if pd.notna(score_series[idx]):
                    component_scores.append(score_series[idx])
                    component_weights.append(weight)
            
            if component_scores:
                # Normalize weights
                total_weight = sum(component_weights)
                normalized_weights = [w/total_weight for w in component_weights]
                
                # Calculate weighted score
                liquidity_score[idx] = sum(
                    score * weight 
                    for score, weight in zip(component_scores, normalized_weights)
                )
        
        # MARKET CAP ADJUSTMENTS
        # Different liquidity expectations for different caps
        if 'category' in df.columns and liquidity_score.notna().any():
            category = pd.Series(df['category'].values, index=df.index)
            
            # Large/Mega caps
            is_large = category.isin(['Large Cap', 'Mega Cap'])
            large_mask = is_large & liquidity_score.notna()
            
            if large_mask.any():
                # Large caps should have high liquidity
                # Penalize if low, bonus if high
                liquidity_score[large_mask] = np.where(
                    liquidity_score[large_mask] < 50,
                    liquidity_score[large_mask] * 0.8,  # Penalty for illiquid large cap
                    np.minimum(liquidity_score[large_mask] * 1.1, 100)  # Bonus for liquid
                )
            
            # Small/Micro caps
            is_small = category.isin(['Small Cap', 'Micro Cap'])
            small_mask = is_small & liquidity_score.notna()
            
            if small_mask.any():
                # Small caps naturally less liquid - adjust expectations
                # Boost scores slightly to normalize
                liquidity_score[small_mask] = np.minimum(
                    liquidity_score[small_mask] + 10, 
                    100
                )
        
        # Clip final scores
        liquidity_score = liquidity_score.clip(0, 100)
        
        # NO DEFAULT FILLING
        # Stocks without data remain NaN
        
        # LOGGING
        valid_scores = liquidity_score.notna().sum()
        if valid_scores > 0:
            logger.info(f"Liquidity scores calculated: {valid_scores}/{len(df)} stocks")
            
            score_dist = liquidity_score[liquidity_score.notna()]
            logger.info(f"Liquidity distribution - Mean: {score_dist.mean():.1f}, "
                       f"Median: {score_dist.median():.1f}, "
                       f"Std: {score_dist.std():.1f}")
            
            # Liquidity categories
            highly_liquid = (liquidity_score > 85).sum()
            good_liquid = ((liquidity_score > 70) & (liquidity_score <= 85)).sum()
            moderate = ((liquidity_score > 50) & (liquidity_score <= 70)).sum()
            low = ((liquidity_score > 30) & (liquidity_score <= 50)).sum()
            illiquid = (liquidity_score <= 30).sum()
            
            logger.debug(f"Liquidity breakdown: Highly={highly_liquid}, Good={good_liquid}, "
                        f"Moderate={moderate}, Low={low}, Illiquid={illiquid}")
            
            # Check if turnover data was available
            if turnover_component.notna().any():
                logger.debug(f"Turnover data available for {turnover_component.notna().sum()} stocks")
            else:
                logger.warning("No turnover data available - liquidity scores may be incomplete")
        
        return liquidity_score
        
    @staticmethod
    def _apply_smart_bonuses(df: pd.DataFrame) -> pd.DataFrame:
        """
        Apply smart bonuses based on confirmed exceptional patterns.
        FIXED: All additive, order-independent, transparent tracking, justified amounts.
        
        Bonus Philosophy:
        - Only reward EXCEPTIONAL patterns (top 5% cases)
        - All bonuses are ADDITIVE (no multiplicative chaos)
        - Maximum total bonus capped at 10 points
        - Each bonus requires multiple confirmations
        - Full transparency with bonus tracking
        - Never modify NaN scores
        
        Bonus Categories:
        1. Perfect Alignment (max 4 points)
        2. Momentum Confirmation (max 3 points)
        3. Breakout Validation (max 3 points)
        4. Volume Surge Quality (max 2 points)
        5. Recovery Pattern (max 2 points)
        
        Maximum cumulative bonus: 10 points (prevents score inflation)
        """
        # Don't modify original
        df = df.copy()
        
        # Initialize bonus tracking
        df['bonus_points'] = 0.0
        df['bonus_reasons'] = ''
        
        # Only apply bonuses to stocks with valid master scores
        valid_scores = df['master_score'].notna()
        
        if not valid_scores.any():
            logger.info("No valid master scores for bonus application")
            return df
        
        # Track individual bonuses for transparency
        perfect_alignment_bonus = pd.Series(0, index=df.index, dtype=float)
        momentum_bonus = pd.Series(0, index=df.index, dtype=float)
        breakout_bonus = pd.Series(0, index=df.index, dtype=float)
        volume_bonus = pd.Series(0, index=df.index, dtype=float)
        recovery_bonus = pd.Series(0, index=df.index, dtype=float)
        
        # BONUS 1: PERFECT ALIGNMENT (max 4 points)
        # All technical indicators aligned perfectly
        if all(col in df.columns for col in ['position_score', 'momentum_score', 'trend_quality', 'volume_score']):
            perfect_alignment = (
                valid_scores &
                (df['position_score'] > 80) &
                (df['momentum_score'] > 70) &
                (df['trend_quality'] > 80) &
                (df['volume_score'] > 70)
            )
            
            if perfect_alignment.any():
                perfect_alignment_bonus[perfect_alignment] = 4
                df.loc[perfect_alignment, 'bonus_reasons'] += 'PerfectAlign(+4) '
                logger.debug(f"Perfect alignment bonus applied to {perfect_alignment.sum()} stocks")
        
        # BONUS 2: MOMENTUM CONFIRMATION (max 3 points)
        # Strong momentum with acceleration and volume
        if all(col in df.columns for col in ['momentum_score', 'acceleration_score', 'rvol_score']):
            # Check for confirmed momentum
            momentum_confirmed = (
                valid_scores &
                (df['momentum_score'] > 75) &
                (df['acceleration_score'] > 70) &
                (df['rvol_score'] > 60)
            )
            
            if momentum_confirmed.any():
                # Graduated bonus based on strength
                super_momentum = momentum_confirmed & (df['momentum_score'] > 85)
                regular_momentum = momentum_confirmed & ~super_momentum
                
                momentum_bonus[super_momentum] = 3
                momentum_bonus[regular_momentum] = 2
                
                df.loc[super_momentum, 'bonus_reasons'] += 'SuperMomentum(+3) '
                df.loc[regular_momentum, 'bonus_reasons'] += 'Momentum(+2) '
                logger.debug(f"Momentum bonus applied to {momentum_confirmed.sum()} stocks")
        
        # BONUS 3: BREAKOUT VALIDATION (max 3 points)
        # Confirmed breakout with volume
        if all(col in df.columns for col in ['breakout_score', 'position_score', 'rvol_score']):
            breakout_confirmed = (
                valid_scores &
                (df['breakout_score'] > 75) &
                (df['position_score'] > 70) &  # Near highs
                (df['rvol_score'] > 65)  # With volume
            )
            
            if breakout_confirmed.any():
                # Extra bonus for extreme breakout scores
                extreme_breakout = breakout_confirmed & (df['breakout_score'] > 90)
                regular_breakout = breakout_confirmed & ~extreme_breakout
                
                breakout_bonus[extreme_breakout] = 3
                breakout_bonus[regular_breakout] = 2
                
                df.loc[extreme_breakout, 'bonus_reasons'] += 'ExtremeBreakout(+3) '
                df.loc[regular_breakout, 'bonus_reasons'] += 'Breakout(+2) '
                logger.debug(f"Breakout bonus applied to {breakout_confirmed.sum()} stocks")
        
        # BONUS 4: VOLUME SURGE QUALITY (max 2 points)
        # Exceptional volume with price confirmation
        if all(col in df.columns for col in ['volume_score', 'rvol_score', 'momentum_score']):
            volume_surge = (
                valid_scores &
                (df['volume_score'] > 80) &
                (df['rvol_score'] > 75) &
                (df['momentum_score'] > 50)  # Positive momentum
            )
            
            if volume_surge.any():
                volume_bonus[volume_surge] = 2
                df.loc[volume_surge, 'bonus_reasons'] += 'VolumeSurge(+2) '
                logger.debug(f"Volume surge bonus applied to {volume_surge.sum()} stocks")
        
        # BONUS 5: RECOVERY PATTERN (max 2 points)
        # Strong recovery from oversold
        if all(col in df.columns for col in ['position_score', 'momentum_score', 'acceleration_score']):
            recovery_pattern = (
                valid_scores &
                (df['position_score'] < 40) &  # Still relatively low
                (df['momentum_score'] > 60) &  # But strong momentum
                (df['acceleration_score'] > 70)  # And accelerating
            )
            
            if recovery_pattern.any():
                recovery_bonus[recovery_pattern] = 2
                df.loc[recovery_pattern, 'bonus_reasons'] += 'Recovery(+2) '
                logger.debug(f"Recovery bonus applied to {recovery_pattern.sum()} stocks")
        
        # BONUS 6: MARKET STATE MOMENTUM (max 3 points)
        # Individual stock in STRONG_UPTREND with high momentum
        market_state_bonus = pd.Series(0, index=df.index, dtype=float)
        if 'market_state' in df.columns and 'momentum_score' in df.columns:
            strong_uptrend_momentum = (
                valid_scores &
                (df['market_state'] == 'STRONG_UPTREND') &
                (df['momentum_score'] > 70)
            )
            
            if strong_uptrend_momentum.any():
                market_state_bonus[strong_uptrend_momentum] = 3
                df.loc[strong_uptrend_momentum, 'bonus_reasons'] += 'StrongMomentum(+3) '
                logger.debug(f"Strong uptrend momentum bonus applied to {strong_uptrend_momentum.sum()} stocks")
        
        # BONUS 7: MARKET STATE VALUE OPPORTUNITY (max 2 points)
        # Stock in PULLBACK state with low position score (value opportunity)
        if 'market_state' in df.columns and 'position_score' in df.columns:
            pullback_value = (
                valid_scores &
                (df['market_state'] == 'PULLBACK') &
                (df['position_score'] < 30)
            )
            
            if pullback_value.any():
                market_state_bonus[pullback_value] = 2
                df.loc[pullback_value, 'bonus_reasons'] += 'PullbackValue(+2) '
                logger.debug(f"Pullback value opportunity bonus applied to {pullback_value.sum()} stocks")
        
        # PENALTY PATTERNS (negative bonuses)
        penalty = pd.Series(0, index=df.index, dtype=float)
        
        # Penalty 1: Divergence (high score components but low others)
        if all(col in df.columns for col in ['momentum_score', 'volume_score']):
            divergence = (
                valid_scores &
                ((df['momentum_score'] > 80) & (df['volume_score'] < 30)) |
                ((df['momentum_score'] < 30) & (df['volume_score'] > 80))
            )
            
            if divergence.any():
                penalty[divergence] = -2
                df.loc[divergence, 'bonus_reasons'] += 'Divergence(-2) '
                logger.debug(f"Divergence penalty applied to {divergence.sum()} stocks")
        
        # Penalty 2: Low confidence scores
        if 'confidence_score' in df.columns:
            low_confidence = (
                valid_scores &
                (df['confidence_score'] < 30) &
                (df['master_score'] > 70)  # High score but low confidence
            )
            
            if low_confidence.any():
                penalty[low_confidence] = -3
                df.loc[low_confidence, 'bonus_reasons'] += 'LowConfidence(-3) '
                logger.debug(f"Low confidence penalty applied to {low_confidence.sum()} stocks")
        
        # CALCULATE TOTAL BONUS
        # Sum all bonuses (including negative)
        total_bonus = (
            perfect_alignment_bonus +
            momentum_bonus +
            breakout_bonus +
            volume_bonus +
            recovery_bonus +
            market_state_bonus +
            penalty
        )
        
        # Cap total bonus to prevent score inflation
        MAX_POSITIVE_BONUS = 10
        MAX_NEGATIVE_PENALTY = -5
        
        total_bonus = total_bonus.clip(MAX_NEGATIVE_PENALTY, MAX_POSITIVE_BONUS)
        
        # Apply bonuses only to valid scores
        df.loc[valid_scores, 'bonus_points'] = total_bonus[valid_scores]
        
        # Calculate final score with bonuses
        df['master_score_before_bonus'] = df['master_score'].copy()
        df.loc[valid_scores, 'master_score'] = (
            df.loc[valid_scores, 'master_score'] + total_bonus[valid_scores]
        ).clip(0, 100)
        
        # SPECIAL ADJUSTMENTS (applied after bonuses)
        # Market cap adjustments
        if 'category' in df.columns:
            # Mega caps with high scores get stability bonus
            mega_caps = df['category'] == 'Mega Cap'
            mega_high = mega_caps & valid_scores & (df['master_score'] > 80)
            if mega_high.any():
                df.loc[mega_high, 'master_score'] = np.minimum(
                    df.loc[mega_high, 'master_score'] + 2, 
                    100
                )
                df.loc[mega_high, 'bonus_reasons'] += 'MegaCap(+2) '
            
            # Penny stocks with extreme scores get capped
            penny_stocks = df['category'].isin(['Micro Cap', 'Small Cap'])
            penny_extreme = penny_stocks & valid_scores & (df['master_score'] > 90)
            if penny_extreme.any():
                df.loc[penny_extreme, 'master_score'] = np.minimum(
                    df.loc[penny_extreme, 'master_score'],
                    88  # Cap at 88
                )
                df.loc[penny_extreme, 'bonus_reasons'] += 'PennyCap(capped) '
        
        # COMPREHENSIVE LOGGING
        bonuses_applied = (df['bonus_points'] != 0).sum()
        
        if bonuses_applied > 0:
            logger.info(f"Smart bonuses applied to {bonuses_applied} stocks")
            
            # Bonus distribution
            positive_bonuses = (df['bonus_points'] > 0).sum()
            negative_penalties = (df['bonus_points'] < 0).sum()
            
            logger.info(f"Bonus distribution: {positive_bonuses} bonuses, {negative_penalties} penalties")
            
            # Average bonus impact
            avg_bonus = df.loc[df['bonus_points'] != 0, 'bonus_points'].mean()
            max_bonus = df['bonus_points'].max()
            min_bonus = df['bonus_points'].min()
            
            logger.debug(f"Bonus impact - Avg: {avg_bonus:.1f}, Max: {max_bonus:.1f}, Min: {min_bonus:.1f}")
            
            # Score changes
            score_changes = df.loc[valid_scores, 'master_score'] - df.loc[valid_scores, 'master_score_before_bonus']
            if (score_changes != 0).any():
                avg_change = score_changes[score_changes != 0].mean()
                logger.debug(f"Average score change from bonuses: {avg_change:.1f} points")
            
            # Top bonus recipients
            if 'ticker' in df.columns:
                top_bonus = df.nlargest(3, 'bonus_points')
                if len(top_bonus) > 0:
                    logger.debug("Top bonus recipients:")
                    for _, row in top_bonus.iterrows():
                        if row['bonus_points'] > 0:
                            logger.debug(f"  {row['ticker']}: +{row['bonus_points']:.1f} ({row['bonus_reasons'].strip()})")
        else:
            logger.info("No stocks qualified for smart bonuses")
        
        # Verify score integrity
        if (df['master_score'] > 100).any():
            over_100 = (df['master_score'] > 100).sum()
            logger.error(f"ERROR: {over_100} stocks have scores > 100 after bonuses!")
            df['master_score'] = df['master_score'].clip(0, 100)
        
        if (df['master_score'] < 0).any():
            below_0 = (df['master_score'] < 0).sum()
            logger.error(f"ERROR: {below_0} stocks have scores < 0 after bonuses!")
            df['master_score'] = df['master_score'].clip(0, 100)
        
        return df

    @staticmethod
    def _calculate_category_ranks(df: pd.DataFrame) -> pd.DataFrame:
        """
        Calculate comprehensive category-based rankings with full error handling.
        FIXED: Fully vectorized, handles all edge cases, provides multiple metrics.
        
        Category Ranking Philosophy:
        - Ranks within peer groups (Large Cap, Mid Cap, etc.)
        - Provides both rank and percentile
        - Handles small categories intelligently
        - Adds category statistics for context
        - Size-adjusted rankings for fairness
        - Completely vectorized (no loops)
        
        Outputs:
        - category_rank: Rank within category (1 = best)
        - category_percentile: Percentile within category (100 = best)
        - category_size: Number of stocks in category
        - category_decile: Which decile within category (1-10)
        - category_relative_score: Score vs category average
        - peer_group_rank: Rank within similar-sized categories
        """
        # Don't modify original
        df = df.copy()
        
        # Initialize all category columns with NaN
        category_columns = [
            'category_rank',
            'category_percentile', 
            'category_size',
            'category_decile',
            'category_relative_score',
            'peer_group_rank',
            'category_avg_score',
            'category_median_score',
            'category_std_score'
        ]
        
        for col in category_columns:
            df[col] = np.nan
        
        # Check if category column exists
        if 'category' not in df.columns:
            logger.warning("No 'category' column found, skipping category rankings")
            return df
        
        # Check if master_score exists
        if 'master_score' not in df.columns:
            logger.warning("No 'master_score' column found, skipping category rankings")
            return df
        
        # Get unique categories
        categories = df['category'].unique()
        valid_categories = [c for c in categories if pd.notna(c)]
        
        if len(valid_categories) == 0:
            logger.warning("No valid categories found")
            return df
        
        logger.info(f"Calculating category ranks for {len(valid_categories)} categories")
        
        # VECTORIZED CATEGORY RANKING - No loops!
        # Use groupby for all operations at once
        
        # 1. Basic category rankings
        category_groups = df.groupby('category')['master_score']
        
        # Rank within category (1 = best)
        df['category_rank'] = category_groups.rank(
            method='first',  # First occurrence wins ties
            ascending=False,  # Higher score = better rank
            na_option='bottom'  # NaN scores go to bottom
        )
        
        # Percentile within category (100 = best)
        df['category_percentile'] = category_groups.rank(
            method='average',  # Average for ties
            ascending=True,  # Lower score = lower percentile
            pct=True,  # Return as percentile
            na_option='bottom'
        ) * 100
        
        # 2. Category size (for context)
        category_sizes = df.groupby('category').size()
        df['category_size'] = df['category'].map(category_sizes)
        
        # 3. Category statistics
        category_stats = df.groupby('category')['master_score'].agg([
            ('category_avg_score', 'mean'),
            ('category_median_score', 'median'),
            ('category_std_score', 'std'),
            ('category_min_score', 'min'),
            ('category_max_score', 'max'),
            ('category_valid_count', 'count')
        ])
        
        # Map statistics back to dataframe
        for stat_col in category_stats.columns:
            df[stat_col] = df['category'].map(category_stats[stat_col])
        
        # 4. Relative score vs category average
        df['category_relative_score'] = safe_divide(
            df['master_score'] - df['category_avg_score'],
            df['category_std_score'],
            default=0.0
        )
        
        # 5. Decile within category (1-10, 1 = best)
        df['category_decile'] = pd.cut(
            df['category_percentile'],
            bins=[0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],
            labels=[10, 9, 8, 7, 6, 5, 4, 3, 2, 1],  # Reverse order (1 = best)
            include_lowest=True
        )
        
        # 6. Handle small categories (less than 10 stocks)
        small_categories = df['category_size'] < 10
        if small_categories.any():
            # Mark small category ranks as less reliable
            df.loc[small_categories, 'category_rank_reliability'] = 'Low'
            logger.info(f"{small_categories.sum()} stocks in small categories (<10 stocks)")
        else:
            df['category_rank_reliability'] = 'Normal'
        
        # Very small categories (less than 3 stocks)
        tiny_categories = df['category_size'] < 3
        if tiny_categories.any():
            # Don't rank if category too small
            df.loc[tiny_categories, 'category_rank_reliability'] = 'Insufficient'
            logger.warning(f"{tiny_categories.sum()} stocks in tiny categories (<3 stocks)")
        
        # 7. Peer group ranking (compare similar market cap categories)
        # Define peer groups
        peer_groups = {
            'Large_Mega': ['Large Cap', 'Mega Cap'],
            'Mid': ['Mid Cap'],
            'Small_Micro': ['Small Cap', 'Micro Cap']
        }
        
        df['peer_group'] = 'Other'  # Default
        for group_name, categories_in_group in peer_groups.items():
            mask = df['category'].isin(categories_in_group)
            df.loc[mask, 'peer_group'] = group_name
        
        # Rank within peer groups
        peer_groups_data = df.groupby('peer_group')['master_score']
        df['peer_group_rank'] = peer_groups_data.rank(
            method='first',
            ascending=False,
            na_option='bottom'
        )
        
        df['peer_group_size'] = df.groupby('peer_group')['peer_group'].transform('size')
        
        df['peer_group_percentile'] = peer_groups_data.rank(
            method='average',
            ascending=True,
            pct=True,
            na_option='bottom'
        ) * 100
        
        # 8. Category performance tiers
        # Classify categories by average score
        category_performance = category_stats['category_avg_score'].to_dict()
        
        def classify_category_performance(cat):
            if cat not in category_performance or pd.isna(category_performance[cat]):
                return 'Unknown'
            avg_score = category_performance[cat]
            if avg_score >= 70:
                return 'Strong'
            elif avg_score >= 50:
                return 'Average'
            elif avg_score >= 30:
                return 'Weak'
            else:
                return 'Very Weak'
        
        df['category_performance'] = df['category'].apply(classify_category_performance)
        
        # 9. Best in category flags
        # Mark top stocks in each category
        df['is_category_leader'] = df['category_rank'] == 1
        df['is_category_top5'] = df['category_rank'] <= 5
        df['is_category_top10'] = df['category_rank'] <= 10
        df['is_category_top_decile'] = df['category_decile'] == 1
        
        # 10. Category momentum (if historical data available)
        if 'momentum_score' in df.columns:
            category_momentum = df.groupby('category')['momentum_score'].mean()
            df['category_momentum'] = df['category'].map(category_momentum)
            
            # Flag if stock momentum better than category
            df['beats_category_momentum'] = df['momentum_score'] > df['category_momentum']
        
        # COMPREHENSIVE LOGGING
        for category in valid_categories:
            cat_data = df[df['category'] == category]
            valid_scores = cat_data['master_score'].notna().sum()
            
            if valid_scores > 0:
                avg_score = cat_data['master_score'].mean()
                median_score = cat_data['master_score'].median()
                
                logger.debug(f"{category}: {valid_scores} stocks, "
                            f"Avg: {avg_score:.1f}, Median: {median_score:.1f}")
        
        # Log category distribution
        category_dist = df['category'].value_counts()
        logger.info("Category distribution:")
        for cat, count in category_dist.head().items():
            logger.info(f"  {cat}: {count} stocks")
        
        # Find category leaders
        if 'ticker' in df.columns:
            leaders = df[df['is_category_leader'] == True]
            if len(leaders) > 0:
                logger.info(f"Category leaders identified: {len(leaders)}")
                for _, leader in leaders.head(5).iterrows():
                    if pd.notna(leader['master_score']):
                        logger.debug(f"  {leader['category']}: {leader['ticker']} "
                                   f"(Score: {leader['master_score']:.1f})")
        
        # Verify rankings integrity
        for category in valid_categories:
            cat_mask = df['category'] == category
            cat_ranks = df.loc[cat_mask, 'category_rank']
            
            if cat_ranks.notna().any():
                # Check for duplicate ranks
                rank_counts = cat_ranks.value_counts()
                duplicates = rank_counts[rank_counts > 1]
                if len(duplicates) > 0:
                    logger.error(f"Duplicate ranks in {category}: {duplicates.to_dict()}")
                
                # Check for gaps in ranking
                valid_ranks = cat_ranks.dropna().sort_values()
                expected_ranks = range(1, len(valid_ranks) + 1)
                if not all(r in valid_ranks.values for r in expected_ranks):
                    logger.warning(f"Ranking gaps detected in {category}")
        
        # Performance statistics
        logger.info("Category ranking summary:")
        logger.info(f"  Categories processed: {len(valid_categories)}")
        logger.info(f"  Stocks with category ranks: {df['category_rank'].notna().sum()}")
        logger.info(f"  Category leaders: {df['is_category_leader'].sum()}")
        logger.info(f"  Top decile stocks: {df['is_category_top_decile'].sum()}")
        
        # Small category warning
        small_cat_count = (df['category_size'] < 10).sum()
        if small_cat_count > 0:
            small_cat_pct = (small_cat_count / len(df)) * 100
            logger.warning(f"{small_cat_count} stocks ({small_cat_pct:.1f}%) in small categories")
        
        # Cross-category comparison
        if len(valid_categories) > 1:
            best_category = df.groupby('category')['master_score'].mean().idxmax()
            worst_category = df.groupby('category')['master_score'].mean().idxmin()
            
            if pd.notna(best_category) and pd.notna(worst_category):
                logger.info(f"Best performing category: {best_category}")
                logger.info(f"Worst performing category: {worst_category}")
        
        return df

# ============================================
# MARKET OVERVIEW ANALYSIS - PROFESSIONAL MARKET INSIGHTS
# ============================================

class SimpleMarketAnalysis:
    """Market analysis using actual V9.py columns
    
    This class provides market analysis functionality aligned with V9.py data structure.
    It follows V9.py standards for error handling, data validation, and logging.
    
    Return columns mapping:
    - 1 Day → ret_1d
    - 7 Days → ret_7d  
    - 30 Days → ret_30d
    - 3 Months → ret_3m
    - 6 Months → ret_6m
    - 1 Year → ret_1y
    
    52-week metrics (existing in V9.py):
    - from_low_pct: % distance FROM 52-week low
    - from_high_pct: % distance FROM 52-week high
    """
    
    @staticmethod
    def calculate_52w_metrics(df: pd.DataFrame) -> pd.DataFrame:
        """Use existing 52-week metrics from V9.py data
        
        V9.py already has:
        - from_low_pct: percentage distance FROM 52-week low (how much above low)
        - from_high_pct: percentage distance FROM 52-week high (how much below high)
        
        Args:
            df: DataFrame with V9.py columns
            
        Returns:
            DataFrame with calculated 52-week position metrics
        """
        if df is None or df.empty:
            logger.warning("calculate_52w_metrics: Empty DataFrame received")
            return df
            
        try:
            df = df.copy()
            
            # Use existing columns directly with validation
            if 'from_low_pct' in df.columns:
                # Clean and validate numeric values
                df['from_low_pct'] = pd.to_numeric(df['from_low_pct'], errors='coerce')
                df['52w_low_distance'] = df['from_low_pct']
            else:
                logger.debug("calculate_52w_metrics: 'from_low_pct' column not found")
            
            if 'from_high_pct' in df.columns:
                # Clean and validate numeric values
                df['from_high_pct'] = pd.to_numeric(df['from_high_pct'], errors='coerce')
                df['52w_high_distance'] = df['from_high_pct']
                
                # Calculate position: 0% from high = 100 position, 100% from high = 0 position
                df['52w_position'] = np.maximum(0, 100 - df['from_high_pct'].fillna(100))
            else:
                logger.debug("calculate_52w_metrics: 'from_high_pct' column not found")
            
            logger.debug(f"calculate_52w_metrics: Processed {len(df)} rows")
            return df
            
        except Exception as e:
            logger.error(f"calculate_52w_metrics failed: {e}", exc_info=True)
            return df  # Return original df on error
    
    @staticmethod
    def get_return_column_for_period(period: str) -> str:
        """Map time period to appropriate return column
        
        Args:
            period: Time period string (e.g., "1 Day", "30 Days")
            
        Returns:
            Column name for the return data (e.g., "ret_1d", "ret_30d")
        """
        period_map = {
            "1 Day": "ret_1d",
            "7 Days": "ret_7d", 
            "30 Days": "ret_30d",
            "3 Months": "ret_3m",
            "6 Months": "ret_6m",
            "1 Year": "ret_1y"
        }
        return period_map.get(period, "ret_30d")
    
    @staticmethod
    def filter_stocks_by_universe(df: pd.DataFrame, universe: str) -> pd.DataFrame:
        """Filter stocks based on selected universe
        
        Args:
            df: DataFrame to filter
            universe: Universe selection ("Top 500 Stocks" or "All Stocks")
            
        Returns:
            Filtered DataFrame
        """
        if df is None or df.empty:
            logger.warning("filter_stocks_by_universe: Empty DataFrame received")
            return pd.DataFrame()
            
        try:
            if universe == "Top 500 Stocks":
                # Filter by rank or master_score to get top 500
                if 'rank' in df.columns:
                    result = df.nsmallest(500, 'rank')
                    logger.info(f"filter_stocks_by_universe: Filtered to top 500 by rank")
                elif 'master_score' in df.columns:
                    result = df.nlargest(500, 'master_score')
                    logger.info(f"filter_stocks_by_universe: Filtered to top 500 by master_score")
                else:
                    result = df.head(500)
                    logger.warning(f"filter_stocks_by_universe: No rank/master_score, using first 500")
            else:  # "All Stocks"
                result = df
                logger.info(f"filter_stocks_by_universe: Using all {len(df)} stocks")
            
            return result
            
        except Exception as e:
            logger.error(f"filter_stocks_by_universe failed: {e}", exc_info=True)
            return df
    
    @staticmethod
    def get_stock_views(df: pd.DataFrame, view_type: str, time_period: str, universe: str) -> pd.DataFrame:
        """Get different stock views based on selection
        
        Args:
            df: Source DataFrame with stock data
            view_type: Type of view ("Gainers", "Losers", "52W High", "52W Low", "Active")
            time_period: Time period for returns
            universe: Stock universe filter
            
        Returns:
            Filtered and sorted DataFrame for the requested view
        """
        # Critical validation - V9.py standard
        if df is None or df.empty:
            logger.warning("get_stock_views: Empty DataFrame received")
            return pd.DataFrame()
        
        if 'ticker' not in df.columns:
            logger.error("get_stock_views: Missing critical 'ticker' column")
            return pd.DataFrame()
        
        try:
            logger.info(f"get_stock_views: Processing {len(df)} stocks, view={view_type}, period={time_period}, universe={universe}")
            
            # First filter by universe
            df = SimpleMarketAnalysis.filter_stocks_by_universe(df, universe)
            
            if df.empty:
                logger.warning("get_stock_views: Empty DataFrame after universe filter")
                return pd.DataFrame()
            
            # Calculate/map 52w metrics (uses existing from_low_pct and from_high_pct)
            df = SimpleMarketAnalysis.calculate_52w_metrics(df)
            
            # Get the appropriate return column for the time period
            return_col = SimpleMarketAnalysis.get_return_column_for_period(time_period)
            
            # Process based on view type
            if view_type == "52W High":
                if 'from_high_pct' in df.columns:
                    # Clean numeric values
                    df['from_high_pct'] = pd.to_numeric(df['from_high_pct'], errors='coerce')
                    # Stocks close to 52W high have LOW from_high_pct
                    result = df[df['from_high_pct'] < 15]  # Within 15% of high
                    result = result.sort_values('from_high_pct', ascending=True)  # Closest to high first
                    logger.info(f"get_stock_views: Found {len(result)} stocks near 52W high")
                else:
                    logger.warning("get_stock_views: 'from_high_pct' column not found for 52W High view")
                    result = df
                
            elif view_type == "52W Low":
                if 'from_low_pct' in df.columns:
                    # Clean numeric values
                    df['from_low_pct'] = pd.to_numeric(df['from_low_pct'], errors='coerce')
                    # Stocks close to 52W low have LOW from_low_pct
                    result = df[df['from_low_pct'] < 15]  # Within 15% of low
                    result = result.sort_values('from_low_pct', ascending=True)  # Closest to low first
                    logger.info(f"get_stock_views: Found {len(result)} stocks near 52W low")
                else:
                    logger.warning("get_stock_views: 'from_low_pct' column not found for 52W Low view")
                    result = df
                
            elif view_type == "Gainers":
                if return_col in df.columns:
                    # Clean numeric values
                    df[return_col] = pd.to_numeric(df[return_col], errors='coerce')
                    result = df[df[return_col] > 0].sort_values(return_col, ascending=False)
                    logger.info(f"get_stock_views: Found {len(result)} gainers for {time_period}")
                else:
                    logger.warning(f"get_stock_views: Column '{return_col}' not found for Gainers view")
                    result = pd.DataFrame()
                        
            elif view_type == "Losers":
                if return_col in df.columns:
                    # Clean numeric values
                    df[return_col] = pd.to_numeric(df[return_col], errors='coerce')
                    result = df[df[return_col] < 0].sort_values(return_col, ascending=True)
                    logger.info(f"get_stock_views: Found {len(result)} losers for {time_period}")
                else:
                    logger.warning(f"get_stock_views: Column '{return_col}' not found for Losers view")
                    result = pd.DataFrame()
                        
            elif view_type == "Active":
                if 'volume_30d' in df.columns:
                    result = df.sort_values('volume_30d', ascending=False)
                    logger.info(f"get_stock_views: Sorted by volume_30d")
                elif 'rvol' in df.columns:
                    result = df.sort_values('rvol', ascending=False)
                    logger.info(f"get_stock_views: Sorted by rvol")
                else:
                    logger.warning("get_stock_views: No volume columns found for Active view")
                    result = df
            
            else:  # Default view
                if 'rank' in df.columns:
                    result = df.sort_values('rank')
                else:
                    result = df
                logger.info(f"get_stock_views: Using default view")
            
            # Return top 100 results for comprehensive market overview
            final_result = result.head(100)
            logger.info(f"get_stock_views: Returning {len(final_result)} stocks")
            return final_result
            
        except Exception as e:
            logger.error(f"get_stock_views failed: {e}", exc_info=True)
            return pd.DataFrame()
    
    @staticmethod
    def get_industry_analysis(df: pd.DataFrame, group_by: str, time_period: str) -> pd.DataFrame:
        """Analyze performance by Industry or Sector for specific time period
        
        Args:
            df: Source DataFrame
            group_by: Grouping type ("Industry" or "Sector")
            time_period: Time period for analysis
            
        Returns:
            Aggregated DataFrame with industry/sector metrics
        """
        if df is None or df.empty:
            logger.warning("get_industry_analysis: Empty DataFrame received")
            return pd.DataFrame()
        
        try:
            logger.info(f"get_industry_analysis: Processing {len(df)} stocks, group_by={group_by}, period={time_period}")
            
            # Determine grouping column
            if group_by == "Sector" and 'sector' in df.columns:
                group_col = 'sector'
            elif 'industry' in df.columns:
                group_col = 'industry'
            elif 'sector' in df.columns:
                group_col = 'sector'
            else:
                logger.warning("get_industry_analysis: No industry or sector column found")
                return pd.DataFrame()
            
            # Get return column for the selected period
            return_col = SimpleMarketAnalysis.get_return_column_for_period(time_period)
            
            # Clean numeric columns before aggregation
            if return_col in df.columns:
                df[return_col] = pd.to_numeric(df[return_col], errors='coerce')
            
            if 'from_high_pct' in df.columns:
                df['from_high_pct'] = pd.to_numeric(df['from_high_pct'], errors='coerce')
            
            if 'rvol' in df.columns:
                df['rvol'] = pd.to_numeric(df['rvol'], errors='coerce')
            
            if 'master_score' in df.columns:
                df['master_score'] = pd.to_numeric(df['master_score'], errors='coerce')
            
            # Build aggregation dict based on available columns
            agg_dict = {'ticker': 'count'}
            
            if return_col in df.columns:
                agg_dict[return_col] = ['mean', 'median']
            
            if 'rvol' in df.columns:
                agg_dict['rvol'] = 'mean'
                
            if 'master_score' in df.columns:
                agg_dict['master_score'] = 'mean'
                
            # Use actual 52w columns from V9.py
            if 'from_high_pct' in df.columns:
                agg_dict['from_high_pct'] = 'mean'  # Average distance from 52W high
            
            # Calculate metrics by group
            grouped = df.groupby(group_col).agg(agg_dict).round(2)
            
            # Flatten column names
            grouped.columns = ['_'.join(col).strip() if col[1] else col[0] 
                              for col in grouped.columns.values]
            grouped = grouped.reset_index()
            
            # Rename columns for display
            rename_map = {'ticker_count': 'Companies'}
            
            if return_col in df.columns:
                if f'{return_col}_mean' in grouped.columns:
                    rename_map[f'{return_col}_mean'] = 'Avg Return'
                if f'{return_col}_median' in grouped.columns:
                    rename_map[f'{return_col}_median'] = 'Median Return'
            
            if 'rvol_mean' in grouped.columns:
                rename_map['rvol_mean'] = 'Avg Volume'
                
            if 'master_score_mean' in grouped.columns:
                rename_map['master_score_mean'] = 'Avg Score'
                
            if 'from_high_pct_mean' in grouped.columns:
                rename_map['from_high_pct_mean'] = '52WH Distance'  # Distance from 52W high
            
            grouped = grouped.rename(columns=rename_map)
            
            # Sort by performance if available
            if 'Avg Return' in grouped.columns:
                grouped = grouped.sort_values('Avg Return', ascending=False)
            
            # Calculate market outperformance
            if 'Avg Return' in grouped.columns and return_col in df.columns:
                market_return = df[return_col].mean()
                grouped['Outperformance'] = (grouped['Avg Return'] - market_return).round(2)
            
            logger.info(f"get_industry_analysis: Returning {len(grouped)} groups")
            return grouped
            
        except Exception as e:
            logger.error(f"get_industry_analysis failed: {e}", exc_info=True)
            return pd.DataFrame()


class SimpleMarketUI:
    """UI Components for Simple Market Analysis
    
    This class provides Streamlit UI components following V9.py patterns for
    session state management, error handling, and user experience.
    """
    
    @staticmethod
    def render_market_analysis(df: pd.DataFrame):
        """Render market analysis with Stocks and Industries sections
        
        Args:
            df: Source DataFrame with stock data
        """
        if df is None or df.empty:
            st.warning("⚠️ No data available for market analysis")
            logger.warning("render_market_analysis: Empty DataFrame received")
            return
        
        try:
            logger.info(f"render_market_analysis: Rendering with {len(df)} stocks")
            
            # Initialize session states for button persistence (V9.py pattern)
            if 'market_analysis_stock_view' not in st.session_state:
                st.session_state.market_analysis_stock_view = "Gainers"
            if 'market_analysis_industry_filter' not in st.session_state:
                st.session_state.market_analysis_industry_filter = "All"
            
            # Custom CSS for better styling
            st.markdown("""
            <style>
            .stock-header {
                background: linear-gradient(90deg, #1e3c72 0%, #2a5298 100%);
                color: white;
                padding: 10px;
                border-radius: 5px;
                margin-bottom: 10px;
            }
            .industry-header {
                background: linear-gradient(90deg, #16a085 0%, #27ae60 100%);
                color: white;
                padding: 10px;
                border-radius: 5px;
                margin-bottom: 10px;
            }
            </style>
            """, unsafe_allow_html=True)
            
            # Create two columns for the two sections
            col1, col2 = st.columns([1, 1])
            
            # ====================================
            # STOCKS SECTION
            # ====================================
            with col1:
                st.markdown('<div class="stock-header">📊 <b>Stocks</b></div>', unsafe_allow_html=True)
                
                # Create sub-columns for controls
                control_col1, control_col2 = st.columns(2)
                
                with control_col1:
                    # Stock universe filter
                    stock_universe = st.selectbox(
                        "Select Universe",
                        ["Top 500 Stocks", "All Stocks"],
                        key="stock_universe_filter"
                    )
                
                with control_col2:
                    # Time period selector
                    stock_time_period = st.selectbox(
                        "Time Period",
                        ["1 Day", "7 Days", "30 Days", "3 Months", "6 Months", "1 Year"],
                        index=2,  # Default to 30 Days
                        key="stock_time_period"
                    )
                
                # View type buttons
                st.markdown("##### Select View")
                view_cols = st.columns(5)
                
                with view_cols[0]:
                    gainers_btn = st.button("📈 Gainers", key="gainers_btn", width="stretch")
                with view_cols[1]:
                    losers_btn = st.button("📉 Losers", key="losers_btn", width="stretch")
                with view_cols[2]:
                    high_btn = st.button("⬆️ 52W High", key="52w_high_btn", width="stretch")
                with view_cols[3]:
                    low_btn = st.button("⬇️ 52W Low", key="52w_low_btn", width="stretch")
                with view_cols[4]:
                    active_btn = st.button("🔥 Active", key="active_btn", width="stretch")
                
                # Update session state based on button clicks (V9.py pattern)
                if gainers_btn:
                    st.session_state.market_analysis_stock_view = "Gainers"
                elif losers_btn:
                    st.session_state.market_analysis_stock_view = "Losers"
                elif high_btn:
                    st.session_state.market_analysis_stock_view = "52W High"
                elif low_btn:
                    st.session_state.market_analysis_stock_view = "52W Low"
                elif active_btn:
                    st.session_state.market_analysis_stock_view = "Active"
                
                # Use the stored session state value
                stock_view = st.session_state.market_analysis_stock_view
                
                # Show current selection
                st.info(f"📊 Current View: **{stock_view}**")
                
                # Get filtered data
                stocks_df = SimpleMarketAnalysis.get_stock_views(df, stock_view, stock_time_period, stock_universe)
                
                if stocks_df.empty:
                    st.info(f"No stocks found for {stock_view} in {stock_time_period}")
                else:
                    # Determine which columns to display based on view
                    return_col = SimpleMarketAnalysis.get_return_column_for_period(stock_time_period)
                    
                    # Check if return column exists for gainers/losers view
                    if stock_view in ["Gainers", "Losers"] and return_col not in stocks_df.columns:
                        st.warning(f"⚠️ Data for {stock_time_period} not available")
                    else:
                        # Determine company column name (V9.py uses 'company_name')
                        company_col = None
                        if 'company_name' in stocks_df.columns:
                            company_col = 'company_name'
                        elif 'company' in stocks_df.columns:
                            company_col = 'company'
                        
                        # Build display columns based on view type
                        if stock_view == "52W Low":
                            base_cols = ['ticker', 'from_low_pct', 'price']
                            if company_col:
                                display_cols = ['ticker', company_col, 'from_low_pct', 'price']
                            else:
                                display_cols = base_cols
                            metric_col = 'from_low_pct'
                            metric_label = f"From 52W Low"
                            
                        elif stock_view == "52W High":
                            base_cols = ['ticker', 'from_high_pct', 'price']
                            if company_col:
                                display_cols = ['ticker', company_col, 'from_high_pct', 'price']
                            else:
                                display_cols = base_cols
                            metric_col = 'from_high_pct'
                            metric_label = f"From 52W High"
                            
                        elif stock_view in ["Gainers", "Losers"]:
                            base_cols = ['ticker', return_col, 'price']
                            if company_col:
                                display_cols = ['ticker', company_col, return_col, 'price']
                            else:
                                display_cols = base_cols
                            metric_col = return_col
                            metric_label = f"{stock_time_period}"
                            
                        elif stock_view == "Active":
                            vol_col = 'volume_30d' if 'volume_30d' in stocks_df.columns else 'rvol'
                            base_cols = ['ticker', vol_col, 'price']
                            if company_col:
                                display_cols = ['ticker', company_col, vol_col, 'price']
                            else:
                                display_cols = base_cols
                            metric_col = vol_col
                            metric_label = "Volume" if vol_col == 'volume_30d' else "Rel Volume"
                            
                        else:
                            base_cols = ['ticker', 'master_score', 'price']
                            if company_col:
                                display_cols = ['ticker', company_col, 'master_score', 'price']
                            else:
                                display_cols = base_cols
                            metric_col = 'master_score'
                            metric_label = "Score"
                        
                        # Filter available columns
                        available_cols = [col for col in display_cols if col in stocks_df.columns]
                        display_df = stocks_df[available_cols].head(100).copy()
                        
                        # Format the display (V9.py formatting pattern)
                        if 'price' in display_df.columns:
                            display_df['Price'] = display_df['price'].apply(lambda x: f"₹{x:,.0f}" if pd.notna(x) else '-')
                            display_df = display_df.drop('price', axis=1)
                        
                        if metric_col in display_df.columns and metric_col != 'price':
                            if 'ret' in metric_col or 'pct' in metric_col:
                                display_df[metric_label] = display_df[metric_col].apply(
                                    lambda x: f"{x:+.2f}%" if pd.notna(x) else '-'
                                )
                            elif metric_col == 'rvol':
                                display_df[metric_label] = display_df[metric_col].apply(
                                    lambda x: f"{x:.2f}x" if pd.notna(x) else '-'
                                )
                            elif metric_col == 'volume_30d':
                                display_df[metric_label] = display_df[metric_col].apply(
                                    lambda x: f"{x/1000000:.1f}M" if pd.notna(x) and x > 1000000 
                                    else f"{x/1000:.1f}K" if pd.notna(x) and x > 1000
                                    else f"{x:.0f}" if pd.notna(x) else '-'
                                )
                            else:
                                display_df[metric_label] = display_df[metric_col].apply(
                                    lambda x: f"{x:.0f}" if pd.notna(x) else '-'
                                )
                            
                            # Drop original column if we created a formatted version
                            if metric_label != metric_col:
                                display_df = display_df.drop(metric_col, axis=1)
                        
                        # Rename columns for display
                        rename_map = {'ticker': 'Symbol'}
                        if company_col:
                            rename_map[company_col] = 'Company'
                        display_df = display_df.rename(columns=rename_map)
                        
                        # Display the table with enhanced height for 100 stocks
                        st.dataframe(
                            display_df,
                            width="stretch",
                            hide_index=True,
                            height=600
                        )
                        
                        # Show summary with enhanced information
                        st.caption(f"📊 {stock_view} • {stock_time_period} • {stock_universe} • Showing {len(display_df)} of {len(stocks_df)} stocks")
            
            # ====================================
            # INDUSTRIES/SECTORS SECTION
            # ====================================
            with col2:
                st.markdown('<div class="industry-header">🏭 <b>Industries</b></div>', unsafe_allow_html=True)
                
                # Create sub-columns for controls
                ind_col1, ind_col2 = st.columns(2)
                
                with ind_col1:
                    # Industry/Sector selector
                    group_type = st.selectbox(
                        "Group By",
                        ["Industry", "Sector"],
                        key="industry_sector_selector"
                    )
                
                with ind_col2:
                    # Time period selector
                    industry_time_period = st.selectbox(
                        "Time Period",
                        ["1 Day", "7 Days", "30 Days", "3 Months", "6 Months", "1 Year"],
                        index=2,  # Default to 30 Days
                        key="industry_time_period"
                    )
                
                # Performance filter buttons
                st.markdown("##### Performance Filter")
                perf_cols = st.columns(2)
                
                with perf_cols[0]:
                    outperformers_btn = st.button("🚀 Outperformers", key="outperformers_btn", width="stretch")
                with perf_cols[1]:
                    underperformers_btn = st.button("📉 Underperformers", key="underperformers_btn", width="stretch")
                
                # Update session state based on button clicks (V9.py pattern)
                if outperformers_btn:
                    st.session_state.market_analysis_industry_filter = "Outperformers"
                elif underperformers_btn:
                    st.session_state.market_analysis_industry_filter = "Underperformers"
                
                # Show current filter
                filter_status = st.session_state.market_analysis_industry_filter
                if filter_status != "All":
                    st.info(f"🏭 Showing: **{filter_status}**")
                
                # Get industry/sector analysis
                industry_df = SimpleMarketAnalysis.get_industry_analysis(df, group_type, industry_time_period)
                
                if industry_df.empty:
                    st.info(f"No {group_type.lower()} data available for {industry_time_period}")
                else:
                    # Filter based on session state
                    if st.session_state.market_analysis_industry_filter == "Outperformers" and 'Avg Return' in industry_df.columns:
                        industry_df = industry_df[industry_df['Avg Return'] > 0]
                    elif st.session_state.market_analysis_industry_filter == "Underperformers" and 'Avg Return' in industry_df.columns:
                        industry_df = industry_df[industry_df['Avg Return'] < 0]
                    
                    if not industry_df.empty:
                        # Prepare display dataframe
                        display_cols = []
                        
                        # Industry/Sector column
                        group_col = group_type.lower() if group_type.lower() in industry_df.columns else industry_df.columns[0]
                        display_cols.append(group_col)
                        
                        # Add available columns
                        if 'Companies' in industry_df.columns:
                            display_cols.append('Companies')
                        if 'Avg Return' in industry_df.columns:
                            display_cols.append('Avg Return')
                        if '52WH Distance' in industry_df.columns:
                            display_cols.append('52WH Distance')
                        
                        # Filter to available columns
                        available_display = [col for col in display_cols if col in industry_df.columns]
                        industry_display = industry_df[available_display].head(50).copy()
                        
                        # Format the display
                        final_display = pd.DataFrame()
                        
                        # Industry/Sector column with company count
                        if group_col in industry_display.columns:
                            column_name = group_type  # "Industry" or "Sector"
                            final_display[column_name] = industry_display[group_col]
                            if 'Companies' in industry_display.columns:
                                final_display[column_name] = final_display[column_name] + '\n' + industry_display['Companies'].apply(lambda x: f"{x} companies")
                        
                        # Returns column with indicator
                        if 'Avg Return' in industry_display.columns:
                            def get_arrow(val):
                                if pd.isna(val):
                                    return ""
                                elif val > 0:
                                    return "▲"
                                else:
                                    return "▼"
                            
                            arrows = industry_display['Avg Return'].apply(get_arrow)
                            returns = industry_display['Avg Return'].apply(lambda x: f"{x:+.1f}%" if pd.notna(x) else '-')
                            
                            return_col_name = f"{industry_time_period}"
                            final_display[return_col_name] = arrows + " " + returns
                            final_display['_return_value'] = industry_display['Avg Return']
                        
                        # 52WH Distance column
                        if '52WH Distance' in industry_display.columns:
                            final_display['52WH Distance'] = industry_display['52WH Distance'].apply(
                                lambda x: f"{x:.1f}%" if pd.notna(x) else '-'
                            )
                        
                        # Remove internal columns
                        display_columns = [col for col in final_display.columns if not col.startswith('_')]
                        industry_display_clean = final_display[display_columns]
                        
                        # Build column config
                        column_config = {}
                        
                        if "52WH Distance" in industry_display_clean.columns:
                            column_config["52WH Distance"] = st.column_config.TextColumn(
                                "52WH Distance", 
                                width="small",
                                help="Average distance from 52-week high"
                            )
                        
                        if group_type in industry_display_clean.columns:
                            column_config[group_type] = st.column_config.TextColumn(
                                group_type,
                                width="large",
                                help=f"{group_type} name and company count"
                            )
                        
                        if industry_time_period in industry_display_clean.columns:
                            column_config[industry_time_period] = st.column_config.TextColumn(
                                industry_time_period,
                                width="small",
                                help=f"Average {industry_time_period.lower()} returns"
                            )
                        
                        # Display the table with enhanced height for 50 industries/sectors
                        st.dataframe(
                            industry_display_clean,
                            width="stretch",
                            hide_index=True,
                            height=700,
                            column_config=column_config
                        )
                        
                        # Show summary with enhanced information
                        filter_text = "Outperformers" if st.session_state.market_analysis_industry_filter == "Outperformers" else "Underperformers" if st.session_state.market_analysis_industry_filter == "Underperformers" else "All"
                        st.caption(f"🏭 {filter_text} {group_type}s • {industry_time_period} • Showing {len(industry_display_clean)} of {len(industry_df)} items")
                    else:
                        st.info(f"No {filter_status.lower()} found for {group_type.lower()}s in {industry_time_period}")
        
        except Exception as e:
            logger.error(f"render_market_analysis failed: {e}", exc_info=True)
            st.error(f"⚠️ An error occurred while rendering the market analysis")


# ============================================
# PATTERN DETECTION ENGINE - FULLY OPTIMIZED & FIXED
# ============================================

class PatternDetector:
    """
    Advanced pattern detection using vectorized operations for maximum performance.
    This class identifies a comprehensive set of 41 technical, fundamental,
    and intelligent trading patterns.
    FIXED: Pattern confidence calculation now works correctly.
    """

    # Pattern metadata for intelligent confidence scoring
    PATTERN_METADATA = {
            '🐱 CAT LEADER': {'importance_weight': 10, 'category': 'momentum'},
            '💎 HIDDEN GEM': {'importance_weight': 10, 'category': 'value'},
            '🏦 INSTITUTIONAL': {'importance_weight': 10, 'category': 'volume'},
            '⚡ VOL EXPLOSION': {'importance_weight': 15, 'category': 'volume'},
            '👑 MARKET LEADER': {'importance_weight': 10, 'category': 'leadership'},
            '🌊 MOMENTUM WAVE': {'importance_weight': 10, 'category': 'momentum'},
            '💰 LIQUID LEADER': {'importance_weight': 10, 'category': 'liquidity'},
            '🔥 PREMIUM MOMENTUM': {'importance_weight': 15, 'category': 'premium'},
            '🧩 ENTROPY COMPRESSION': {'importance_weight': 20, 'category': 'mathematical'},
            '🚀 VELOCITY BREAKOUT': {'importance_weight': 15, 'category': 'acceleration'},
            '🌋 INSTITUTIONAL TSUNAMI': {'importance_weight': 25, 'category': 'institutional'},
            '📈 VALUE MOMENTUM': {'importance_weight': 10, 'category': 'fundamental'},
            '🎯 EARNINGS ROCKET': {'importance_weight': 10, 'category': 'fundamental'},
            '🏆 QUALITY LEADER': {'importance_weight': 10, 'category': 'fundamental'},
            '🔄 TURNAROUND': {'importance_weight': 10, 'category': 'fundamental'},
            '⚠️ HIGH PE': {'importance_weight': -5, 'category': 'warning'},
            '🎲 52W HIGH APPROACH': {'importance_weight': 10, 'category': 'range'},
            '↗️ 52W LOW BOUNCE': {'importance_weight': 10, 'category': 'range'},
            '🔀 MOMENTUM DIVERGE': {'importance_weight': 10, 'category': 'divergence'},
            '🤏 RANGE COMPRESS': {'importance_weight': 5, 'category': 'range'},
            '🤫 STEALTH': {'importance_weight': 10, 'category': 'hidden'},
            '🏎️ ACCELERATION': {'importance_weight': 10, 'category': 'aggressive'},
            '⛈️ PERFECT STORM': {'importance_weight': 20, 'category': 'extreme'},
            '🪤 BULL TRAP': {'importance_weight': 15, 'category': 'reversal'},
            '💣 CAPITULATION': {'importance_weight': 20, 'category': 'reversal'},
            '🏃 RUNAWAY GAP': {'importance_weight': 12, 'category': 'continuation'},
            '🔃 ROTATION LEADER': {'importance_weight': 10, 'category': 'rotation'},
            '📊 DISTRIBUTION': {'importance_weight': 15, 'category': 'warning'},
            '🗜️ VELOCITY SQUEEZE': {'importance_weight': 15, 'category': 'coiled'},
            '🔉 VOLUME DIVERGENCE': {'importance_weight': -10, 'category': 'warning'},
            '✨ GOLDEN CROSS': {'importance_weight': 12, 'category': 'bullish'},
            '📉 EXHAUSTION': {'importance_weight': -15, 'category': 'bearish'},
            '🔺 PYRAMID': {'importance_weight': 8, 'category': 'accumulation'},
            '🌪️ VACUUM': {'importance_weight': 18, 'category': 'reversal'},
            '🎆 EARNINGS SURPRISE LEADER': {'importance_weight': 22, 'category': 'fundamental'},
            '🕰️ INFORMATION DECAY ARBITRAGE': {'importance_weight': 25, 'category': 'mathematical'},
            '🐦 PHOENIX RISING': {'importance_weight': 28, 'category': 'transformation'},
            '⚛️ ATOMIC DECAY MOMENTUM': {'importance_weight': 20, 'category': 'physics'},
            '💹 GARP LEADER': {'importance_weight': 18, 'category': 'fundamental'},
            '🛡️ PULLBACK SUPPORT': {'importance_weight': 12, 'category': 'technical'},
            '💳 OVERSOLD QUALITY': {'importance_weight': 15, 'category': 'value'}
    }

    @staticmethod
    @PerformanceMonitor.timer(target_time=0.3)
    def detect_all_patterns_optimized(df: pd.DataFrame) -> pd.DataFrame:
        """
        Detects all trading patterns using highly efficient vectorized operations.
        Returns a DataFrame with 'patterns' column and 'pattern_confidence' score.
        """
        if df.empty:
            df['patterns'] = ''
            df['pattern_confidence'] = 0.0
            df['pattern_count'] = 0
            df['pattern_categories'] = ''
            return df
        
        logger.info(f"Starting pattern detection for {len(df)} stocks...")
        
        # Get all pattern definitions
        patterns_with_masks = PatternDetector._get_all_pattern_definitions(df)
        
        # Create pattern matrix for vectorized processing
        pattern_names = [name for name, _ in patterns_with_masks]
        pattern_matrix = pd.DataFrame(False, index=df.index, columns=pattern_names)
        
        # Fill pattern matrix with detection results
        patterns_detected = 0
        for name, mask in patterns_with_masks:
            if mask is not None:
                # Convert mask to pandas Series if it's a numpy array
                if isinstance(mask, np.ndarray):
                    mask = pd.Series(mask, index=df.index)
                elif not isinstance(mask, pd.Series):
                    mask = pd.Series(mask, index=df.index)
                
                # Check if mask has any data
                if len(mask) > 0:
                    pattern_matrix[name] = mask.reindex(df.index, fill_value=False)
                    detected_count = mask.sum() if hasattr(mask, 'sum') else np.sum(mask)
                    if detected_count > 0:
                        patterns_detected += 1
                        logger.debug(f"Pattern '{name}' detected in {detected_count} stocks")
        
        # Combine patterns into string column
        df['patterns'] = pattern_matrix.apply(
            lambda row: ' | '.join(row.index[row].tolist()), axis=1
        )
        df['patterns'] = df['patterns'].fillna('')
        
        # Count patterns per stock
        df['pattern_count'] = pattern_matrix.sum(axis=1)
        
        # Calculate pattern categories
        df['pattern_categories'] = pattern_matrix.apply(
            lambda row: PatternDetector._get_pattern_categories(row), axis=1
        )
        
        # Calculate confidence score with FIXED calculation
        df = PatternDetector._calculate_pattern_confidence(df)
        
        # Log summary
        stocks_with_patterns = (df['patterns'] != '').sum()
        avg_patterns_per_stock = df['pattern_count'].mean()
        logger.info(f"Pattern detection complete: {patterns_detected} patterns found, "
                   f"{stocks_with_patterns} stocks with patterns, "
                   f"avg {avg_patterns_per_stock:.1f} patterns/stock")
        
        return df

    @staticmethod
    def _calculate_pattern_confidence(df: pd.DataFrame) -> pd.DataFrame:
        """
        FIXED: Calculate confidence score based on pattern importance weights.
        Now properly calculates max_possible_score.
        """
        
        # Calculate maximum possible score for normalization
        all_positive_weights = [
            abs(meta['importance_weight']) 
            for meta in PatternDetector.PATTERN_METADATA.values()
            if meta['importance_weight'] > 0
        ]
        max_possible_score = sum(sorted(all_positive_weights, reverse=True)[:5])  # Top 5 patterns
        
        def calculate_confidence(patterns_str):
            """Calculate confidence for a single stock's patterns"""
            if pd.isna(patterns_str) or patterns_str == '':
                return 0.0
            
            patterns = [p.strip() for p in patterns_str.split(' | ')]
            total_weight = 0
            pattern_categories = set()
            
            for pattern in patterns:
                # Match pattern with metadata (handle emoji differences)
                for key, meta in PatternDetector.PATTERN_METADATA.items():
                    if pattern == key or pattern.replace(' ', '') == key.replace(' ', ''):
                        total_weight += meta['importance_weight']
                        pattern_categories.add(meta.get('category', 'unknown'))
                        break
            
            # Bonus for diverse categories
            category_bonus = len(pattern_categories) * 2
            
            # Calculate final confidence
            if max_possible_score > 0:
                raw_confidence = (abs(total_weight) + category_bonus) / max_possible_score * 100
                # Apply sigmoid smoothing for better distribution
                confidence = 100 * (2 / (1 + np.exp(-raw_confidence/50)) - 1)
                return min(100, max(0, confidence))
            return 0.0
        
        # Apply calculation to all rows
        df['pattern_confidence'] = df['patterns'].apply(calculate_confidence).round(2)
        
        # Add confidence tier
        df['confidence_tier'] = pd.cut(
            df['pattern_confidence'],
            bins=[0, 25, 50, 75, 100],
            labels=['Low', 'Medium', 'High', 'Very High'],
            include_lowest=True
        )
        
        return df
    
    @staticmethod
    def _get_pattern_categories(row: pd.Series) -> str:
        """Get unique categories for detected patterns"""
        categories = set()
        for pattern_name in row.index[row]:
            for key, meta in PatternDetector.PATTERN_METADATA.items():
                if pattern_name == key or pattern_name.replace(' ', '') == key.replace(' ', ''):
                    categories.add(meta.get('category', 'unknown'))
                    break
        return ', '.join(sorted(categories)) if categories else ''

    @staticmethod
    def _get_all_pattern_definitions(df: pd.DataFrame) -> List[Tuple[str, pd.Series]]:
        """
        Defines all 41 patterns using vectorized boolean masks.
        FIXED: Uses raw data instead of scores that don't exist yet.
        Returns list of (pattern_name, mask) tuples.
        """
        patterns = []
        
        # Helper function to safely get column data
        def get_col_safe(col_name: str, default_value: Any = np.nan) -> pd.Series:
            if col_name in df.columns:
                return df[col_name].copy()
            return pd.Series(default_value, index=df.index)
        
        # Helper function to ensure mask is a pandas Series
        def ensure_series(mask: Any) -> pd.Series:
            if isinstance(mask, pd.Series):
                return mask
            elif isinstance(mask, np.ndarray):
                return pd.Series(mask, index=df.index)
            else:
                return pd.Series(mask, index=df.index)
    
        # ========== MOMENTUM & LEADERSHIP PATTERNS (1-11) ==========
        
        # 1. Category Leader - FIXED: category_percentile might not exist yet
        # The column exists, but we'll validate the threshold is working
        cat_percentile = get_col_safe('category_percentile', 0)
        threshold = CONFIG.PATTERN_THRESHOLDS.get('category_leader', 90)
        
        # Create mask
        mask = ensure_series(cat_percentile >= threshold)
        
        # Log for debugging (optional - remove in production)
        leaders_count = mask.sum()
        if leaders_count > 0:
            logger.debug(f"CAT LEADER: Found {leaders_count} category leaders (threshold: {threshold})")
        
        patterns.append(('🐱 CAT LEADER', mask))
        
        # 2. Hidden Gem - FIXED: percentile might not exist yet
        mask = ensure_series(
            (get_col_safe('category_percentile', 0) >= CONFIG.PATTERN_THRESHOLDS.get('hidden_gem', 80)) & 
            (get_col_safe('percentile', 100) < 70)
        )
        patterns.append(('💎 HIDDEN GEM', mask))
        
        # 3. Institutional - FIXED: Use raw volume ratios
        if all(col in df.columns for col in ['vol_ratio_90d_180d', 'vol_ratio_30d_90d', 'ret_3m', 'from_low_pct', 'from_high_pct']):
            mask = (
                (get_col_safe('vol_ratio_90d_180d', 1) > 1.2) &
                (get_col_safe('vol_ratio_30d_90d', 1).between(0.9, 1.1)) &
                (get_col_safe('ret_3m', 0).between(5, 25)) &
                (get_col_safe('from_low_pct', 0) > 30) &
                (get_col_safe('from_high_pct', -100) > -30)
            )
        else:
            mask = pd.Series(False, index=df.index)
        patterns.append(('🏦 INSTITUTIONAL', mask))
        
        # 4. Volume Explosion - Works with raw data
        mask = get_col_safe('rvol', 0) > 3
        patterns.append(('⚡ VOL EXPLOSION', mask))
        
        # 5. Market Leader - FIXED: percentile might not exist
        if 'percentile' in df.columns:
            mask = get_col_safe('percentile', 0) >= CONFIG.PATTERN_THRESHOLDS.get('market_leader', 95)
        else:
            mask = pd.Series(False, index=df.index)
        patterns.append(('👑 MARKET LEADER', mask))
        
        # 6. Momentum Wave - FIXED: Use return data directly
        if all(col in df.columns for col in ['ret_7d', 'ret_30d', 'rvol']):
            ret_7d = get_col_safe('ret_7d', 0)
            ret_30d = get_col_safe('ret_30d', 0)
            
            # Calculate acceleration
            with np.errstate(divide='ignore', invalid='ignore'):
                daily_7d = ret_7d / 7
                daily_30d = ret_30d / 30
                accelerating = (daily_7d > daily_30d * 1.2)  # 20% faster pace
            
            mask = (
                (ret_30d >= 15) &  # Strong 30-day momentum
                (ret_7d >= 5) &    # Good 7-day momentum
                accelerating &      # Acceleration confirmed
                (get_col_safe('rvol', 1) > 1.5)  # Volume confirmation
            )
        else:
            mask = pd.Series(False, index=df.index)
        patterns.append(('🌊 MOMENTUM WAVE', mask))
        
        # 7. Liquid Leader - FIXED: Check if liquidity_score exists
        if 'liquidity_score' in df.columns and 'percentile' in df.columns:
            mask = (
                (get_col_safe('liquidity_score', 0) >= CONFIG.PATTERN_THRESHOLDS.get('liquid_leader', 80)) & 
                (get_col_safe('percentile', 0) >= CONFIG.PATTERN_THRESHOLDS.get('liquid_leader', 80))
            )
        else:
            # Use volume as proxy for liquidity
            mask = (
                (get_col_safe('volume_1d', 0) > df['volume_1d'].median() * 2) &
                (get_col_safe('rvol', 1) > 1.5)
            )
        patterns.append(('💰 LIQUID LEADER', mask))
        
        # 8. Premium Momentum Grade - FIXED: Use master_score if available
        try:
            ret_1d = get_col_safe('ret_1d', 0)
            ret_7d = get_col_safe('ret_7d', 0)
            ret_30d = get_col_safe('ret_30d', 0)
            
            # Build momentum DNA without master_score
            momentum_dna_score = (
                np.where((ret_1d > 0) & (ret_7d > 0) & (ret_30d > 0), 25, 0) +
                np.where(ret_7d > safe_divide(ret_30d, 4, default=0), 30, 0) +
                np.where(get_col_safe('rvol', 1) > 1.2, 20, 0) +
                np.where(ret_30d > 20, 25, 0)  # Use return instead of score
            )
            
            mask = (
                (momentum_dna_score >= 75) &
                (get_col_safe('from_low_pct', 0) > 15) &
                (get_col_safe('from_high_pct', 0) > -15) &
                (get_col_safe('vol_ratio_7d_90d', 1) > 1.1)
            )
        except Exception:
            mask = pd.Series(False, index=df.index)
        patterns.append(('🔥 PREMIUM MOMENTUM', mask))

        # 9. Entropy Compression - Volatility breakout prediction using information theory
        # FIXED: Properly implements entropy compression detection with actual data structure
        try:
            if all(col in df.columns for col in ['ret_1d', 'ret_7d', 'ret_30d', 'volume_1d', 'volume_30d', 'volume_90d', 'rvol']):
                ret_1d = get_col_safe('ret_1d', 0)
                ret_7d = get_col_safe('ret_7d', 0)
                ret_30d = get_col_safe('ret_30d', 0)
                
                # FIXED: Calculate volatility compression (not rolling - we have point-in-time data)
                # Entropy Metric 1: Return volatility compression
                # Compare short-term vs long-term volatility using actual returns
                with np.errstate(divide='ignore', invalid='ignore'):
                    # Daily equivalent volatility
                    daily_vol_7d = np.abs(ret_7d / 7)    # Average daily volatility over 7 days
                    daily_vol_30d = np.abs(ret_30d / 30)  # Average daily volatility over 30 days
                    
                    # Volatility compression: short-term vol < long-term vol (tightening range)
                    volatility_compressed = (
                        (daily_vol_7d < daily_vol_30d * 0.7) &  # 7-day vol is 30% lower than 30-day
                        (daily_vol_7d < 2)  # And absolute volatility is low (< 2% daily)
                    )
                
                # Entropy Metric 2: Volume stability (low entropy in volume)
                volume_1d = get_col_safe('volume_1d', 0)
                volume_30d = get_col_safe('volume_30d', 0)
                volume_90d = get_col_safe('volume_90d', 0)
                
                with np.errstate(divide='ignore', invalid='ignore'):
                    # Volume consistency check
                    vol_ratio_1d_30d = np.where(volume_30d > 0, volume_1d / volume_30d, 1)
                    vol_ratio_30d_90d = np.where(volume_90d > 0, volume_30d / volume_90d, 1)
                    
                    # Stable volume = ratios close to 1 (low entropy)
                    volume_stable = (
                        (vol_ratio_1d_30d > 0.7) & (vol_ratio_1d_30d < 1.5) &  # Daily volume within normal range
                        (vol_ratio_30d_90d > 0.8) & (vol_ratio_30d_90d < 1.2)   # Monthly volume stable
                    )
                
                # Entropy Metric 3: Price range compression
                high_52w = get_col_safe('high_52w', 0)
                low_52w = get_col_safe('low_52w', 0)
                price = get_col_safe('price', 0)
                from_low_pct = get_col_safe('from_low_pct', 0)
                
                with np.errstate(divide='ignore', invalid='ignore'):
                    # Calculate 52-week range as percentage
                    range_pct = np.where(low_52w > 0, ((high_52w - low_52w) / low_52w) * 100, 100)
                    
                    # Price compression: tight range + middle position
                    price_compressed = (
                        (range_pct < 50) &  # Less than 50% range in 52 weeks
                        (from_low_pct > 30) & (from_low_pct < 70)  # In middle 40% of range
                    )
                
                # Entropy Metric 4: Technical structure alignment (order from chaos)
                sma_20d = get_col_safe('sma_20d', 0)
                sma_50d = get_col_safe('sma_50d', 0)
                sma_200d = get_col_safe('sma_200d', 0)
                
                # Price structure shows order (bullish alignment)
                price_structure_aligned = (
                    (price > sma_20d) &
                    (sma_20d > sma_50d) &
                    (sma_50d > sma_200d)
                )
                
                # Catalyst Detection: Signs of energy building
                rvol = get_col_safe('rvol', 1)
                eps_change_pct = get_col_safe('eps_change_pct', 0)
                vol_ratio_7d_90d = get_col_safe('vol_ratio_7d_90d', 1)
                
                # Calculate catalyst score (energy building up)
                catalyst_indicators = (
                    np.where(rvol > 1.2, 1, 0) +  # Volume picking up
                    np.where(eps_change_pct > 10, 1, 0) +  # Earnings momentum
                    np.where(vol_ratio_7d_90d > 1.1, 1, 0) +  # Recent volume trend
                    np.where(ret_7d > 0, 1, 0)  # Positive recent momentum
                )
                
                # FINAL ENTROPY COMPRESSION DETECTION
                # Low entropy (compressed state) + energy building = potential breakout
                mask = ensure_series(
                    volatility_compressed &  # Volatility is compressed
                    volume_stable &  # Volume patterns are stable
                    price_compressed &  # Price range is tight
                    price_structure_aligned &  # Technical structure is ordered
                    (catalyst_indicators >= 2)  # At least 2 catalyst signals
                )
                
                # Log detection stats for debugging
                compression_count = mask.sum() if hasattr(mask, 'sum') else 0
                if compression_count > 0:
                    logger.debug(f"ENTROPY COMPRESSION: {compression_count} stocks detected")
                    
            else:
                # Missing required columns
                logger.warning("ENTROPY COMPRESSION: Missing required columns")
                mask = pd.Series(False, index=df.index)
                
        except Exception as e:
            logger.error(f"Error in ENTROPY COMPRESSION pattern: {str(e)}")
            mask = pd.Series(False, index=df.index)
        
        patterns.append(('🧩 ENTROPY COMPRESSION', mask))
        
        # 10. Velocity Breakout - Multi-timeframe momentum acceleration
        if all(col in df.columns for col in ['ret_1d', 'ret_7d', 'ret_30d', 'ret_3m', 'rvol', 'from_high_pct']):
            mask = (
                (get_col_safe('ret_1d', 0) > 3) &                              # Recent significant pop
                (get_col_safe('ret_7d', 0) > get_col_safe('ret_30d', 1) * 0.5) &  # Weekly pace exceeds monthly
                (get_col_safe('ret_30d', 0) > get_col_safe('ret_3m', 1) * 0.7) &  # Monthly pace exceeds quarterly
                (get_col_safe('rvol', 0) > 2) &                                # Strong volume confirmation
                (get_col_safe('from_high_pct', -100) > -15)                    # Near highs for continuation
            )
        else:
            mask = pd.Series(False, index=df.index)
        patterns.append(('🚀 VELOCITY BREAKOUT', mask))

        # 11. Institutional Tsunami - Multi-dimensional confluence scoring
        try:
            if all(col in df.columns for col in ['vol_ratio_7d_90d', 'vol_ratio_30d_90d', 'vol_ratio_90d_180d', 'ret_1y', 'from_high_pct', 'ret_7d', 'ret_30d', 'pe', 'eps_change_pct']):
                ret_7d = get_col_safe('ret_7d', 0)
                ret_30d = get_col_safe('ret_30d', 0)
                ret_1y = get_col_safe('ret_1y', 0)
                pe = get_col_safe('pe')
                eps_change_pct = get_col_safe('eps_change_pct')
                
                # Multi-timeframe volume tsunami scoring (75 points max)
                vol_tsunami_score = (
                    np.where(get_col_safe('vol_ratio_7d_90d', 1) > 2.0, 30, 0) +
                    np.where(get_col_safe('vol_ratio_30d_90d', 1) > 1.5, 25, 0) +
                    np.where(get_col_safe('vol_ratio_90d_180d', 1) > 1.3, 20, 0)
                )
                
                # Hidden strength - institutions accumulating quietly (25 points)
                hidden_strength = np.where(
                    (ret_1y > 50) & (get_col_safe('from_high_pct', -100) < -20), 
                    25, 0
                )
                
                # Fresh acceleration confirmation (15 points)
                with np.errstate(divide='ignore', invalid='ignore'):
                    fresh_accel = np.where(ret_7d > ret_30d / 4, 15, 0)
                
                # Quality confirmation - profitable growth (15 points)
                quality_conf = np.where(
                    pe.notna() & (pe < 30) & eps_change_pct.notna() & (eps_change_pct > 20), 
                    15, 0
                )
                
                # Total tsunami score (max 130 points)
                tsunami_score = vol_tsunami_score + hidden_strength + fresh_accel + quality_conf
                
                # Requires ≥90 points for institutional tsunami pattern
                mask = (tsunami_score >= 90)
            else:
                mask = pd.Series(False, index=df.index)
        except Exception as e:
            mask = pd.Series(False, index=df.index)
            logger.warning(f"Error in INSTITUTIONAL TSUNAMI pattern: {e}")
        patterns.append(('🌋 INSTITUTIONAL TSUNAMI', mask))

        # ========== FUNDAMENTAL PATTERNS (12-16) ==========
    
        # 12. Value Momentum - FIXED: Check master_score existence
        pe = get_col_safe('pe')
        if 'master_score' in df.columns:
            mask = pe.notna() & (pe > 0) & (pe < 15) & (get_col_safe('master_score', 0) >= 70)
        else:
            # Use returns as proxy for score
            mask = pe.notna() & (pe > 0) & (pe < 15) & (get_col_safe('ret_30d', 0) > 20)
        patterns.append(('📈 VALUE MOMENTUM', mask))
        
        # 13. Earnings Rocket - FIXED: acceleration_score doesn't exist
        eps_change_pct = get_col_safe('eps_change_pct')
        if 'acceleration_score' in df.columns:
            mask = eps_change_pct.notna() & (eps_change_pct > 50) & (get_col_safe('acceleration_score', 0) >= 70)
        else:
            # Calculate acceleration directly
            if all(col in df.columns for col in ['ret_1d', 'ret_7d', 'ret_30d']):
                with np.errstate(divide='ignore', invalid='ignore'):
                    accel = (get_col_safe('ret_1d', 0) > get_col_safe('ret_7d', 0)/7) & \
                           (get_col_safe('ret_7d', 0)/7 > get_col_safe('ret_30d', 0)/30)
                mask = eps_change_pct.notna() & (eps_change_pct > 50) & accel
            else:
                mask = eps_change_pct.notna() & (eps_change_pct > 50)
        patterns.append(('🎯 EARNINGS ROCKET', mask))

        # 14. Quality Leader - Good PE, EPS growth, and percentile
        if all(col in df.columns for col in ['pe', 'eps_change_pct', 'percentile']):
            pe, eps_change_pct, percentile = get_col_safe('pe'), get_col_safe('eps_change_pct'), get_col_safe('percentile')
            mask = pe.notna() & eps_change_pct.notna() & (pe.between(10, 25)) & (eps_change_pct > 20) & (percentile >= 80)
            patterns.append(('🏆 QUALITY LEADER', mask))
        
        # 15. Turnaround Play - Enhanced 5-factor confirmation
        if all(col in df.columns for col in ['eps_change_pct', 'ret_30d', 'vol_ratio_30d_90d', 'from_low_pct', 'pe']):
            eps_change_pct = get_col_safe('eps_change_pct')
            pe = get_col_safe('pe')
            mask = (
                eps_change_pct.notna() & pe.notna() &
                (eps_change_pct > 100) &                                       # Massive improvement
                (get_col_safe('ret_30d', 0) > 15) &                           # Recent momentum confirmation
                (get_col_safe('vol_ratio_30d_90d', 1) > 1.5) &                # Volume confirmation
                (get_col_safe('from_low_pct', 0) < 60) &                      # Still reasonable entry point
                (pe < 30)                                                      # Valuation protection
            )
        else:
            # Fallback to basic version if enhanced columns not available
            eps_change_pct = get_col_safe('eps_change_pct')
            mask = eps_change_pct.notna() & (eps_change_pct > 100) & (get_col_safe('volume_score', 0) >= 60)
        patterns.append(('🔄 TURNAROUND', mask))
        
        # 16. High PE Warning - Realistic threshold for Indian markets
        pe = get_col_safe('pe')
        mask = pe.notna() & (pe > 50)  # Lowered from 100 to 50 for practical screening
        patterns.append(('⚠️ HIGH PE', mask))

        # ========== RANGE PATTERNS (17-20) ==========
        
        # 17. 52W High Approach
        mask = (
            (get_col_safe('from_high_pct', -100) > -5) & 
            (get_col_safe('volume_score', 0) >= 70) & 
            (get_col_safe('momentum_score', 0) >= 60)
        )
        patterns.append(('🎲 52W HIGH APPROACH', mask))
        
        # 18. 52W Low Bounce
        mask = (
            (get_col_safe('from_low_pct', 100) < 20) & 
            (get_col_safe('acceleration_score', 0) >= 80) & 
            (get_col_safe('ret_30d', 0) > 10)
        )
        patterns.append(('↗️ 52W LOW BOUNCE', mask))
        
        # 19. Momentum Divergence
        if all(col in df.columns for col in ['ret_7d', 'ret_30d', 'acceleration_score', 'rvol']):
            with np.errstate(divide='ignore', invalid='ignore'):
                daily_7d_pace = np.where(df['ret_7d'].fillna(0) != 0, df['ret_7d'].fillna(0) / 7, np.nan)
                daily_30d_pace = np.where(df['ret_30d'].fillna(0) != 0, df['ret_30d'].fillna(0) / 30, np.nan)
            mask = (
                pd.Series(daily_7d_pace > daily_30d_pace * 1.5, index=df.index).fillna(False) & 
                (get_col_safe('acceleration_score', 0) >= 85) & 
                (get_col_safe('rvol', 0) > 2)
            )
            patterns.append(('🔀 MOMENTUM DIVERGE', mask))
        
        # 20. Range Compression - Smart 20-day volatility approach
        if all(col in df.columns for col in ['ret_7d', 'ret_30d', 'from_low_pct', 'rvol']):
            # Calculate recent volatility as proxy for range compression
            volatility_7d = abs(get_col_safe('ret_7d', 0))
            volatility_30d = abs(get_col_safe('ret_30d', 0)) / 4  # Daily equivalent
            from_low_pct = get_col_safe('from_low_pct', 0)
            
            # Range compression: low recent volatility + not at extremes + building volume
            mask = (
                (volatility_7d < 5) &          # Low 7-day volatility (tight range)
                (volatility_30d < 3) &         # Low daily volatility over month  
                (from_low_pct > 20) &          # Not at 52W lows
                (from_low_pct < 80) &          # Not at 52W highs
                (get_col_safe('rvol', 0) > 0.8)  # Some volume interest
            )
            patterns.append(('🤏 RANGE COMPRESS', mask))

        # ========== INTELLIGENCE PATTERNS (21-23) ==========
        
        # 21. Stealth Accumulator
        if all(col in df.columns for col in ['vol_ratio_90d_180d', 'vol_ratio_30d_90d', 'from_low_pct', 'ret_7d', 'ret_30d']):
            ret_7d, ret_30d = get_col_safe('ret_7d'), get_col_safe('ret_30d')
            with np.errstate(divide='ignore', invalid='ignore'):
                ret_ratio = pd.Series(
                    np.where(ret_30d != 0, ret_7d / (ret_30d / 4), np.nan), 
                    index=df.index
                ).fillna(0)
            mask = (
                (get_col_safe('vol_ratio_90d_180d', 1) > 1.1) & 
                (get_col_safe('vol_ratio_30d_90d', 1).between(0.9, 1.1)) & 
                (get_col_safe('from_low_pct', 0) > 40) & 
                (ret_ratio > 1)
            )
            patterns.append(('🤫 STEALTH', mask))

        # 22. 🏎️ ACCELERATION - Momentum acceleration pattern
        if all(col in df.columns for col in ['ret_1d', 'ret_7d', 'rvol', 'from_high_pct', 'category']):
            ret_1d, ret_7d = get_col_safe('ret_1d'), get_col_safe('ret_7d')
            with np.errstate(divide='ignore', invalid='ignore'):
                daily_pace_ratio = pd.Series(
                    np.where(ret_7d != 0, ret_1d / (ret_7d / 7), np.nan), 
                    index=df.index
                ).fillna(0)
            mask = (
                (daily_pace_ratio > 2) & 
                (get_col_safe('rvol', 0) > 3) & 
                (get_col_safe('from_high_pct', -100) > -15) & 
                (df['category'].isin(['Small Cap', 'Micro Cap']))
            )
            patterns.append(('🏎️ ACCELERATION', mask))
        
        # 23. Perfect Storm
        if 'momentum_harmony' in df.columns and 'master_score' in df.columns:
            mask = (
                (get_col_safe('momentum_harmony', 0) == 4) & 
                (get_col_safe('ret_30d', 0) > 20) &  # Use return data instead
                (get_col_safe('rvol', 0) > 2)  # Add volume confirmation
            )
            patterns.append(('⛈️ PERFECT STORM', mask))

        # ========== REVERSAL & CONTINUATION PATTERNS (24-34) ==========
        
        # 24. BULL TRAP - Failed breakout/shorting opportunity
        if all(col in df.columns for col in ['from_high_pct', 'ret_7d', 'volume_7d', 'volume_30d']):
            mask = (
                (get_col_safe('from_high_pct', -100) > -5) &     # Was near 52W high
                (get_col_safe('ret_7d', 0) < -10) &              # Now falling hard
                (get_col_safe('volume_7d', 0) > get_col_safe('volume_30d', 1))  # High volume selling
            )
            patterns.append(('🪤 BULL TRAP', mask))
        
        # 25. CAPITULATION BOTTOM - Panic selling exhaustion
        if all(col in df.columns for col in ['ret_1d', 'from_low_pct', 'rvol', 'volume_1d', 'volume_90d']):
            mask = (
                (get_col_safe('ret_1d', 0) < -7) &               # Huge down day
                (get_col_safe('from_low_pct', 100) < 20) &       # Near 52W low
                (get_col_safe('rvol', 0) > 5) &                  # Extreme volume
                (get_col_safe('volume_1d', 0) > get_col_safe('volume_90d', 1) * 3)  # Panic volume
            )
            patterns.append(('💣 CAPITULATION', mask))
        
        # 26. RUNAWAY GAP - Continuation pattern
        if all(col in df.columns for col in ['price', 'prev_close', 'ret_30d', 'rvol', 'from_high_pct']):
            price = get_col_safe('price', 0)
            prev_close = get_col_safe('prev_close', 1)
            
            # Calculate gap percentage safely
            with np.errstate(divide='ignore', invalid='ignore'):
                gap = np.where(prev_close > 0, 
                              ((price - prev_close) / prev_close) * 100,
                              0)
            gap_series = pd.Series(gap, index=df.index)
            
            mask = (
                (gap_series > 5) &                               # Big gap up
                (get_col_safe('ret_30d', 0) > 20) &             # Already trending
                (get_col_safe('rvol', 0) > 3) &                 # Institutional volume
                (get_col_safe('from_high_pct', -100) > -3)      # Making new highs
            )
            patterns.append(('🏃 RUNAWAY GAP', mask))
        
        # 27. ROTATION LEADER - First mover in sector rotation
        if all(col in df.columns for col in ['ret_7d', 'sector', 'rvol']):
            ret_7d = get_col_safe('ret_7d', 0)
            
            # Calculate sector average return safely
            if 'sector' in df.columns:
                sector_avg = df.groupby('sector')['ret_7d'].transform('mean').fillna(0)
            else:
                sector_avg = pd.Series(0, index=df.index)
            
            mask = (
                (ret_7d > sector_avg + 5) &                      # Beating sector by 5%
                (ret_7d > 0) &                                   # Positive absolute return
                (sector_avg < 0) &                               # Sector still negative
                (get_col_safe('rvol', 0) > 2)                   # Volume confirmation
            )
            patterns.append(('🔃 ROTATION LEADER', mask))
        
        # 28. DISTRIBUTION TOP - Smart money selling
        if all(col in df.columns for col in ['from_high_pct', 'rvol', 'ret_1d', 'ret_30d', 'volume_7d', 'volume_30d']):
            mask = (
                (get_col_safe('from_high_pct', -100) > -10) &    # Near highs
                (get_col_safe('rvol', 0) > 2) &                  # High volume
                (get_col_safe('ret_1d', 0) < 2) &                # Price not moving up
                (get_col_safe('ret_30d', 0) > 25) &              # After reasonable rally (reduced from 50%)
                (get_col_safe('volume_7d', 0) > get_col_safe('volume_30d', 1) * 1.5)  # Volume spike
            )
            patterns.append(('📊 DISTRIBUTION', mask))

        # 29. VELOCITY SQUEEZE
        if all(col in df.columns for col in ['ret_7d', 'ret_30d', 'from_high_pct', 'from_low_pct', 'high_52w', 'low_52w']):
            with np.errstate(divide='ignore', invalid='ignore'):
                daily_7d = np.where(df['ret_7d'] != 0, df['ret_7d'] / 7, 0)
                daily_30d = np.where(df['ret_30d'] != 0, df['ret_30d'] / 30, 0)
                range_pct = np.where(df['low_52w'] > 0, 
                                   (df['high_52w'] - df['low_52w']) / df['low_52w'], 
                                   np.inf)
            
            mask = (
                (daily_7d > daily_30d) &  # Velocity increasing
                (abs(df['from_high_pct']) + df['from_low_pct'] < 30) &  # Middle of range
                (range_pct < 0.5)  # Tight range
            )
            patterns.append(('🗜️ VELOCITY SQUEEZE', mask))
        
        # 30. VOLUME DIVERGENCE WARNING - Clarified bearish divergence logic
        if all(col in df.columns for col in ['ret_7d', 'ret_30d', 'vol_ratio_7d_90d', 'vol_ratio_30d_90d', 'from_high_pct']):
            # Classic bearish divergence: price making new highs but volume declining
            mask = (
                (df['ret_7d'] > 5) &              # Recent price strength
                (df['ret_30d'] > 15) &            # Monthly price advance
                (df['vol_ratio_7d_90d'] < 0.8) &  # Recent volume declining
                (df['vol_ratio_30d_90d'] < 0.9) & # Monthly volume declining
                (df['from_high_pct'] > -10)       # Near highs but volume not confirming
            )
            patterns.append(('🔉 VOLUME DIVERGENCE', mask))
        
        # 31. GOLDEN CROSS MOMENTUM
        if all(col in df.columns for col in ['sma_20d', 'sma_50d', 'sma_200d', 'rvol', 'ret_7d', 'ret_30d']):
            mask = (
                (df['sma_20d'] > df['sma_50d']) &
                (df['sma_50d'] > df['sma_200d']) &
                ((df['sma_20d'] - df['sma_50d']) / df['sma_50d'] > 0.02) &
                (df['rvol'] > 1.5) &
                (df['ret_7d'] > df['ret_30d'] / 4)
            )
            patterns.append(('✨ GOLDEN CROSS', mask))
        
        # 32. MOMENTUM EXHAUSTION
        if all(col in df.columns for col in ['ret_7d', 'ret_1d', 'rvol', 'from_low_pct', 'price', 'sma_20d']):
            with np.errstate(divide='ignore', invalid='ignore'):
                sma_deviation = np.where(df['sma_20d'] > 0,
                                        (df['price'] - df['sma_20d']) / df['sma_20d'],
                                        0)
            
            # Handle RVOL shift safely
            rvol_shifted = df['rvol'].shift(1).fillna(df['rvol'].median())
            
            mask = (
                (df['ret_7d'] > 25) &
                (df['ret_1d'] < 0) &
                (df['rvol'] < rvol_shifted) &
                (df['from_low_pct'] > 80) &
                (sma_deviation > 0.15)
            )
            patterns.append(('📉 EXHAUSTION', mask))
        
        # 33. PYRAMID ACCUMULATION
        if all(col in df.columns for col in ['vol_ratio_7d_90d', 'vol_ratio_30d_90d', 'vol_ratio_90d_180d', 'ret_30d', 'from_low_pct']):
            mask = (
                (df['vol_ratio_7d_90d'] > 1.1) &
                (df['vol_ratio_30d_90d'] > 1.05) &
                (df['vol_ratio_90d_180d'] > 1.02) &
                (df['ret_30d'].between(5, 15)) &
                (df['from_low_pct'] < 50)
            )
            patterns.append(('🔺 PYRAMID', mask))
        
        # 34. MOMENTUM VACUUM
        if all(col in df.columns for col in ['ret_30d', 'ret_7d', 'ret_1d', 'rvol', 'from_low_pct']):
            mask = (
                (df['ret_30d'] < -20) &
                (df['ret_7d'] > 0) &
                (df['ret_1d'] > 2) &
                (df['rvol'] > 3) &
                (df['from_low_pct'] < 10)
            )
            patterns.append(('🌪️ VACUUM', mask))

        # 35. EARNINGS SURPRISE LEADER - Multi-timeframe earnings acceleration with pace calculation
        try:
            if all(col in df.columns for col in ['eps_change_pct', 'ret_1d', 'ret_7d', 'ret_30d', 'pe', 'vol_ratio_30d_90d', 'price', 'sma_20d']):
                eps_change_pct = get_col_safe('eps_change_pct', 0)
                pe = get_col_safe('pe', 100)
                price = get_col_safe('price', 0)
                sma_20d = get_col_safe('sma_20d', 0)
                
                # Calculate sector median PE safely
                if 'sector' in df.columns:
                    sector_pe_median = df.groupby('sector')['pe'].transform('median').fillna(20)
                else:
                    sector_pe_median = pd.Series(20, index=df.index)
                
                # Safe pace calculation with sophisticated velocity analysis
                with np.errstate(divide='ignore', invalid='ignore'):
                    ret_7d = get_col_safe('ret_7d', 0)
                    ret_30d = get_col_safe('ret_30d', 0)
                    weekly_vs_monthly_pace = np.where(ret_30d != 0, ret_7d / (ret_30d / 4), 0)
                
                mask = (
                    # Fundamental acceleration - EPS growth exceeding price growth
                    eps_change_pct.notna() & (eps_change_pct > 50) &
                    (eps_change_pct > ret_30d) &                                 # EPS > price growth
                    
                    # Valuation opportunity - below sector median
                    pe.notna() & (pe < sector_pe_median) &
                    
                    # Multi-timeframe momentum building
                    (get_col_safe('ret_1d', 0) > 0) &
                    (weekly_vs_monthly_pace > 1) &                               # Accelerating pace
                    
                    # Volume confirmation
                    (get_col_safe('vol_ratio_30d_90d', 1) > 1.2) &
                    
                    # Technical position above trend
                    price.notna() & sma_20d.notna() & (price > sma_20d)
                )
                patterns.append(('🎆 EARNINGS SURPRISE LEADER', mask))
        except Exception as e:
            logger.warning(f"Error in EARNINGS SURPRISE LEADER pattern: {e}")
            patterns.append(('🎆 EARNINGS SURPRISE LEADER', pd.Series(False, index=df.index)))

        # 36. INFORMATION DECAY ARBITRAGE - Advanced information theory application
        try:
            if all(col in df.columns for col in ['ret_1d', 'ret_3d', 'ret_7d', 'ret_30d', 'vol_ratio_7d_90d', 'vol_ratio_30d_90d']):
                # Information decay modeling with empirically derived half-lives
                SHORT_HALFLIFE = 3.5    # days - market microstructure response
                MEDIUM_HALFLIFE = 14    # days - institutional response time
                LONG_HALFLIFE = 45      # days - fundamental analysis integration
                
                ret_1d = get_col_safe('ret_1d', 0)
                ret_3d = get_col_safe('ret_3d', 0)
                ret_7d = get_col_safe('ret_7d', 0)
                ret_30d = get_col_safe('ret_30d', 0)
                
                # Calculate theoretical decay curves using exponential decay model
                with np.errstate(divide='ignore', invalid='ignore'):
                    theoretical_3d = ret_1d * np.exp(-3/SHORT_HALFLIFE)
                    theoretical_7d = ret_1d * np.exp(-7/SHORT_HALFLIFE)
                    theoretical_30d = ret_1d * np.exp(-30/MEDIUM_HALFLIFE)
                    
                    # Calculate normalized decay rate anomalies
                    decay_anomaly_3d = np.where(theoretical_3d != 0, (ret_3d - theoretical_3d) / abs(theoretical_3d), 0)
                    decay_anomaly_7d = np.where(theoretical_7d != 0, (ret_7d - theoretical_7d) / abs(theoretical_7d), 0)
                    decay_anomaly_30d = np.where(theoretical_30d != 0, (ret_30d - theoretical_30d) / abs(theoretical_30d), 0)
                
                # Multi-dimensional decay score with adaptive weighting
                decay_score = (
                    (decay_anomaly_3d > 0.15).astype(int) * 25 +      # Short-term underpricing
                    (decay_anomaly_7d > 0.10).astype(int) * 35 +      # Medium-term persistence
                    (decay_anomaly_30d > 0.05).astype(int) * 40       # Long-term sustainability
                )
                
                # Volume stealth confirmation - building interest but not explosive
                stealth_volume = (
                    (get_col_safe('vol_ratio_7d_90d', 1) > 1.1) &     # Building interest
                    (get_col_safe('vol_ratio_7d_90d', 1) < 2.0) &     # But not explosive yet
                    (get_col_safe('vol_ratio_30d_90d', 1) > 1.05)     # Consistent trend
                )
                
                # Position and quality filters for implementation
                arbitrage_setup = (
                    (get_col_safe('from_high_pct', 0) < -8) &         # Room to move up
                    (get_col_safe('from_low_pct', 0) > 25) &          # Not at lows
                    (get_col_safe('price', 0) > get_col_safe('sma_50d', 0) * 0.95) &  # Technical support
                    (get_col_safe('pe', 100) < 40) &                 # Not extremely overvalued
                    (get_col_safe('eps_change_pct', 0) > -5)          # Earnings not collapsing
                )
                
                # Final information decay arbitrage detection
                mask = (
                    (decay_score >= 60) &
                    stealth_volume &
                    arbitrage_setup
                )
                patterns.append(('🕰️ INFORMATION DECAY ARBITRAGE', mask))
        except Exception as e:
            logger.warning(f"Error in INFORMATION DECAY ARBITRAGE pattern: {e}")
            patterns.append(('🕰️ INFORMATION DECAY ARBITRAGE', pd.Series(False, index=df.index)))

        # 37. PHOENIX RISING - Epic comeback with dramatic transformation
        try:
            if all(col in df.columns for col in ['from_low_pct', 'eps_change_pct', 'rvol', 'vol_ratio_90d_180d', 'pe']):
                from_low_pct = get_col_safe('from_low_pct', 0)
                eps_change_pct = get_col_safe('eps_change_pct', 0)
                pe = get_col_safe('pe', 100)
                
                mask = (
                    # Strong recovery from significant lows - realistic comeback
                    (from_low_pct > 40) &
                    
                    # Substantial fundamental turnaround - achievable transformation
                    eps_change_pct.notna() & (eps_change_pct > 75) &
                    
                    # Notable volume interest - institutional recognition
                    (get_col_safe('rvol', 0) > 2.5) &
                    (get_col_safe('vol_ratio_90d_180d', 1) > 1.4) &
                    
                    # Quality confirmation - reasonable valuation after turnaround
                    pe.notna() & (pe > 0) & (pe < 60)
                )
                patterns.append(('🐦 PHOENIX RISING', ensure_series(mask)))
        except Exception as e:
            logger.warning(f"Error in PHOENIX RISING pattern: {e}")
            patterns.append(('🐦 PHOENIX RISING', pd.Series(False, index=df.index)))

        # 38. ATOMIC DECAY MOMENTUM - Physics-based momentum timing using radioactive decay mathematics
        try:
            if all(col in df.columns for col in ['ret_1d', 'ret_7d', 'ret_30d', 'rvol', 'from_low_pct', 'acceleration_score']):
                ret_1d = get_col_safe('ret_1d', 0)
                ret_7d = get_col_safe('ret_7d', 0)
                ret_30d = get_col_safe('ret_30d', 0)
                
                # Calculate momentum half-life using atomic decay physics (t½ = ln(2)/λ)
                with np.errstate(divide='ignore', invalid='ignore'):
                    # Momentum decay rate calculation - ratio of short to long momentum
                    momentum_ratio = np.where(ret_30d != 0, abs(ret_7d / ret_30d), 0.1)
                    momentum_decay_rate = np.log(2) / np.maximum(momentum_ratio, 0.1)  # Half-life calculation
                
                # Atomic momentum strength criteria (undecayed energy state)
                atomic_strength = (
                    (momentum_decay_rate > 2) &                      # Slow decay rate (sustained momentum)
                    (ret_7d > 3) &                                   # Strong recent momentum
                    (ret_30d > 8) &                                  # Sustained longer momentum
                    (get_col_safe('rvol', 1) > 1.3) &               # Volume confirmation
                    (get_col_safe('from_low_pct', 0) > 15) &        # Off lows
                    (get_col_safe('acceleration_score', 0) >= 75)    # Accelerating
                )
                
                patterns.append(('⚛️ ATOMIC DECAY MOMENTUM', ensure_series(atomic_strength)))
        except Exception as e:
            logger.warning(f"Error in ATOMIC DECAY MOMENTUM pattern: {e}")
            patterns.append(('⚛️ ATOMIC DECAY MOMENTUM', pd.Series(False, index=df.index)))

        # 39. GARP LEADER - Growth At Reasonable Price methodology
        try:
            if all(col in df.columns for col in ['eps_change_pct', 'pe', 'ret_6m', 'from_low_pct']):
                eps_change_pct = get_col_safe('eps_change_pct')
                pe = get_col_safe('pe')
                
                mask = (
                    eps_change_pct.notna() & pe.notna() &
                    (eps_change_pct > 20) &                                        # Strong growth
                    (pe.between(8, 25)) &                                          # Reasonable valuation
                    (get_col_safe('ret_6m', 0) > 10) &                            # Market recognition
                    (get_col_safe('from_low_pct', 0) > 40)                        # Not oversold
                )
                patterns.append(('💹 GARP LEADER', ensure_series(mask)))
        except Exception as e:
            logger.warning(f"Error in GARP LEADER pattern: {e}")
            patterns.append(('💹 GARP LEADER', pd.Series(False, index=df.index)))

        # 40. PULLBACK SUPPORT - Realistic support bounce detection
        try:
            if all(col in df.columns for col in ['price', 'sma_20d', 'sma_200d', 'ret_1d', 'rvol']):
                price = get_col_safe('price', 0)
                sma_200d = get_col_safe('sma_200d', 0)
                sma_20d = get_col_safe('sma_20d', 0)
                
                # Calculate realistic support zone around 20-day SMA (±3% for practical trading)
                with np.errstate(divide='ignore', invalid='ignore'):
                    support_zone_low = sma_20d * 0.97   # Widened from 0.98 to 0.97
                    support_zone_high = sma_20d * 1.03  # Widened from 1.02 to 1.03
                
                mask = (
                    price.notna() & sma_200d.notna() & sma_20d.notna() &
                    (price > sma_200d) &                                          # Above long-term trend
                    (price >= support_zone_low) & (price <= support_zone_high) &  # Near 20-day SMA (±3%)
                    (get_col_safe('ret_1d', 0) > 0) &                            # Bouncing
                    (get_col_safe('rvol', 0) > 1.2)                              # Lowered volume threshold for practicality
                )
                patterns.append(('🛡️ PULLBACK SUPPORT', ensure_series(mask)))
        except Exception as e:
            logger.warning(f"Error in PULLBACK SUPPORT pattern: {e}")
            patterns.append(('🛡️ PULLBACK SUPPORT', pd.Series(False, index=df.index)))

        # 41. OVERSOLD QUALITY - Value opportunity identification
        try:
            if all(col in df.columns for col in ['from_low_pct', 'eps_change_pct', 'pe', 'ret_1d', 'rvol']):
                eps_change_pct = get_col_safe('eps_change_pct')
                pe = get_col_safe('pe')
                
                mask = (
                    (get_col_safe('from_low_pct', 0) < 25) &                      # Oversold
                    eps_change_pct.notna() & (eps_change_pct > 0) &               # Still growing
                    pe.notna() & (pe < 20) &                                      # Reasonable valuation
                    (get_col_safe('ret_1d', 0) > 1) &                            # Starting to bounce
                    (get_col_safe('rvol', 0) > 1.5)                              # Interest building
                )
                patterns.append(('💳 OVERSOLD QUALITY', ensure_series(mask)))
        except Exception as e:
            logger.warning(f"Error in OVERSOLD QUALITY pattern: {e}")
            patterns.append(('💳 OVERSOLD QUALITY', pd.Series(False, index=df.index)))
        
        # Ensure all patterns have Series masks
        patterns = [(name, ensure_series(mask)) for name, mask in patterns]
        
        return patterns
    
    @staticmethod
    def get_pattern_summary(df: pd.DataFrame) -> pd.DataFrame:
        """
        Generate a summary of pattern detections
        """
        if 'patterns' not in df.columns:
            return pd.DataFrame()
        
        pattern_counts = {}
        pattern_stocks = {}
        
        for idx, patterns_str in df['patterns'].items():
            if patterns_str:
                for pattern in patterns_str.split(' | '):
                    pattern = pattern.strip()
                    if pattern:
                        pattern_counts[pattern] = pattern_counts.get(pattern, 0) + 1
                        if pattern not in pattern_stocks:
                            pattern_stocks[pattern] = []
                        pattern_stocks[pattern].append(df.loc[idx, 'ticker'])
        
        if not pattern_counts:
            return pd.DataFrame()
        
        # Create summary dataframe
        summary_data = []
        for pattern, count in pattern_counts.items():
            meta = PatternDetector.PATTERN_METADATA.get(pattern, {})
            top_stocks = pattern_stocks[pattern][:3]
            
            summary_data.append({
                'Pattern': pattern,
                'Count': count,
                'Weight': meta.get('importance_weight', 0),
                'Category': meta.get('category', 'unknown'),
                'Top Stocks': ', '.join(top_stocks)
            })
        
        summary_df = pd.DataFrame(summary_data)
        summary_df = summary_df.sort_values('Count', ascending=False)
        
        return summary_df 
        
# ============================================
# LEADERSHIP DENSITY ENGINE - ALL-TIME BEST APPROACH
# ============================================

class LeadershipDensityEngine:
    """
    Revolutionary Leadership Density Index (LDI) approach for sector/industry/category analysis.
    
    This approach measures sector strength through leadership density rather than sampling bias.
    LDI = (Number of Market Leaders in Group) / (Total Stocks in Group) × 100
    
    Key Benefits:
    - No sampling bias - uses entire universe
    - Fair comparison across all group sizes  
    - Market-relative leadership assessment
    - True sector strength measurement
    """
    
    @staticmethod
    @st.cache_data(ttl=300, show_spinner=False)
    def _calculate_global_threshold_cached(df_json: str, percentile: float = 90.0) -> float:
        """Cached calculation of global market leadership threshold"""
        df = pd.read_json(df_json)
        
        if df.empty or 'master_score' not in df.columns:
            return 75.0  # Default threshold
        
        # Calculate global threshold (top 10% of entire market)
        threshold = df['master_score'].quantile(percentile / 100.0)
        return max(threshold, 60.0)  # Minimum threshold of 60
    
    @staticmethod
    def calculate_global_threshold(df: pd.DataFrame, percentile: float = 90.0) -> float:
        """Calculate global market leadership threshold (top 10% default)"""
        if df.empty or 'master_score' not in df.columns:
            return 75.0
        
        try:
            # Use only relevant columns for caching
            cache_df = df[['master_score']].copy()
            df_json = cache_df.to_json()
            return LeadershipDensityEngine._calculate_global_threshold_cached(df_json, percentile)
        except Exception as e:
            logger.warning(f"Cache failed for global threshold: {str(e)}")
            # Fallback calculation
            threshold = df['master_score'].quantile(percentile / 100.0)
            return max(threshold, 60.0)
    
    @staticmethod
    @st.cache_data(ttl=300, show_spinner=False)
    def _calculate_sector_ldi_cached(df_json: str, threshold: float) -> pd.DataFrame:
        """Cached calculation of sector Leadership Density Index"""
        df = pd.read_json(df_json)
        
        if df.empty or 'sector' not in df.columns or 'master_score' not in df.columns:
            return pd.DataFrame()
        
        # Calculate LDI for each sector
        sector_stats = []
        
        for sector in df['sector'].unique():
            if sector == 'Unknown':
                continue
                
            sector_df = df[df['sector'] == sector]
            total_stocks = len(sector_df)
            
            if total_stocks == 0:
                continue
            
            # Count market leaders in this sector
            leaders = sector_df[sector_df['master_score'] >= threshold]
            leader_count = len(leaders)
            
            # Calculate Leadership Density Index
            ldi = (leader_count / total_stocks) * 100
            
            # Additional metrics for enhanced analysis
            avg_score = sector_df['master_score'].mean()
            median_score = sector_df['master_score'].median()
            top_10_pct_count = max(1, int(total_stocks * 0.1))
            top_performers = sector_df.nlargest(top_10_pct_count, 'master_score')
            elite_avg_score = top_performers['master_score'].mean()
            
            # Calculate momentum and volume metrics if available
            avg_momentum = sector_df['momentum_score'].mean() if 'momentum_score' in sector_df.columns else 50.0
            avg_volume = sector_df['volume_score'].mean() if 'volume_score' in sector_df.columns else 50.0
            avg_rvol = sector_df['rvol'].mean() if 'rvol' in sector_df.columns else 1.0
            avg_ret_30d = sector_df['ret_30d'].mean() if 'ret_30d' in sector_df.columns else 0.0
            
            # Money flow if available
            total_money_flow = sector_df['money_flow_mm'].sum() if 'money_flow_mm' in sector_df.columns else 0.0
            
            sector_stats.append({
                'sector': sector,
                'ldi_score': round(ldi, 2),
                'leader_count': leader_count,
                'total_stocks': total_stocks,
                'avg_score': round(avg_score, 2),
                'median_score': round(median_score, 2),
                'elite_avg_score': round(elite_avg_score, 2),
                'avg_momentum': round(avg_momentum, 2),
                'avg_volume': round(avg_volume, 2),
                'avg_rvol': round(avg_rvol, 2),
                'avg_ret_30d': round(avg_ret_30d, 2),
                'total_money_flow': round(total_money_flow, 2),
                'leadership_density': f"{ldi:.1f}%"
            })
        
        if not sector_stats:
            return pd.DataFrame()
        
        # Create DataFrame and sort by LDI score
        ldi_df = pd.DataFrame(sector_stats)
        ldi_df = ldi_df.sort_values('ldi_score', ascending=False)
        ldi_df.set_index('sector', inplace=True)
        
        return ldi_df
    
    @staticmethod
    def calculate_sector_ldi(df: pd.DataFrame, percentile: float = 90.0) -> pd.DataFrame:
        """Calculate Leadership Density Index for all sectors"""
        if df.empty or 'sector' not in df.columns:
            return pd.DataFrame()
        
        try:
            # Calculate global threshold
            threshold = LeadershipDensityEngine.calculate_global_threshold(df, percentile)
            
            # Use only relevant columns for caching
            cache_cols = ['sector', 'master_score']
            optional_cols = ['momentum_score', 'volume_score', 'rvol', 'ret_30d', 'money_flow_mm']
            cache_cols.extend([col for col in optional_cols if col in df.columns])
            
            cache_df = df[cache_cols].copy()
            df_json = cache_df.to_json()
            
            return LeadershipDensityEngine._calculate_sector_ldi_cached(df_json, threshold)
        except Exception as e:
            logger.error(f"Error calculating sector LDI: {str(e)}")
            return pd.DataFrame()
    
    @staticmethod
    @st.cache_data(ttl=300, show_spinner=False)
    def _calculate_industry_ldi_cached(df_json: str, threshold: float) -> pd.DataFrame:
        """Cached calculation of industry Leadership Density Index"""
        df = pd.read_json(df_json)
        
        if df.empty or 'industry' not in df.columns or 'master_score' not in df.columns:
            return pd.DataFrame()
        
        # Calculate LDI for each industry
        industry_stats = []
        
        for industry in df['industry'].unique():
            if industry == 'Unknown':
                continue
                
            industry_df = df[df['industry'] == industry]
            total_stocks = len(industry_df)
            
            if total_stocks == 0:
                continue
            
            # Count market leaders in this industry
            leaders = industry_df[industry_df['master_score'] >= threshold]
            leader_count = len(leaders)
            
            # Calculate Leadership Density Index
            ldi = (leader_count / total_stocks) * 100
            
            # Additional metrics
            avg_score = industry_df['master_score'].mean()
            median_score = industry_df['master_score'].median()
            
            # Quality assessment
            quality_flag = ''
            if total_stocks < 5:
                quality_flag = '⚠️ Small Sample'
            elif ldi == 0 and total_stocks > 10:
                quality_flag = '📉 No Leaders'
            elif ldi > 20:
                quality_flag = '🔥 High Density'
            
            # Calculate momentum and volume metrics if available
            avg_momentum = industry_df['momentum_score'].mean() if 'momentum_score' in industry_df.columns else 50.0
            avg_volume = industry_df['volume_score'].mean() if 'volume_score' in industry_df.columns else 50.0
            
            industry_stats.append({
                'industry': industry,
                'ldi_score': round(ldi, 2),
                'leader_count': leader_count,
                'total_stocks': total_stocks,
                'avg_score': round(avg_score, 2),
                'median_score': round(median_score, 2),
                'avg_momentum': round(avg_momentum, 2),
                'avg_volume': round(avg_volume, 2),
                'quality_flag': quality_flag,
                'leadership_density': f"{ldi:.1f}%"
            })
        
        if not industry_stats:
            return pd.DataFrame()
        
        # Create DataFrame and sort by LDI score
        ldi_df = pd.DataFrame(industry_stats)
        ldi_df = ldi_df.sort_values('ldi_score', ascending=False)
        ldi_df.set_index('industry', inplace=True)
        
        return ldi_df
    
    @staticmethod
    def calculate_industry_ldi(df: pd.DataFrame, percentile: float = 90.0) -> pd.DataFrame:
        """Calculate Leadership Density Index for all industries"""
        if df.empty or 'industry' not in df.columns:
            return pd.DataFrame()
        
        try:
            # Calculate global threshold
            threshold = LeadershipDensityEngine.calculate_global_threshold(df, percentile)
            
            # Use only relevant columns for caching
            cache_cols = ['industry', 'master_score']
            optional_cols = ['momentum_score', 'volume_score', 'rvol', 'ret_30d', 'money_flow_mm']
            cache_cols.extend([col for col in optional_cols if col in df.columns])
            
            cache_df = df[cache_cols].copy()
            df_json = cache_df.to_json()
            
            return LeadershipDensityEngine._calculate_industry_ldi_cached(df_json, threshold)
        except Exception as e:
            logger.error(f"Error calculating industry LDI: {str(e)}")
            return pd.DataFrame()
    
    @staticmethod
    @st.cache_data(ttl=300, show_spinner=False)
    def _calculate_category_ldi_cached(df_json: str, threshold: float) -> pd.DataFrame:
        """Cached calculation of category Leadership Density Index"""
        df = pd.read_json(df_json)
        
        if df.empty or 'category' not in df.columns or 'master_score' not in df.columns:
            return pd.DataFrame()
        
        # Calculate LDI for each category
        category_stats = []
        
        for category in df['category'].unique():
            if category == 'Unknown':
                continue
                
            category_df = df[df['category'] == category]
            total_stocks = len(category_df)
            
            if total_stocks == 0:
                continue
            
            # Count market leaders in this category
            leaders = category_df[category_df['master_score'] >= threshold]
            leader_count = len(leaders)
            
            # Calculate Leadership Density Index
            ldi = (leader_count / total_stocks) * 100
            
            # Additional metrics for enhanced analysis
            avg_score = category_df['master_score'].mean()
            median_score = category_df['master_score'].median()
            top_10_pct_count = max(1, int(total_stocks * 0.1))
            top_performers = category_df.nlargest(top_10_pct_count, 'master_score')
            elite_avg_score = top_performers['master_score'].mean()
            
            # Calculate momentum and volume metrics if available
            avg_momentum = category_df['momentum_score'].mean() if 'momentum_score' in category_df.columns else 50.0
            avg_volume = category_df['volume_score'].mean() if 'volume_score' in category_df.columns else 50.0
            avg_rvol = category_df['rvol'].mean() if 'rvol' in category_df.columns else 1.0
            avg_ret_30d = category_df['ret_30d'].mean() if 'ret_30d' in category_df.columns else 0.0
            
            # Money flow if available
            total_money_flow = category_df['money_flow_mm'].sum() if 'money_flow_mm' in category_df.columns else 0.0
            
            category_stats.append({
                'category': category,
                'ldi_score': round(ldi, 2),
                'leader_count': leader_count,
                'total_stocks': total_stocks,
                'avg_score': round(avg_score, 2),
                'median_score': round(median_score, 2),
                'elite_avg_score': round(elite_avg_score, 2),
                'avg_momentum': round(avg_momentum, 2),
                'avg_volume': round(avg_volume, 2),
                'avg_rvol': round(avg_rvol, 2),
                'avg_ret_30d': round(avg_ret_30d, 2),
                'total_money_flow': round(total_money_flow, 2),
                'leadership_density': f"{ldi:.1f}%"
            })
        
        if not category_stats:
            return pd.DataFrame()
        
        # Create DataFrame and sort by LDI score
        ldi_df = pd.DataFrame(category_stats)
        ldi_df = ldi_df.sort_values('ldi_score', ascending=False)
        ldi_df.set_index('category', inplace=True)
        
        return ldi_df
    
    @staticmethod
    def calculate_category_ldi(df: pd.DataFrame, percentile: float = 90.0) -> pd.DataFrame:
        """Calculate Leadership Density Index for all categories"""
        if df.empty or 'category' not in df.columns:
            return pd.DataFrame()
        
        try:
            # Calculate global threshold
            threshold = LeadershipDensityEngine.calculate_global_threshold(df, percentile)
            
            # Use only relevant columns for caching
            cache_cols = ['category', 'master_score']
            optional_cols = ['momentum_score', 'volume_score', 'rvol', 'ret_30d', 'money_flow_mm']
            cache_cols.extend([col for col in optional_cols if col in df.columns])
            
            cache_df = df[cache_cols].copy()
            df_json = cache_df.to_json()
            
            return LeadershipDensityEngine._calculate_category_ldi_cached(df_json, threshold)
        except Exception as e:
            logger.error(f"Error calculating category LDI: {str(e)}")
            return pd.DataFrame()

# ============================================
# MARKET INTELLIGENCE
# ============================================

class MarketIntelligence:
    """Professional market regime detection and sector analysis"""
    
    @staticmethod
    def detect_market_regime(df: pd.DataFrame) -> Tuple[str, Dict[str, Any]]:
        """Detect current market regime with supporting data"""
        
        if df.empty:
            return "NO DATA", {}
        
        metrics = {}
        
        # Calculate category-based metrics
        if 'category' in df.columns and 'master_score' in df.columns:
            category_scores = df.groupby('category')['master_score'].mean()
            
            micro_small_avg = category_scores[category_scores.index.isin(['Micro Cap', 'Small Cap'])].mean() if any(category_scores.index.isin(['Micro Cap', 'Small Cap'])) else 50
            large_mega_avg = category_scores[category_scores.index.isin(['Large Cap', 'Mega Cap'])].mean() if any(category_scores.index.isin(['Large Cap', 'Mega Cap'])) else 50
            
            metrics['micro_small_avg'] = micro_small_avg if pd.notna(micro_small_avg) else 50
            metrics['large_mega_avg'] = large_mega_avg if pd.notna(large_mega_avg) else 50
            metrics['category_spread'] = metrics['micro_small_avg'] - metrics['large_mega_avg']
        else:
            metrics['micro_small_avg'] = 50
            metrics['large_mega_avg'] = 50
            metrics['category_spread'] = 0
        
        # Calculate market breadth using SAME timeframe as A/D (1-day)
        if 'ret_1d' in df.columns:
            breadth = len(df[df['ret_1d'] > 0]) / len(df) if len(df) > 0 else 0.5
            metrics['breadth'] = breadth
        else:
            breadth = 0.5
            metrics['breadth'] = breadth
        
        # Calculate volatility metrics
        if 'rvol' in df.columns:
            avg_rvol = df['rvol'].median()
            metrics['avg_rvol'] = avg_rvol if pd.notna(avg_rvol) else 1.0
        else:
            metrics['avg_rvol'] = 1.0
        
        # UNIFIED MARKET REGIME DETECTION - Using ALL metrics for consistency
        category_diff = metrics['micro_small_avg'] - metrics['large_mega_avg']
        
        # Get additional metrics for comprehensive regime detection
        momentum_pct = 0
        volume_level = metrics['avg_rvol']
        
        # Calculate momentum percentage if available (BULLETPROOF)
        if 'momentum_score' in df.columns and len(df) > 0:
            valid_momentum = df['momentum_score'].dropna()
            if len(valid_momentum) > 0:
                momentum_pct = (len(valid_momentum[valid_momentum >= 70]) / len(valid_momentum) * 100)
            else:
                momentum_pct = 0
        else:
            momentum_pct = 0
        
        # COMPREHENSIVE REGIME LOGIC - All factors must align
        
        # RISK-ON BULL: Needs strong breadth + decent momentum OR strong category rotation
        if (breadth > 0.58 and momentum_pct > 6) or (category_diff > 8 and breadth > 0.55):
            regime = "🔥 RISK-ON BULL"
        
        # RISK-OFF DEFENSIVE: Poor breadth + weak fundamentals
        elif breadth < 0.42 or (category_diff < -8 and breadth < 0.48):
            regime = "🛡️ RISK-OFF DEFENSIVE"
        
        # VOLATILE OPPORTUNITY: High volume + mixed breadth
        elif volume_level > 1.3 and breadth > 0.48 and breadth < 0.58:
            regime = "⚡ VOLATILE OPPORTUNITY"
        
        # RANGE-BOUND: Specific criteria for sideways/choppy markets
        elif (0.45 <= breadth <= 0.55 and 
              momentum_pct < 8 and 
              volume_level < 1.4 and 
              abs(category_diff) < 6):
            regime = "😴 RANGE-BOUND"
        
        # MIXED SIGNALS: When criteria don't clearly align (new fallback)
        else:
            regime = "🔄 MIXED SIGNALS"
        
        metrics['regime'] = regime
        
        return regime, metrics
    
    @staticmethod
    def calculate_advance_decline_ratio(df: pd.DataFrame) -> Dict[str, Any]:
        """Calculate advance/decline ratio and related metrics"""
        
        if 'ret_1d' not in df.columns or df.empty:
            return {'advancing': 0, 'declining': 0, 'unchanged': 0, 'ad_ratio': 1.0, 'ad_line': 0, 'breadth_pct': 0}
        
        advancing = len(df[df['ret_1d'] > 0])
        declining = len(df[df['ret_1d'] < 0])
        unchanged = len(df[df['ret_1d'] == 0])
        
        ad_metrics = {
            'advancing': advancing,
            'declining': declining,
            'unchanged': unchanged,
            'ad_ratio': safe_divide(advancing, declining, default=1.0) if declining > 0 else (float('inf') if advancing > 0 else 1.0),
            'ad_line': advancing - declining,
            'breadth_pct': safe_percentage(advancing, len(df), default=0.0)
        }
        
        return ad_metrics
    
    @staticmethod
    def detect_sector_rotation(df: pd.DataFrame) -> pd.DataFrame:
        """Enhanced sector rotation detection using Leadership Density Index"""
        
        if df.empty or 'sector' not in df.columns:
            return pd.DataFrame()
        
        try:
            # Primary approach: Use LDI for accurate analysis
            ldi_df = LeadershipDensityEngine.calculate_sector_ldi(df)
            
            if ldi_df.empty:
                return pd.DataFrame()
            
            # Calculate enhanced flow score
            ldi_df['flow_score'] = (
                ldi_df['ldi_score'] * 0.4 +
                ldi_df['avg_score'] * 0.3 +
                ldi_df['avg_momentum'] * 0.15 +
                ldi_df['avg_volume'] * 0.15
            )
            
            ldi_df['rank'] = ldi_df['flow_score'].rank(ascending=False)
            
            # Ensure compatibility with existing UI
            display_df = ldi_df.rename(columns={
                'leader_count': 'analyzed_stocks'
            }).copy()
            
            display_df['sampling_pct'] = 100.0
            
            # Professional quality indicators
            display_df['ldi_quality'] = display_df['ldi_score'].apply(
                lambda x: 'Elite' if x >= 20 else 
                         'Strong' if x >= 10 else 
                         'Moderate' if x >= 5 else 
                         'Weak'
            )
            
            return display_df.sort_values('flow_score', ascending=False)
            
        except Exception as e:
            logger.error(f"Error in LDI sector rotation: {str(e)}")
            # Fallback to traditional method
            return MarketIntelligence._calculate_traditional_sectors(df)
    
    @staticmethod
    def detect_industry_rotation(df: pd.DataFrame) -> pd.DataFrame:
        """Enhanced industry rotation detection using Leadership Density Index"""
        
        if df.empty or 'industry' not in df.columns:
            return pd.DataFrame()
        
        try:
            # Primary approach: Use LDI for accurate analysis
            ldi_df = LeadershipDensityEngine.calculate_industry_ldi(df)
            
            if ldi_df.empty:
                return pd.DataFrame()
            
            # Calculate enhanced flow score
            ldi_df['flow_score'] = (
                ldi_df['ldi_score'] * 0.4 +
                ldi_df['avg_score'] * 0.3 +
                ldi_df['avg_momentum'] * 0.15 +
                ldi_df['avg_volume'] * 0.15
            )
            
            ldi_df['rank'] = ldi_df['flow_score'].rank(ascending=False)
            
            # Ensure compatibility with existing UI
            display_df = ldi_df.rename(columns={
                'leader_count': 'analyzed_stocks'
            }).copy()
            
            display_df['sampling_pct'] = 100.0
            
            # Professional quality indicators
            display_df['ldi_quality'] = display_df['ldi_score'].apply(
                lambda x: 'Elite' if x >= 20 else 
                         'Strong' if x >= 10 else 
                         'Moderate' if x >= 5 else 
                         'Weak'
            )
            
            return display_df.sort_values('flow_score', ascending=False)
            
        except Exception as e:
            logger.error(f"Error in LDI industry rotation: {str(e)}")
            # Fallback to traditional method
            return MarketIntelligence._calculate_traditional_industries(df)
    
    @staticmethod
    def detect_category_rotation(df: pd.DataFrame) -> pd.DataFrame:
        """Enhanced category rotation detection using Leadership Density Index"""
        
        if df.empty or 'category' not in df.columns:
            return pd.DataFrame()
        
        try:
            # Primary approach: Use LDI for accurate analysis
            ldi_df = LeadershipDensityEngine.calculate_category_ldi(df)
            
            if ldi_df.empty:
                return pd.DataFrame()
            
            # Calculate enhanced flow score
            ldi_df['flow_score'] = (
                ldi_df['ldi_score'] * 0.4 +
                ldi_df['avg_score'] * 0.3 +
                ldi_df['avg_momentum'] * 0.15 +
                ldi_df['avg_volume'] * 0.15
            )
            
            ldi_df['rank'] = ldi_df['flow_score'].rank(ascending=False)
            
            # Ensure compatibility with existing UI
            display_df = ldi_df.rename(columns={
                'leader_count': 'analyzed_stocks'
            }).copy()
            
            display_df['sampling_pct'] = 100.0
            
            # Professional quality indicators
            display_df['ldi_quality'] = display_df['ldi_score'].apply(
                lambda x: 'Elite' if x >= 20 else 
                         'Strong' if x >= 10 else 
                         'Moderate' if x >= 5 else 
                         'Weak'
            )
            
            return display_df.sort_values('flow_score', ascending=False)
            
        except Exception as e:
            logger.error(f"Error in LDI category rotation: {str(e)}")
            # Fallback to traditional method
            return MarketIntelligence._calculate_traditional_categories(df)
    
    @staticmethod
    @st.cache_data(ttl=300, show_spinner=False)
    def _calculate_traditional_sectors(df: pd.DataFrame) -> pd.DataFrame:
        """Fallback traditional sector analysis with optimized sampling"""
        
        if 'sector' not in df.columns or df.empty:
            return pd.DataFrame()
        
        sector_data = []
        
        for sector in df['sector'].unique():
            if sector == 'Unknown':
                continue
                
            sector_df = df[df['sector'] == sector].copy()
            total_count = len(sector_df)
            
            if total_count == 0:
                continue
            
            # Simplified sampling logic
            if total_count <= 10:
                sample_count = total_count
            elif total_count <= 50:
                sample_count = max(8, int(total_count * 0.6))
            else:
                sample_count = min(30, int(total_count * 0.3))
            
            # Take top performers
            sampled_df = sector_df.nlargest(sample_count, 'master_score')
            sector_data.append(sampled_df)
        
        if not sector_data:
            return pd.DataFrame()
        
        combined_df = pd.concat(sector_data, ignore_index=True)
        
        # Calculate aggregated metrics
        sector_metrics = combined_df.groupby('sector').agg({
            'master_score': ['mean', 'median', 'count'],
            'momentum_score': 'mean',
            'volume_score': 'mean',
            'rvol': 'mean',
            'ret_30d': 'mean',
            'money_flow_mm': 'sum' if 'money_flow_mm' in combined_df.columns else lambda x: 0
        }).round(2)
        
        # Flatten column names
        sector_metrics.columns = [
            'avg_score', 'median_score', 'analyzed_stocks',
            'avg_momentum', 'avg_volume', 'avg_rvol', 'avg_ret_30d', 'total_money_flow'
        ]
        
        # Add total stock counts and sampling percentage
        total_counts = df.groupby('sector').size().rename('total_stocks')
        sector_metrics = sector_metrics.join(total_counts, how='left')
        sector_metrics['sampling_pct'] = (sector_metrics['analyzed_stocks'] / sector_metrics['total_stocks'] * 100).round(1)
        
        # Calculate flow score
        sector_metrics['flow_score'] = (
            sector_metrics['avg_score'] * 0.3 +
            sector_metrics['median_score'] * 0.2 +
            sector_metrics['avg_momentum'] * 0.25 +
            sector_metrics['avg_volume'] * 0.25
        )
        
        sector_metrics['rank'] = sector_metrics['flow_score'].rank(ascending=False)
        
        return sector_metrics.sort_values('flow_score', ascending=False)
    
    @staticmethod
    @st.cache_data(ttl=300, show_spinner=False)
    def _calculate_traditional_industries(df: pd.DataFrame) -> pd.DataFrame:
        """Fallback traditional industry analysis with optimized sampling"""
        
        if 'industry' not in df.columns or df.empty:
            return pd.DataFrame()
        
        industry_data = []
        
        for industry in df['industry'].unique():
            if industry == 'Unknown':
                continue
                
            industry_df = df[df['industry'] == industry].copy()
            total_count = len(industry_df)
            
            if total_count == 0:
                continue
            
            # Simplified sampling logic for industries
            if total_count <= 5:
                sample_count = total_count
            elif total_count <= 25:
                sample_count = max(5, int(total_count * 0.7))
            elif total_count <= 100:
                sample_count = max(10, int(total_count * 0.4))
            else:
                sample_count = min(40, int(total_count * 0.2))
            
            # Take top performers
            sampled_df = industry_df.nlargest(sample_count, 'master_score')
            industry_data.append(sampled_df)
        
        if not industry_data:
            return pd.DataFrame()
        
        combined_df = pd.concat(industry_data, ignore_index=True)
        
        # Calculate aggregated metrics
        industry_metrics = combined_df.groupby('industry').agg({
            'master_score': ['mean', 'median', 'count'],
            'momentum_score': 'mean',
            'volume_score': 'mean',
            'rvol': 'mean',
            'ret_30d': 'mean',
            'money_flow_mm': 'sum' if 'money_flow_mm' in combined_df.columns else lambda x: 0
        }).round(2)
        
        # Flatten column names
        industry_metrics.columns = [
            'avg_score', 'median_score', 'analyzed_stocks',
            'avg_momentum', 'avg_volume', 'avg_rvol', 'avg_ret_30d', 'total_money_flow'
        ]
        
        # Add total stock counts and sampling percentage
        total_counts = df.groupby('industry').size().rename('total_stocks')
        industry_metrics = industry_metrics.join(total_counts, how='left')
        industry_metrics['sampling_pct'] = (industry_metrics['analyzed_stocks'] / industry_metrics['total_stocks'] * 100).round(1)
        
        # Calculate flow score
        industry_metrics['flow_score'] = (
            industry_metrics['avg_score'] * 0.3 +
            industry_metrics['median_score'] * 0.2 +
            industry_metrics['avg_momentum'] * 0.25 +
            industry_metrics['avg_volume'] * 0.25
        )
        
        industry_metrics['rank'] = industry_metrics['flow_score'].rank(ascending=False)
        
        return industry_metrics.sort_values('flow_score', ascending=False)

    @staticmethod
    @st.cache_data(ttl=300, show_spinner=False)
    def _calculate_traditional_categories(df: pd.DataFrame) -> pd.DataFrame:
        """Fallback traditional category analysis with optimized sampling"""
        
        if 'category' not in df.columns or df.empty:
            return pd.DataFrame()
        
        category_data = []
        
        for category in df['category'].unique():
            if category == 'Unknown':
                continue
                
            category_df = df[df['category'] == category].copy()
            total_count = len(category_df)
            
            if total_count == 0:
                continue
            
            # Simplified sampling logic for categories
            if total_count <= 5:
                sample_count = total_count
            elif total_count <= 25:
                sample_count = max(5, int(total_count * 0.7))
            elif total_count <= 100:
                sample_count = max(10, int(total_count * 0.4))
            else:
                sample_count = min(40, int(total_count * 0.2))
            
            # Take top performers
            sampled_df = category_df.nlargest(sample_count, 'master_score')
            category_data.append(sampled_df)
        
        if not category_data:
            return pd.DataFrame()
        
        combined_df = pd.concat(category_data, ignore_index=True)
        
        # Calculate aggregated metrics
        category_metrics = combined_df.groupby('category').agg({
            'master_score': ['mean', 'median', 'count'],
            'momentum_score': 'mean',
            'volume_score': 'mean',
            'rvol': 'mean',
            'ret_30d': 'mean',
            'money_flow_mm': 'sum' if 'money_flow_mm' in combined_df.columns else lambda x: 0
        }).round(2)
        
        # Flatten column names
        category_metrics.columns = [
            'avg_score', 'median_score', 'analyzed_stocks',
            'avg_momentum', 'avg_volume', 'avg_rvol', 'avg_ret_30d', 'total_money_flow'
        ]
        
        # Add total stock counts and sampling percentage
        total_counts = df.groupby('category').size().rename('total_stocks')
        category_metrics = category_metrics.join(total_counts, how='left')
        category_metrics['sampling_pct'] = (category_metrics['analyzed_stocks'] / category_metrics['total_stocks'] * 100).round(1)
        
        # Calculate flow score
        category_metrics['flow_score'] = (
            category_metrics['avg_score'] * 0.3 +
            category_metrics['median_score'] * 0.2 +
            category_metrics['avg_momentum'] * 0.25 +
            category_metrics['avg_volume'] * 0.25
        )
        
        category_metrics['rank'] = category_metrics['flow_score'].rank(ascending=False)
        
        return category_metrics.sort_values('flow_score', ascending=False)


# ============================================
# VISUALIZATION ENGINE
# ============================================

class Visualizer:
    """Create all visualizations with proper error handling"""
    
    @staticmethod
    def create_score_distribution(df: pd.DataFrame) -> go.Figure:
        """Create score distribution chart"""
        fig = go.Figure()
        
        if df.empty:
            fig.add_annotation(
                text="No data available for visualization",
                xref="paper", yref="paper",
                x=0.5, y=0.5, showarrow=False
            )
            return fig
        
        scores = [
            ('position_score', 'Position', '#3498db'),
            ('volume_score', 'Volume', '#e74c3c'),
            ('momentum_score', 'Momentum', '#2ecc71'),
            ('acceleration_score', 'Acceleration', '#f39c12'),
            ('breakout_score', 'Breakout', '#9b59b6'),
            ('rvol_score', 'RVOL', '#e67e22')
        ]
        
        for score_col, label, color in scores:
            if score_col in df.columns:
                score_data = df[score_col].dropna()
                if len(score_data) > 0:
                    fig.add_trace(go.Box(
                        y=score_data,
                        name=label,
                        marker_color=color,
                        boxpoints='outliers',
                        hovertemplate=f'{label}<br>Score: %{{y:.1f}}<extra></extra>'
                    ))
        
        fig.update_layout(
            title="Score Component Distribution",
            yaxis_title="Score (0-100)",
            template='plotly_white',
            height=400,
            showlegend=False
        )
        
        return fig

    @staticmethod
    def create_acceleration_profiles(df: pd.DataFrame, n: int = 10) -> go.Figure:
        """Create acceleration profiles showing momentum over time"""
        try:
            accel_df = df.nlargest(min(n, len(df)), 'acceleration_score')
            
            if len(accel_df) == 0:
                return go.Figure()
            
            fig = go.Figure()
            
            for _, stock in accel_df.iterrows():
                x_points = []
                y_points = []
                
                x_points.append('Start')
                y_points.append(0)
                
                if 'ret_30d' in stock.index and pd.notna(stock['ret_30d']):
                    x_points.append('30D')
                    y_points.append(stock['ret_30d'])
                
                if 'ret_7d' in stock.index and pd.notna(stock['ret_7d']):
                    x_points.append('7D')
                    y_points.append(stock['ret_7d'])
                
                if 'ret_1d' in stock.index and pd.notna(stock['ret_1d']):
                    x_points.append('Today')
                    y_points.append(stock['ret_1d'])
                
                if len(x_points) > 1:
                    if stock['acceleration_score'] >= 85:
                        line_style = dict(width=3, dash='solid')
                        marker_style = dict(size=10, symbol='star')
                    elif stock['acceleration_score'] >= 70:
                        line_style = dict(width=2, dash='solid')
                        marker_style = dict(size=8)
                    else:
                        line_style = dict(width=2, dash='dot')
                        marker_style = dict(size=6)
                    
                    fig.add_trace(go.Scatter(
                        x=x_points,
                        y=y_points,
                        mode='lines+markers',
                        name=f"{stock['ticker']} ({stock['acceleration_score']:.0f})",
                        line=line_style,
                        marker=marker_style,
                        hovertemplate=(
                            f"<b>{stock['ticker']}</b><br>" +
                            "%{x}: %{y:.1f}%<br>" +
                            f"Accel Score: {stock['acceleration_score']:.0f}<extra></extra>"
                        )
                    ))
            
            fig.add_hline(y=0, line_dash="dash", line_color="gray", opacity=0.5)
            
            fig.update_layout(
                title=f"Acceleration Profiles - Top {len(accel_df)} Momentum Builders",
                xaxis_title="Time Frame",
                yaxis_title="Return %",
                height=400,
                template='plotly_white',
                showlegend=True,
                legend=dict(
                    orientation="v",
                    yanchor="top",
                    y=1,
                    xanchor="left",
                    x=1.02
                ),
                hovermode='x unified'
            )
            
            return fig
            
        except Exception as e:
            logger.error(f"Error creating acceleration profiles: {str(e)}")
            return go.Figure()

# ============================================
# FILTER ENGINE - OPTIMIZED VERSION
# ============================================

class FilterEngine:
    """
    Centralized filter management with single state object.
    This eliminates 15+ separate session state keys.
    FIXED: Now properly cleans up ALL dynamic widget keys.
    """
    
    @staticmethod
    def initialize_filters():
        """Initialize single filter state object"""
        if 'filter_state' not in st.session_state:
            st.session_state.filter_state = {
                'categories': [],
                'sectors': [],
                'industries': [],
                'min_score': 0,
                'patterns': [],
                'trend_filter': "All Trends",
                'trend_range': (0, 100),
                'trend_custom_range': (0, 100),
                'eps_tiers': [],
                'pe_tiers': [],
                'price_tiers': [],
                'eps_change_tiers': [],
                'position_tiers': [],
                'position_range': (0, 100),
                'performance_tiers': [],
                'performance_custom_range': (-100, 500),
                'ret_1d_range': (2.0, 25.0),
                'ret_3d_range': (3.0, 50.0),
                'ret_7d_range': (5.0, 75.0),
                'ret_30d_range': (10.0, 150.0),
                'ret_3m_range': (15.0, 200.0),
                'ret_6m_range': (20.0, 500.0),
                'ret_1y_range': (25.0, 1000.0),
                'ret_3y_range': (50.0, 2000.0),
                'ret_5y_range': (75.0, 5000.0),
                'min_pe': None,
                'max_pe': None,
                'require_fundamental_data': False,
                'market_states': [],
                'market_strength_range': (0, 100),
                'long_term_strength_range': (0, 100),
                'position_score_range': (0, 100),
                'volume_score_range': (0, 100),
                'momentum_score_range': (0, 100),
                'acceleration_score_range': (0, 100),
                'breakout_score_range': (0, 100),
                'rvol_score_range': (0, 100),
                'position_score_selection': "All Scores",
                'volume_score_selection': "All Scores",
                'momentum_score_selection': "All Scores",
                'acceleration_score_selection': "All Scores",
                'breakout_score_selection': "All Scores",
                'rvol_score_selection': "All Scores",
                'quick_filter': None,
                'quick_filter_applied': False,
                'volume_tiers': [],
                'rvol_range': (0.1, 20.0),
                'vmi_tiers': [],
                'custom_vmi_range': (0.5, 3.0),
                'momentum_harmony_tiers': [],
                # Performance filter selections
                'ret_1d_selection': "All Returns",
                'ret_3d_selection': "All Returns", 
                'ret_7d_selection': "All Returns",
                'ret_30d_selection': "All Returns",
                'ret_3m_selection': "All Returns",
                'ret_6m_selection': "All Returns",
                'ret_1y_selection': "All Returns",
                'ret_3y_selection': "All Returns",
                'ret_5y_selection': "All Returns",
                # Two-Stage Pattern Filtering System
                'exclude_patterns': [],
                'include_patterns': [],
                'combination_patterns': []
            }
        
        # CRITICAL FIX: Clean up any corrupted performance_tiers values
        # This prevents the "default value not in options" error
        valid_performance_options = [
            "🚀 Strong Gainers (>3% 1D)",
            "⚡ Power Moves (>7% 1D)",
            "💥 Explosive (>15% 1D)",
            "🌟 3-Day Surge (>6% 3D)",
            "📈 Weekly Winners (>12% 7D)",
            "🏆 Monthly Champions (>25% 30D)",
            "🎯 Quarterly Stars (>40% 3M)",
            "💎 Half-Year Heroes (>60% 6M)",
            "🌙 Annual Winners (>80% 1Y)",
            "👑 Multi-Year Champions (>150% 3Y)",
            "🏛️ Long-Term Legends (>250% 5Y)",
            "🎯 Custom Range"
        ]
        
        # Clean up performance_tiers to only include valid options
        if 'performance_tiers' in st.session_state.filter_state:
            current_tiers = st.session_state.filter_state['performance_tiers']
            if isinstance(current_tiers, list):
                # Remove any invalid options (like those with extra spaces)
                cleaned_tiers = [tier for tier in current_tiers if tier in valid_performance_options]
                st.session_state.filter_state['performance_tiers'] = cleaned_tiers
    
    @staticmethod
    def get_filter(key: str, default: Any = None) -> Any:
        """Get filter value from centralized state"""
        FilterEngine.initialize_filters()
        return st.session_state.filter_state.get(key, default)
    
    @staticmethod
    def set_filter(key: str, value: Any) -> None:
        """Set filter value in centralized state"""
        FilterEngine.initialize_filters()
        st.session_state.filter_state[key] = value
    
    @staticmethod
    def get_active_count() -> int:
        """Count active filters"""
        FilterEngine.initialize_filters()
        count = 0
        filters = st.session_state.filter_state
        
        # Check each filter type
        if filters.get('categories'): count += 1
        if filters.get('sectors'): count += 1
        if filters.get('industries'): count += 1
        if filters.get('min_score', 0) > 0: count += 1
        if filters.get('patterns'): count += 1
        if filters.get('trend_filter') != "All Trends": count += 1
        if filters.get('eps_tiers'): count += 1
        if filters.get('pe_tiers'): count += 1
        if filters.get('price_tiers'): count += 1
        if filters.get('eps_change_tiers'): count += 1
        if filters.get('min_pe') is not None: count += 1
        if filters.get('max_pe') is not None: count += 1
        if filters.get('require_fundamental_data'): count += 1
        if filters.get('market_states'): count += 1
        if filters.get('market_strength_range') != (0, 100): count += 1
        if filters.get('long_term_strength_range') != (0, 100): count += 1
        if filters.get('performance_tiers'): count += 1
        if filters.get('position_tiers'): count += 1
        if filters.get('volume_tiers'): count += 1
        if filters.get('vmi_tiers'): count += 1
        if filters.get('momentum_harmony_tiers'): count += 1
        if filters.get('custom_vmi_range') and filters.get('custom_vmi_range') != (0.5, 3.0): count += 1
        if filters.get('position_score_range') != (0, 100): count += 1
        if filters.get('volume_score_range') != (0, 100): count += 1
        if filters.get('momentum_score_range') != (0, 100): count += 1
        if filters.get('acceleration_score_range') != (0, 100): count += 1
        if filters.get('breakout_score_range') != (0, 100): count += 1
        if filters.get('rvol_score_range') != (0, 100): count += 1
        
        return count
    
    @staticmethod
    def clear_all_filters():
        """
        Reset all filters to defaults and clear widget states.
        FIXED: Now properly deletes ALL dynamic widget keys to prevent memory leaks.
        """
        # Reset centralized filter state
        st.session_state.filter_state = {
            'categories': [],
            'sectors': [],
            'industries': [],
            'min_score': 0,
            'patterns': [],
            'trend_filter': "All Trends",
            'trend_range': (0, 100),
            'trend_custom_range': (0, 100),
            'eps_tiers': [],
            'pe_tiers': [],
            'price_tiers': [],
            'eps_change_tiers': [],
            'min_pe': None,
            'max_pe': None,
            'require_fundamental_data': False,
            'market_states': [],
            'market_strength_range': (0, 100),
            'long_term_strength_range': (0, 100),
            'quick_filter': None,
            'quick_filter_applied': False,
            'performance_tiers': [],
            'performance_custom_range': (-100, 500),
            'ret_1d_range': (2.0, 25.0),
            'ret_3d_range': (3.0, 50.0),
            'ret_7d_range': (5.0, 75.0),
            'ret_30d_range': (10.0, 150.0),
            'ret_3m_range': (15.0, 200.0),
            'ret_6m_range': (20.0, 500.0),
            'ret_1y_range': (25.0, 1000.0),
            'ret_3y_range': (50.0, 2000.0),
            'ret_5y_range': (75.0, 5000.0),
            'position_tiers': [],
            'position_range': (0, 100),
            'turnover_tiers': [],
            'turnover_30d_tiers': [],
            'turnover_90d_tiers': [],
            'turnover_180d_tiers': [],
            'growth_trend_selection': "All Trends",
            'growth_trend_custom_range': (0.0, 100.0),
            'growth_trends': [],
            'growth_quality_tiers': [],
            'smart_money_flows': [],
            'vqs_grades': [],  # VQS Grade filter
            'advanced_ver_grades': [],  # Advanced VER Grade filter
            'momentum_selection': "All Momentum",
            'momentum_custom_range': (-100.0, 100.0),
            'momentum_range': (-100.0, 100.0),
            'consistency_selection': "All Consistency",
            'consistency_custom_range': (0.0, 100.0),
            'consistency_range': (0.0, 100.0),
            'volume_tiers': [],
            'rvol_range': (0.1, 20.0),
            'vmi_tiers': [],
            'custom_vmi_range': (0.5, 3.0),
            'momentum_harmony_tiers': [],
            'position_score_range': (0, 100),
            'volume_score_range': (0, 100),
            'momentum_score_range': (0, 100),
            'acceleration_score_range': (0, 100),
            'breakout_score_range': (0, 100),
            'rvol_score_range': (0, 100),
            'position_score_selection': "All Scores",
            'volume_score_selection': "All Scores",
            'momentum_score_selection': "All Scores",
            'acceleration_score_selection': "All Scores",
            'breakout_score_selection': "All Scores",
            'rvol_score_selection': "All Scores",
            # Performance filter selections
            'ret_1d_selection': "All Returns",
            'ret_3d_selection': "All Returns", 
            'ret_7d_selection': "All Returns",
            'ret_30d_selection': "All Returns",
            'ret_3m_selection': "All Returns",
            'ret_6m_selection': "All Returns",
            'ret_1y_selection': "All Returns",
            'ret_3y_selection': "All Returns",
            'ret_5y_selection': "All Returns",
            # Two-Stage Pattern Filtering System
            'exclude_patterns': [],
            'include_patterns': [],
            'combination_patterns': []
        }
        
        # CRITICAL FIX: Delete all widget keys to force UI reset
        # First, delete known widget keys
        widget_keys_to_delete = [
            # Multiselect widgets
            'category_multiselect', 'sector_multiselect', 'industry_multiselect',
            'patterns_multiselect', 'market_states_multiselect',
            'eps_tier_multiselect', 'pe_tier_multiselect', 'price_tier_multiselect',
            'eps_change_tiers_widget', 'performance_tier_multiselect', 'position_tier_multiselect',
            'volume_tier_multiselect',
            'performance_tier_multiselect_intelligence', 'volume_tier_multiselect_intelligence',
            'position_tier_multiselect_intelligence', 'turnover_tier_multiselect_dedicated',
            'turnover_30d_tier_multiselect_dedicated', 'turnover_90d_tier_multiselect_dedicated', 'turnover_180d_tier_multiselect_dedicated',
            # Two-Stage Pattern Filter Widgets
            'exclude_patterns_multiselect', 'include_patterns_multiselect',
            # Combination Pattern Filter Widget
            'combination_patterns_multiselect',
            
            # Slider widgets
            'min_score_slider', 'market_strength_slider', 'performance_custom_range_slider',
            'trend_custom_range_slider',
            'ret_1d_range_slider', 'ret_3d_range_slider', 'ret_7d_range_slider', 'ret_30d_range_slider',
            'ret_3m_range_slider', 'ret_6m_range_slider', 'ret_1y_range_slider', 'ret_3y_range_slider', 'ret_5y_range_slider',
            'position_range_slider', 'rvol_range_slider',
            'position_score_slider', 'volume_score_slider', 'momentum_score_slider',
            'acceleration_score_slider', 'breakout_score_slider', 'rvol_score_slider',
            
            # Score dropdown widgets
            'position_score_dropdown', 'volume_score_dropdown', 'momentum_score_dropdown',
            'acceleration_score_dropdown', 'breakout_score_dropdown', 'rvol_score_dropdown',
            
            # Performance dropdown widgets
            'ret_1d_dropdown', 'ret_3d_dropdown', 'ret_7d_dropdown', 'ret_30d_dropdown',
            'ret_3m_dropdown', 'ret_6m_dropdown', 'ret_1y_dropdown', 'ret_3y_dropdown', 'ret_5y_dropdown',
            
            # Selectbox widgets
            'trend_selectbox', 'wave_timeframe_select',
            
            # Text input widgets
            'eps_change_input', 'min_pe_input', 'max_pe_input',
            
            # Checkbox widgets
            'require_fundamental_checkbox',
            
            # Additional filter-related keys
            'display_count_select', 'sort_by_select', 'export_template_radio',
            'wave_sensitivity', 'show_sensitivity_details', 'show_market_regime',
            'score_component_expander'
        ]
        
        # Delete each known widget key if it exists
        deleted_count = 0
        for key in widget_keys_to_delete:
            if key in st.session_state:
                del st.session_state[key]
                deleted_count += 1
                
        # ==== MEMORY LEAK FIX - START ====
        # Now clean up ANY dynamically created widget keys
        # This is crucial for preventing memory leaks
        
        # Define all possible widget suffixes used by Streamlit
        widget_suffixes = [
            '_multiselect', '_slider', '_selectbox', '_checkbox',
            '_input', '_radio', '_button', '_expander', '_toggle',
            '_number_input', '_text_area', '_date_input', '_time_input',
            '_color_picker', '_file_uploader', '_camera_input', '_select_slider'
        ]
        
        # Also check for common prefixes used in dynamic widgets
        widget_prefixes = [
            'FormSubmitter', 'temp_', 'dynamic_', 'filter_', 'widget_'
        ]
        
        # Collect all keys to delete (can't modify dict during iteration)
        dynamic_keys_to_delete = []
        
        # Check all session state keys
        for key in list(st.session_state.keys()):
            # Skip if already deleted
            if key in widget_keys_to_delete:
                continue
            
            # Check if key has widget suffix
            for suffix in widget_suffixes:
                if key.endswith(suffix):
                    dynamic_keys_to_delete.append(key)
                    break
            
            # Check if key has widget prefix
            for prefix in widget_prefixes:
                if key.startswith(prefix) and key not in dynamic_keys_to_delete:
                    dynamic_keys_to_delete.append(key)
                    break
        
        # Delete all collected dynamic keys
        for key in dynamic_keys_to_delete:
            try:
                del st.session_state[key]
                deleted_count += 1
                logger.debug(f"Deleted dynamic widget key: {key}")
            except KeyError:
                # Key might have been deleted already
                pass
        
        # ==== MEMORY LEAK FIX - END ====
        
        # Also clear legacy filter keys for backward compatibility
        legacy_keys = [
            'category_filter', 'sector_filter', 'industry_filter',
            'min_score', 'patterns', 'trend_filter',
            'eps_tier_filter', 'pe_tier_filter', 'price_tier_filter',
            'min_eps_change', 'min_pe', 'max_pe',
            'require_fundamental_data', 'market_states_filter',
            'market_strength_range_slider'
        ]
        
        for key in legacy_keys:
            if key in st.session_state:
                if isinstance(st.session_state[key], list):
                    st.session_state[key] = []
                elif isinstance(st.session_state[key], bool):
                    st.session_state[key] = False
                elif isinstance(st.session_state[key], str):
                    if key == 'trend_filter':
                        st.session_state[key] = "All Trends"
                    else:
                        st.session_state[key] = ""
                elif isinstance(st.session_state[key], tuple):
                    if key == 'market_strength_range_slider':
                        st.session_state[key] = (0, 100)
                elif isinstance(st.session_state[key], (int, float)):
                    if key == 'min_score':
                        st.session_state[key] = 0
                    else:
                        st.session_state[key] = None
                else:
                    st.session_state[key] = None
        
        # Reset active filter count
        st.session_state.active_filter_count = 0
        
        # Clear quick filter
        st.session_state.quick_filter = None
        st.session_state.quick_filter_applied = False
        
        # Clear any cached filter results
        if 'user_preferences' in st.session_state:
            st.session_state.user_preferences['last_filters'] = {}
        
        # Clean up any cached data related to filters
        cache_keys_to_clear = []
        for key in list(st.session_state.keys()):
            if key.startswith('filter_cache_') or key.startswith('filtered_'):
                cache_keys_to_clear.append(key)
        
        for key in cache_keys_to_clear:
            del st.session_state[key]
            deleted_count += 1
        
        logger.info(f"All filters and widget states cleared successfully. Deleted {deleted_count} keys total.")
    
    @staticmethod
    def sync_widget_to_filter(widget_key: str, filter_key: str):
        """Sync widget state to filter state - used in callbacks"""
        if widget_key in st.session_state:
            st.session_state.filter_state[filter_key] = st.session_state[widget_key]
    
    @staticmethod
    def build_filter_dict() -> Dict[str, Any]:
        """
        Build comprehensive filter dictionary for apply_filters method.
        This ensures all filter types are properly included.
        """
        FilterEngine.initialize_filters()
        filters = {}
        
        # Use centralized filter state if available
        if 'filter_state' in st.session_state:
            state = st.session_state.filter_state
            
            # Map centralized state to filter dict
            if state.get('categories'):
                filters['categories'] = state['categories']
            if state.get('sectors'):
                filters['sectors'] = state['sectors']
            if state.get('industries'):
                filters['industries'] = state['industries']
            if state.get('min_score', 0) > 0:
                filters['min_score'] = state['min_score']
            if state.get('patterns'):
                filters['patterns'] = state['patterns']
            if state.get('trend_filter') != "All Trends":
                filters['trend_filter'] = state['trend_filter']
                filters['trend_range'] = state.get('trend_range', (0, 100))
            if state.get('eps_tiers'):
                filters['eps_tiers'] = state['eps_tiers']
            if state.get('pe_tiers'):
                filters['pe_tiers'] = state['pe_tiers']
            if state.get('price_tiers'):
                filters['price_tiers'] = state['price_tiers']
            if state.get('eps_change_tiers'):
                filters['eps_change_tiers'] = state['eps_change_tiers']
            if state.get('position_tiers'):
                filters['position_tiers'] = state['position_tiers']
            if state.get('turnover_tiers'):
                filters['turnover_tiers'] = state['turnover_tiers']
            if state.get('performance_tiers'):
                filters['performance_tiers'] = state['performance_tiers']
            if state.get('volume_tiers'):
                filters['volume_tiers'] = state['volume_tiers']
            if state.get('vmi_tiers'):
                filters['vmi_tiers'] = state['vmi_tiers']
            if state.get('momentum_harmony_tiers'):
                filters['momentum_harmony_tiers'] = state['momentum_harmony_tiers']
            if state.get('min_pe') is not None:
                filters['min_pe'] = state['min_pe']
            if state.get('max_pe') is not None:
                filters['max_pe'] = state['max_pe']
            if state.get('require_fundamental_data'):
                filters['require_fundamental_data'] = True
            if state.get('market_states'):
                filters['market_states'] = state['market_states']
            if state.get('market_strength_range') != (0, 100):
                filters['market_strength_range'] = state['market_strength_range']
            if state.get('long_term_strength_range') != (0, 100):
                filters['long_term_strength_range'] = state['long_term_strength_range']
            
        return filters
    
    @staticmethod
    @PerformanceMonitor.timer(target_time=0.1)
    def apply_filters(df: pd.DataFrame, filters: Optional[Dict[str, Any]] = None) -> pd.DataFrame:
        """
        Apply all filters to dataframe efficiently using vectorized operations.
        If no filters provided, get from centralized state.
        """
        if df.empty:
            return df
        
        # Use provided filters or get from state
        if filters is None:
            filters = FilterEngine.build_filter_dict()
        
        if not filters:
            return df
        
        # Create boolean masks for each filter
        masks = []
        
        # Helper function for isin filters
        def create_mask_from_isin(column: str, values: List[Any]) -> Optional[pd.Series]:
            if values and column in df.columns:
                return df[column].isin(values)
            return None
        
        # 1. Category filters
        if 'categories' in filters:
            mask = create_mask_from_isin('category', filters['categories'])
            if mask is not None:
                masks.append(mask)

        if 'sectors' in filters:
            mask = create_mask_from_isin('sector', filters['sectors'])
            if mask is not None:
                masks.append(mask)

        if 'industries' in filters:
            mask = create_mask_from_isin('industry', filters['industries'])
            if mask is not None:
                masks.append(mask)
        
        # 2. Score filter
        if filters.get('min_score', 0) > 0 and 'master_score' in df.columns:
            masks.append(df['master_score'] >= filters['min_score'])
        
        # 2.1. Intelligence Score Filters
        # Position Score Filter
        if 'position_score_range' in filters and 'position_score' in df.columns:
            position_range = filters['position_score_range']
            if position_range != (0, 100):
                min_pos, max_pos = position_range
                masks.append((df['position_score'] >= min_pos) & (df['position_score'] <= max_pos))
        
        # Volume Score Filter
        if 'volume_score_range' in filters and 'volume_score' in df.columns:
            volume_range = filters['volume_score_range']
            if volume_range != (0, 100):
                min_vol, max_vol = volume_range
                masks.append((df['volume_score'] >= min_vol) & (df['volume_score'] <= max_vol))
        
        # Momentum Score Filter
        if 'momentum_score_range' in filters and 'momentum_score' in df.columns:
            momentum_range = filters['momentum_score_range']
            if momentum_range != (0, 100):
                min_mom, max_mom = momentum_range
                masks.append((df['momentum_score'] >= min_mom) & (df['momentum_score'] <= max_mom))
        
        # Acceleration Score Filter
        if 'acceleration_score_range' in filters and 'acceleration_score' in df.columns:
            acceleration_range = filters['acceleration_score_range']
            if acceleration_range != (0, 100):
                min_acc, max_acc = acceleration_range
                masks.append((df['acceleration_score'] >= min_acc) & (df['acceleration_score'] <= max_acc))
        
        # Breakout Score Filter
        if 'breakout_score_range' in filters and 'breakout_score' in df.columns:
            breakout_range = filters['breakout_score_range']
            if breakout_range != (0, 100):
                min_brk, max_brk = breakout_range
                masks.append((df['breakout_score'] >= min_brk) & (df['breakout_score'] <= max_brk))
        
        # RVOL Score Filter
        if 'rvol_score_range' in filters and 'rvol_score' in df.columns:
            rvol_range = filters['rvol_score_range']
            if rvol_range != (0, 100):
                min_rvol, max_rvol = rvol_range
                masks.append((df['rvol_score'] >= min_rvol) & (df['rvol_score'] <= max_rvol))
        
        # 3. Advanced Two-Stage Pattern Filter (Professional Implementation)
        
        # STAGE 1: EXCLUDE patterns (remove stocks with these patterns)
        if filters.get('exclude_patterns') and 'patterns' in df.columns:
            exclude_mask = pd.Series(False, index=df.index)
            for pattern in filters['exclude_patterns']:
                exclude_mask |= df['patterns'].str.contains(pattern, na=False, regex=False)
            # Apply exclusion: keep stocks that DON'T have excluded patterns
            masks.append(~exclude_mask)
        
        # STAGE 2: INCLUDE patterns (from remaining stocks, show only these)
        if filters.get('include_patterns') and 'patterns' in df.columns:
            include_mask = pd.Series(False, index=df.index)
            for pattern in filters['include_patterns']:
                include_mask |= df['patterns'].str.contains(pattern, na=False, regex=False)
            # Apply inclusion: keep stocks that DO have included patterns
            masks.append(include_mask)
        
        # STAGE 3: COMBINATION patterns (require ALL selected patterns - AND logic)
        if filters.get('combination_patterns') and 'patterns' in df.columns:
            combination_mask = pd.Series(True, index=df.index)  # Start with True for AND logic
            for pattern in filters['combination_patterns']:
                # Each pattern must be present (AND logic)
                pattern_present = df['patterns'].str.contains(pattern, na=False, regex=False)
                combination_mask &= pattern_present
            # Apply combination filter: keep stocks that have ALL combination patterns
            masks.append(combination_mask)
        
        # Legacy pattern filter support (for backward compatibility)
        if filters.get('patterns') and 'patterns' in df.columns:
            pattern_mask = pd.Series(False, index=df.index)
            for pattern in filters['patterns']:
                pattern_mask |= df['patterns'].str.contains(pattern, na=False, regex=False)
            masks.append(pattern_mask)
        
        # 4. Trend filter
        trend_range = filters.get('trend_range')
        if trend_range and trend_range != (0, 100) and 'trend_quality' in df.columns:
            min_trend, max_trend = trend_range
            masks.append((df['trend_quality'] >= min_trend) & (df['trend_quality'] <= max_trend))
        
        # 5. EPS change filter - Now using tiers instead of min value
        if 'eps_change_tiers' in filters:
            masks.append(create_mask_from_isin('eps_change_tier', filters['eps_change_tiers']))
        
        # 5.5. Position Intelligence filters
        if 'position_tiers' in filters:
            selected_tiers = filters['position_tiers']
            if selected_tiers and "🎯 Custom Range" not in selected_tiers:
                masks.append(create_mask_from_isin('position_tier', selected_tiers))
        
        # Custom position range filter (only if "🎯 Custom Range" is selected)
        if 'position_tiers' in filters and "🎯 Custom Range" in filters['position_tiers']:
            if 'position_range' in filters:
                position_range = filters['position_range']
                if 'position_pct' in df.columns:
                    masks.append(df['position_pct'].between(position_range[0], position_range[1], inclusive='both'))
        
        # 5.55. Enhanced Turnover Intelligence filters
        if 'turnover_tiers' in filters:
            selected_tiers = filters['turnover_tiers']
            if selected_tiers:
                masks.append(create_mask_from_isin('daily_turnover_tier', selected_tiers))
        
        # 5.55.1. 30-Day Turnover filters (SMA 20d basis)
        if 'turnover_30d_tiers' in filters:
            selected_tiers = filters['turnover_30d_tiers']
            if selected_tiers:
                masks.append(create_mask_from_isin('turnover_30d_tier', selected_tiers))
        
        # 5.55.2. 90-Day Turnover filters (SMA 50d basis)
        if 'turnover_90d_tiers' in filters:
            selected_tiers = filters['turnover_90d_tiers']
            if selected_tiers:
                masks.append(create_mask_from_isin('turnover_90d_tier', selected_tiers))
        
        # 5.55.3. 180-Day Turnover filters (SMA 200d basis)
        if 'turnover_180d_tiers' in filters:
            selected_tiers = filters['turnover_180d_tiers']
            if selected_tiers:
                masks.append(create_mask_from_isin('turnover_180d_tier', selected_tiers))
        
        # 5.56. 🌊 LIQUIDITY GROWTH ANALYTICS FILTERS - Advanced Multi-Period Analysis
        
        # 5.56.0. 🏆 VQS Grade Filter
        if 'vqs_grades' in filters and 'vqs_grade' in df.columns:
            selected_grades = filters['vqs_grades']
            if selected_grades:
                masks.append(df['vqs_grade'].isin(selected_grades))
        
        # 5.56.0.5. ⚡ Advanced VER Grade Filter
        if 'advanced_ver_grades' in filters and 'advanced_ver_grade' in df.columns:
            selected_grades = filters['advanced_ver_grades']
            if selected_grades:
                masks.append(df['advanced_ver_grade'].isin(selected_grades))
        
        # 5.56.1. Growth Trend Classification Filter (with Custom Range support)
        if 'growth_trend_custom_range' in filters and 'composite_growth_score' in df.columns:
            # Custom Range selected - filter by growth score range
            custom_range = filters['growth_trend_custom_range']
            min_score, max_score = custom_range
            masks.append((df['composite_growth_score'] >= min_score) & (df['composite_growth_score'] <= max_score))
        elif 'growth_trends' in filters and 'growth_trend' in df.columns:
            # Preset trend selected - filter by trend classification
            selected_trends = filters['growth_trends']
            if selected_trends:
                masks.append(create_mask_from_isin('growth_trend', selected_trends))
        
        # 5.56.2. Growth Quality Tier Filter
        if 'growth_quality_tiers' in filters and 'growth_quality_tier' in df.columns:
            selected_tiers = filters['growth_quality_tiers']
            if selected_tiers:
                masks.append(create_mask_from_isin('growth_quality_tier', selected_tiers))
        
        # 5.56.3. Smart Money Flow Detection Filter
        if 'smart_money_flows' in filters and 'smart_money_flow' in df.columns:
            selected_flows = filters['smart_money_flows']
            if selected_flows:
                masks.append(create_mask_from_isin('smart_money_flow', selected_flows))
        
        # 5.56.4. Growth Momentum Range Filter (with Custom Range support)
        if 'momentum_custom_range' in filters and 'growth_momentum' in df.columns:
            # Custom Range selected - filter by custom momentum range
            custom_range = filters['momentum_custom_range']
            min_momentum, max_momentum = custom_range
            masks.append((df['growth_momentum'] >= min_momentum) & (df['growth_momentum'] <= max_momentum))
        elif 'momentum_range' in filters and 'growth_momentum' in df.columns:
            # Preset momentum selected - filter by preset range
            momentum_range = filters['momentum_range']
            if momentum_range != (-100.0, 100.0):  # Only apply if not default
                min_momentum, max_momentum = momentum_range
                masks.append((df['growth_momentum'] >= min_momentum) & (df['growth_momentum'] <= max_momentum))
        
        # 5.56.5. Growth Consistency Range Filter (with Custom Range support)
        if 'consistency_custom_range' in filters and 'growth_consistency' in df.columns:
            # Custom Range selected - filter by custom consistency range
            custom_range = filters['consistency_custom_range']
            min_consistency, max_consistency = custom_range
            masks.append((df['growth_consistency'] >= min_consistency) & (df['growth_consistency'] <= max_consistency))
        elif 'consistency_range' in filters and 'growth_consistency' in df.columns:
            # Preset consistency selected - filter by preset range
            consistency_range = filters['consistency_range']
            if consistency_range != (0.0, 100.0):  # Only apply if not default
                min_consistency, max_consistency = consistency_range
                masks.append((df['growth_consistency'] >= min_consistency) & (df['growth_consistency'] <= max_consistency))
        
        # 5.5.1. Daily vs 30-Day Turnover Growth Filter
        if 'growth_daily_30_custom_range' in filters and 'growth_daily_to_30' in df.columns:
            # Custom range selected for Daily vs 30-Day growth
            custom_range = filters['growth_daily_30_custom_range']
            min_growth, max_growth = custom_range
            masks.append((df['growth_daily_to_30'] >= min_growth) & (df['growth_daily_to_30'] <= max_growth))
        elif 'growth_daily_30_range' in filters and 'growth_daily_to_30' in df.columns:
            # Preset range selected for Daily vs 30-Day growth
            growth_range = filters['growth_daily_30_range']
            min_growth, max_growth = growth_range
            masks.append((df['growth_daily_to_30'] >= min_growth) & (df['growth_daily_to_30'] <= max_growth))
        
        # 5.5.2. 30-Day vs 90-Day Turnover Growth Filter
        if 'growth_30_90_custom_range' in filters and 'growth_30_to_90' in df.columns:
            # Custom range selected for 30-Day vs 90-Day growth
            custom_range = filters['growth_30_90_custom_range']
            min_growth, max_growth = custom_range
            masks.append((df['growth_30_to_90'] >= min_growth) & (df['growth_30_to_90'] <= max_growth))
        elif 'growth_30_90_range' in filters and 'growth_30_to_90' in df.columns:
            # Preset range selected for 30-Day vs 90-Day growth
            growth_range = filters['growth_30_90_range']
            min_growth, max_growth = growth_range
            masks.append((df['growth_30_to_90'] >= min_growth) & (df['growth_30_to_90'] <= max_growth))
        
        # 5.5.3. 90-Day vs 180-Day Turnover Growth Filter
        if 'growth_90_180_custom_range' in filters and 'growth_90_to_180' in df.columns:
            # Custom range selected for 90-Day vs 180-Day growth
            custom_range = filters['growth_90_180_custom_range']
            min_growth, max_growth = custom_range
            masks.append((df['growth_90_to_180'] >= min_growth) & (df['growth_90_to_180'] <= max_growth))
        elif 'growth_90_180_range' in filters and 'growth_90_to_180' in df.columns:
            # Preset range selected for 90-Day vs 180-Day growth
            growth_range = filters['growth_90_180_range']
            min_growth, max_growth = growth_range
            masks.append((df['growth_90_to_180'] >= min_growth) & (df['growth_90_to_180'] <= max_growth))
        
        # 5.6. Performance Intelligence filters
        if 'performance_tiers' in filters:
            selected_tiers = filters['performance_tiers']
            # Only apply preset tier filtering if "Custom Range" is not selected
            custom_range_in_tiers = any("Custom Range" in tier for tier in selected_tiers) if selected_tiers else False
            if selected_tiers and not custom_range_in_tiers:
                masks.append(create_mask_from_isin('performance_tier', selected_tiers))
        
        # Custom performance range filter (only apply if actual range filters are modified from defaults)
        custom_range_selected = 'performance_tiers' in filters and any("Custom Range" in tier for tier in filters['performance_tiers'])
        
        if custom_range_selected:
            # Define default ranges for each timeframe (MUST MATCH UI EXACTLY) - REALISTIC INDIAN MARKET THRESHOLDS
            default_ranges = {
                'ret_1d_range': (2.0, 25.0),
                'ret_3d_range': (3.0, 50.0),
                'ret_7d_range': (5.0, 75.0),
                'ret_30d_range': (10.0, 150.0),
                'ret_3m_range': (15.0, 200.0),
                'ret_6m_range': (20.0, 500.0),
                'ret_1y_range': (25.0, 1000.0),
                'ret_3y_range': (50.0, 2000.0),
                'ret_5y_range': (75.0, 5000.0)
            }
            
            # Only apply filters for ranges that have been modified from defaults
            active_custom_ranges = []
            
            # Short-term performance ranges
            if 'ret_1d_range' in filters and 'ret_1d' in df.columns:
                range_val = filters['ret_1d_range']
                if range_val != default_ranges['ret_1d_range']:
                    masks.append(df['ret_1d'].between(range_val[0], range_val[1], inclusive='both'))
                    active_custom_ranges.append('ret_1d_range')
            
            if 'ret_3d_range' in filters and 'ret_3d' in df.columns:
                range_val = filters['ret_3d_range']
                if range_val != default_ranges['ret_3d_range']:
                    masks.append(df['ret_3d'].between(range_val[0], range_val[1], inclusive='both'))
                    active_custom_ranges.append('ret_3d_range')
            
            if 'ret_7d_range' in filters and 'ret_7d' in df.columns:
                range_val = filters['ret_7d_range']
                if range_val != default_ranges['ret_7d_range']:
                    masks.append(df['ret_7d'].between(range_val[0], range_val[1], inclusive='both'))
                    active_custom_ranges.append('ret_7d_range')
            
            if 'ret_30d_range' in filters and 'ret_30d' in df.columns:
                range_val = filters['ret_30d_range']
                if range_val != default_ranges['ret_30d_range']:
                    masks.append(df['ret_30d'].between(range_val[0], range_val[1], inclusive='both'))
                    active_custom_ranges.append('ret_30d_range')
            
            # Medium-term performance ranges
            if 'ret_3m_range' in filters and 'ret_3m' in df.columns:
                range_val = filters['ret_3m_range']
                if range_val != default_ranges['ret_3m_range']:
                    masks.append(df['ret_3m'].between(range_val[0], range_val[1], inclusive='both'))
                    active_custom_ranges.append('ret_3m_range')
            
            if 'ret_6m_range' in filters and 'ret_6m' in df.columns:
                range_val = filters['ret_6m_range']
                if range_val != default_ranges['ret_6m_range']:
                    masks.append(df['ret_6m'].between(range_val[0], range_val[1], inclusive='both'))
                    active_custom_ranges.append('ret_6m_range')
            
            # Long-term performance ranges
            if 'ret_1y_range' in filters and 'ret_1y' in df.columns:
                range_val = filters['ret_1y_range']
                if range_val != default_ranges['ret_1y_range']:
                    masks.append(df['ret_1y'].between(range_val[0], range_val[1], inclusive='both'))
                    active_custom_ranges.append('ret_1y_range')
            
            if 'ret_3y_range' in filters and 'ret_3y' in df.columns:
                range_val = filters['ret_3y_range']
                if range_val != default_ranges['ret_3y_range']:
                    masks.append(df['ret_3y'].between(range_val[0], range_val[1], inclusive='both'))
                    active_custom_ranges.append('ret_3y_range')
            
            if 'ret_5y_range' in filters and 'ret_5y' in df.columns:
                range_val = filters['ret_5y_range']
                if range_val != default_ranges['ret_5y_range']:
                    masks.append(df['ret_5y'].between(range_val[0], range_val[1], inclusive='both'))
                    active_custom_ranges.append('ret_5y_range')
            
            # CRITICAL FIX: If Custom Range is selected but no ranges are modified,
            # don't apply any performance filtering (show all stocks)
            # This prevents the "0 stocks" issue when just selecting Custom Range
            if not active_custom_ranges:
                # No custom ranges are active, so don't filter by performance
                pass  # This effectively shows all stocks for performance filtering
            else:
                logger.info(f"Active custom ranges: {active_custom_ranges}")
            
            # Legacy support for old performance_custom_range
            if 'performance_custom_range' in filters:
                perf_range = filters['performance_custom_range']
                # Apply Custom Range to any available return percentage column
                perf_masks = []
                for col in ['ret_1d', 'ret_7d', 'ret_30d']:
                    if col in df.columns:
                        perf_masks.append(df[col].between(perf_range[0], perf_range[1], inclusive='both'))
                if perf_masks:
                    # Use OR logic - stock qualifies if it meets the range in ANY timeframe
                    combined_mask = perf_masks[0]
                    for mask in perf_masks[1:]:
                        combined_mask = combined_mask | mask
                    masks.append(combined_mask)
        
        # 5.6.1. NEW Individual Performance Period Filters (V9 Enhancement)
        # Handle individual return period filters from the new Performance Filter UI
        individual_performance_ranges = {
            'ret_1d_range': 'ret_1d',
            'ret_3d_range': 'ret_3d', 
            'ret_7d_range': 'ret_7d',
            'ret_30d_range': 'ret_30d',
            'ret_3m_range': 'ret_3m',
            'ret_6m_range': 'ret_6m',
            'ret_1y_range': 'ret_1y',
            'ret_3y_range': 'ret_3y',
            'ret_5y_range': 'ret_5y'
        }
        
        for range_key, column_name in individual_performance_ranges.items():
            if range_key in filters and column_name in df.columns:
                range_val = filters[range_key]
                if isinstance(range_val, tuple) and len(range_val) == 2:
                    min_val, max_val = range_val
                    masks.append(df[column_name].between(min_val, max_val, inclusive='both'))
                    logger.info(f"Applied {range_key} filter: {range_val} on column {column_name}")
        
        # 5.7. Volume Intelligence filters
        if 'volume_tiers' in filters:
            selected_tiers = filters['volume_tiers']
            if selected_tiers and "🎯 Custom RVOL Range" not in selected_tiers:
                masks.append(create_mask_from_isin('volume_tier', selected_tiers))
        
        # Custom RVOL range filter (only if "🎯 Custom RVOL Range" is selected)
        if 'volume_tiers' in filters and "🎯 Custom RVOL Range" in filters['volume_tiers']:
            if 'rvol_range' in filters and 'rvol' in df.columns:
                rvol_range_val = filters['rvol_range']
                masks.append(df['rvol'].between(rvol_range_val[0], rvol_range_val[1], inclusive='both'))
        
        # 5.8. VMI (Volume Momentum Index) filters
        if 'vmi_tiers' in filters:
            selected_tiers = filters['vmi_tiers']
            if selected_tiers and "🎯 Custom VMI Range" not in selected_tiers:
                masks.append(create_mask_from_isin('vmi_tier', selected_tiers))
        
        # Custom VMI range filter (only if "🎯 Custom VMI Range" is selected)
        if 'vmi_tiers' in filters and "🎯 Custom VMI Range" in filters['vmi_tiers']:
            if 'custom_vmi_range' in filters and 'vmi' in df.columns:
                vmi_range_val = filters['custom_vmi_range']
                masks.append(df['vmi'].between(vmi_range_val[0], vmi_range_val[1], inclusive='both'))
        
        # 5.9. Momentum Harmony filters
        if 'momentum_harmony_tiers' in filters:
            selected_tiers = filters['momentum_harmony_tiers']
            if selected_tiers:
                masks.append(create_mask_from_isin('momentum_harmony_tier', selected_tiers))
        
        # 6. PE filters
        if filters.get('min_pe') is not None and 'pe' in df.columns:
            masks.append(df['pe'] >= filters['min_pe'])
        
        if filters.get('max_pe') is not None and 'pe' in df.columns:
            masks.append(df['pe'] <= filters['max_pe'])
        
        # 7. Tier filters
        if 'eps_tiers' in filters:
            masks.append(create_mask_from_isin('eps_tier', filters['eps_tiers']))
        if 'pe_tiers' in filters:
            masks.append(create_mask_from_isin('pe_tier', filters['pe_tiers']))
        if 'price_tiers' in filters:
            masks.append(create_mask_from_isin('price_tier', filters['price_tiers']))
        
        # 8. Data completeness filter
        if filters.get('require_fundamental_data', False):
            if all(col in df.columns for col in ['pe', 'eps_change_pct']):
                masks.append(df['pe'].notna() & (df['pe'] > 0) & df['eps_change_pct'].notna())
        
        # 9. Market State filters
        if 'market_states' in filters:
            selected_states = filters['market_states']
            if selected_states:
                # Handle preset filters
                preset_mapping = {
                    "🎯 MOMENTUM (Default)": ['STRONG_UPTREND', 'UPTREND', 'PULLBACK'],
                    "⚡ AGGRESSIVE": ['STRONG_UPTREND'],
                    "💎 VALUE": ['PULLBACK', 'BOUNCE', 'SIDEWAYS'],
                    "🛡️ DEFENSIVE": ['STRONG_UPTREND', 'UPTREND', 'PULLBACK', 'SIDEWAYS', 'BOUNCE'],
                    "🌍 ALL": ['STRONG_UPTREND', 'UPTREND', 'PULLBACK', 'ROTATION', 'SIDEWAYS', 'DOWNTREND', 'STRONG_DOWNTREND', 'BOUNCE']
                }
                
                # Valid individual market states
                valid_market_states = {'STRONG_UPTREND', 'UPTREND', 'PULLBACK', 'ROTATION', 'SIDEWAYS', 'DOWNTREND', 'STRONG_DOWNTREND', 'BOUNCE'}
                
                # Collect all allowed states from presets and custom selections
                allowed_states = []
                custom_selection_active = "📊 Custom Selection" in selected_states
                
                for state in selected_states:
                    if state in preset_mapping:
                        # Handle preset selection
                        allowed_states.extend(preset_mapping[state])
                    elif state in valid_market_states:
                        # Handle individual state selection (always include individual states if selected)
                        if custom_selection_active:
                            allowed_states.append(state)
                        else:
                            # If custom selection is not active, but individual states are selected,
                            # this might be a legacy case - still include them
                            allowed_states.append(state)
                    # Skip "📊 Custom Selection" itself as it's just an enabler
                
                # Debug logging for troubleshooting
                if custom_selection_active:
                    individual_states = [s for s in selected_states if s in valid_market_states]
                    logger.info(f"Market State Filter Debug:")
                    logger.info(f"  - Selected states: {selected_states}")
                    logger.info(f"  - Individual states: {individual_states}")
                    logger.info(f"  - Final allowed states: {allowed_states}")
                    
                    # Additional debug: check if market_state column exists and has expected values
                    if 'market_state' in df.columns:
                        unique_values = df['market_state'].value_counts().head(10)
                        logger.info(f"  - Market state values in data: {unique_values.to_dict()}")
                
                # Remove duplicates and apply filter - but only if we have allowed states
                if allowed_states:
                    unique_states = list(set(allowed_states))
                    logger.info(f"  - Applying filter for states: {unique_states}")
                    mask = create_mask_from_isin('market_state', unique_states)
                    if mask is not None:
                        masks.append(mask)
                        logger.info(f"  - Filter mask created successfully, {mask.sum()} stocks match")
                    else:
                        logger.warning("  - Failed to create market state filter mask")
                elif custom_selection_active:
                    # User selected "📊 Custom Selection" but no individual states
                    logger.info("  - Custom selection active but no individual states selected, skipping filter")
                else:
                    logger.warning(f"  - No allowed states found for selection: {selected_states}")
        
        # Market Strength Filter - Professional Implementation
        if 'market_strength_range' in filters and 'overall_market_strength' in df.columns:
            strength_range = filters['market_strength_range']
            if strength_range != (0, 100):
                min_strength, max_strength = strength_range
                strength_mask = (df['overall_market_strength'] >= min_strength) & (df['overall_market_strength'] <= max_strength)
                masks.append(strength_mask)
                logger.info(f"Applied Market Strength filter: {strength_range}, {strength_mask.sum()} stocks match")
        
        # Long Term Strength Filter - Professional Implementation
        if 'long_term_strength_range' in filters and 'long_term_strength' in df.columns:
            lts_range = filters['long_term_strength_range']
            if lts_range != (0, 100):
                min_lts, max_lts = lts_range
                lts_mask = (df['long_term_strength'] >= min_lts) & (df['long_term_strength'] <= max_lts)
                masks.append(lts_mask)
                logger.info(f"Applied Long Term Strength filter: {lts_range}, {lts_mask.sum()} stocks match")
        
        # Combine all masks
        masks = [mask for mask in masks if mask is not None]
        
        if masks:
            combined_mask = np.logical_and.reduce(masks)
            filtered_df = df[combined_mask].copy()
        else:
            filtered_df = df.copy()
        
        logger.info(f"Filters reduced {len(df)} to {len(filtered_df)} stocks")
        
        return filtered_df
    
    @staticmethod
    def get_filter_options(df: pd.DataFrame, column: str, current_filters: Optional[Dict[str, Any]] = None) -> List[str]:
        """
        Get available options for a filter based on other active filters.
        This creates BIDIRECTIONAL SMART INTERCONNECTED filters for Category/Sector/Industry.
        """
        if df is None or df.empty or column not in df.columns:
            return []
        
        # Use current filters or get from state
        if current_filters is None:
            current_filters = FilterEngine.build_filter_dict()
        
        # BIDIRECTIONAL SMART INTERCONNECTION LOGIC
        temp_filters = current_filters.copy()
        
        # For Category/Sector/Industry interconnection - BIDIRECTIONAL FILTERING
        if column == 'category':
            # For categories, apply sector AND industry filters to show only relevant categories
            temp_filters.pop('categories', None)  # Don't apply category filter when getting categories
            # Keep sectors and industries filters to show only categories that have those sectors/industries
        elif column == 'sector':
            # For sectors, apply category AND industry filters to show only relevant sectors
            temp_filters.pop('sectors', None)  # Don't apply sector filter when getting sectors  
            # Keep categories and industries filters to show only sectors that have those categories/industries
        elif column == 'industry':
            # For industries, apply category AND sector filters to show only relevant industries
            temp_filters.pop('industries', None)  # Don't apply industry filter when getting industries
            # Keep categories and sectors filters to show only industries that have those categories/sectors
        else:
            # For non-interconnected filters, remove only the current filter
            filter_key_map = {
                'eps_tier': 'eps_tiers',
                'pe_tier': 'pe_tiers',
                'price_tier': 'price_tiers',
                'eps_change_tier': 'eps_change_tiers',
                'market_state': 'market_states',
                'position_tier': 'position_tiers',
                'volume_tier': 'volume_tiers',
                'performance_tier': 'performance_tiers',
                'vmi_tier': 'vmi_tiers',
                'momentum_harmony_tier': 'momentum_harmony_tiers'
            }
            
            if column in filter_key_map:
                temp_filters.pop(filter_key_map[column], None)
        
        # Apply remaining filters to get the filtered dataset
        filtered_df = FilterEngine.apply_filters(df, temp_filters)
        
        # Get unique values from the filtered dataset
        values = filtered_df[column].dropna().unique()
        
        # Filter out invalid values
        values = [v for v in values if str(v).strip() not in ['Unknown', 'unknown', '', 'nan', 'NaN', 'None', 'N/A', '-']]
        
        # Sort appropriately
        try:
            values = sorted(values, key=lambda x: float(str(x).replace(',', '')))
        except (ValueError, TypeError):
            values = sorted(values, key=str)
        
        return values
    
    @staticmethod
    def reset_to_defaults():
        """Reset filters to default state but keep widget keys"""
        FilterEngine.initialize_filters()
        
        # Reset only the filter values, not the widgets
        st.session_state.filter_state = {
            'categories': [],
            'sectors': [],
            'industries': [],
            'min_score': 0,
            'patterns': [],
            'trend_filter': "All Trends",
            'trend_range': (0, 100),
            'eps_tiers': [],
            'pe_tiers': [],
            'price_tiers': [],
            'eps_change_tiers': [],
            'position_tiers': [],
            'position_range': (0, 100),
            'turnover_tiers': [],
            'turnover_30d_tiers': [],
            'turnover_90d_tiers': [],
            'turnover_180d_tiers': [],
            'growth_trend_selection': "All Trends",
            'growth_trend_custom_range': (0.0, 100.0),
            'growth_trends': [],
            'growth_quality_tiers': [],
            'smart_money_flows': [],
            'vqs_grades': [],  # VQS Grade filter
            'advanced_ver_grades': [],  # Advanced VER Grade filter
            'momentum_selection': "All Momentum",
            'momentum_custom_range': (-100.0, 100.0),
            'momentum_range': (-100.0, 100.0),
            'consistency_selection': "All Consistency",
            'consistency_custom_range': (0.0, 100.0),
            'consistency_range': (0.0, 100.0),
            'performance_tiers': [],
            'performance_custom_range': (-100, 500),
            'volume_tiers': [],
            'rvol_range': (0.1, 20.0),
            'vmi_tiers': [],
            'custom_vmi_range': (0.5, 3.0),
            'momentum_harmony_tiers': [],
            'ret_1d_range': (2.0, 25.0),
            'ret_3d_range': (3.0, 50.0),
            'ret_7d_range': (5.0, 75.0),
            'ret_30d_range': (10.0, 150.0),
            'ret_3m_range': (15.0, 200.0),
            'ret_6m_range': (20.0, 500.0),
            'ret_1y_range': (25.0, 1000.0),
            'ret_3y_range': (50.0, 2000.0),
            'ret_5y_range': (75.0, 5000.0),
            'min_pe': None,
            'max_pe': None,
            'require_fundamental_data': False,
            'market_states': [],
            'market_strength_range': (0, 100),
            'long_term_strength_range': (0, 100),
            'position_score_range': (0, 100),
            'volume_score_range': (0, 100),
            'momentum_score_range': (0, 100),
            'acceleration_score_range': (0, 100),
            'breakout_score_range': (0, 100),
            'rvol_score_range': (0, 100),
            'position_score_selection': "All Scores",
            'volume_score_selection': "All Scores",
            'momentum_score_selection': "All Scores",
            'acceleration_score_selection': "All Scores",
            'breakout_score_selection': "All Scores",
            'rvol_score_selection': "All Scores",
            'ret_1d_selection': "All Returns",
            'ret_3d_selection': "All Returns",
            'ret_7d_selection': "All Returns",
            'ret_30d_selection': "All Returns",
            'ret_3m_selection': "All Returns",
            'ret_6m_selection': "All Returns",
            'ret_1y_selection': "All Returns",
            'ret_3y_selection': "All Returns",
            'ret_5y_selection': "All Returns",
            'quick_filter': None,
            'quick_filter_applied': False,
            # Three-Stage Pattern Filtering System
            'exclude_patterns': [],
            'include_patterns': [],
            'combination_patterns': []
        }

        # Clean up ALL dynamically created widget keys
        all_widget_patterns = [
            '_multiselect', '_slider', '_selectbox', '_checkbox', 
            '_input', '_radio', '_button', '_expander', '_toggle',
            '_number_input', '_text_area', '_date_input', '_time_input',
            '_color_picker', '_file_uploader', '_camera_input'
        ]
        
        # Collect keys to delete (can't modify dict during iteration)
        dynamic_keys_to_delete = []
        
        for key in list(st.session_state.keys()):
            # Check if this key ends with any widget pattern
            for pattern in all_widget_patterns:
                if pattern in key:
                    dynamic_keys_to_delete.append(key)
                    break
        
        # Delete the dynamic keys
        for key in dynamic_keys_to_delete:
            try:
                del st.session_state[key]
                logger.debug(f"Deleted dynamic widget key: {key}")
            except KeyError:
                # Key might have been deleted already
                pass
        
        # Also clean up any keys that start with 'FormSubmitter'
        form_keys_to_delete = [key for key in st.session_state.keys() if key.startswith('FormSubmitter')]
        for key in form_keys_to_delete:
            try:
                del st.session_state[key]
            except KeyError:
                pass
        # ==== COMPREHENSIVE WIDGET CLEANUP - END ====
        st.session_state.active_filter_count = 0
        logger.info("Filters reset to defaults")
        
# ============================================
# SEARCH ENGINE
# ============================================

class SearchEngine:
    """Optimized search functionality"""
    
    @staticmethod
    @PerformanceMonitor.timer(target_time=0.05)
    def search_stocks(df: pd.DataFrame, query: str) -> pd.DataFrame:
        """Search stocks with optimized performance"""
        
        if not query or df.empty:
            return pd.DataFrame()
        
        try:
            query = query.upper().strip()
            df['ticker'].str.upper().str.contains(query.upper())
            
            # Method 1: Direct ticker match
            ticker_exact = df[df['ticker'].str.upper() == query]
            if not ticker_exact.empty:
                return ticker_exact
            
            # Method 2: Ticker contains
            ticker_contains = df[df['ticker'].str.upper().str.contains(query, na=False, regex=False)]
            
            # Method 3: Company name contains
            if 'company_name' in df.columns:
                company_contains = df[df['company_name'].str.upper().str.contains(query, na=False, regex=False)]
            else:
                company_contains = pd.DataFrame()
            
            # Method 4: Word match
            def word_starts_with(company_name_str):
                if pd.isna(company_name_str):
                    return False
                words = str(company_name_str).upper().split()
                return any(word.startswith(query) for word in words)
            
            if 'company_name' in df.columns:
                company_word_match = df[df['company_name'].apply(word_starts_with)]
            else:
                company_word_match = pd.DataFrame()
            
            # Combine results
            all_matches = pd.concat([
                ticker_contains,
                company_contains,
                company_word_match
            ]).drop_duplicates()
            
            # Sort by relevance
            if not all_matches.empty:
                all_matches['relevance'] = 0
                all_matches.loc[all_matches['ticker'].str.upper() == query, 'relevance'] = 100
                all_matches.loc[all_matches['ticker'].str.upper().str.startswith(query), 'relevance'] += 50
                
                if 'company_name' in all_matches.columns:
                    all_matches.loc[all_matches['company_name'].str.upper().str.startswith(query), 'relevance'] += 30
                
                return all_matches.sort_values(['relevance', 'master_score'], ascending=[False, False]).drop('relevance', axis=1)
            
            return pd.DataFrame()
            
        except Exception as e:
            logger.error(f"Search error: {str(e)}")
            return pd.DataFrame()

# ============================================
# EXPORT ENGINE
# ============================================

class ExportEngine:
    """Handle all export operations"""
    
    @staticmethod
    @PerformanceMonitor.timer(target_time=1.0)
    def create_excel_report(df: pd.DataFrame, template: str = 'full') -> BytesIO:
        """Create comprehensive Excel report"""
        
        output = BytesIO()
        
        templates = {
            'day_trader': {
                'columns': ['rank', 'ticker', 'company_name', 'master_score', 'rvol', 
                           'momentum_score', 'acceleration_score', 'ret_1d', 'ret_7d', 
                           'volume_score', 'vmi', 'market_state', 'patterns', 'category', 'industry'],
                'focus': 'Intraday momentum and volume'
            },
            'swing_trader': {
                'columns': ['rank', 'ticker', 'company_name', 'master_score', 
                           'breakout_score', 'position_score', 'position_tension',
                           'from_high_pct', 'from_low_pct', 'trend_quality', 
                           'momentum_harmony', 'patterns', 'industry'],
                'focus': 'Position and breakout setups'
            },
            'investor': {
                'columns': ['rank', 'ticker', 'company_name', 'master_score', 'pe', 
                           'eps_current', 'eps_change_pct', 'ret_1y', 'ret_3y', 
                           'long_term_strength', 'money_flow_mm', 'category', 'sector', 'industry'],
                'focus': 'Fundamentals and long-term performance'
            },
            'full': {
                'columns': None,
                'focus': 'Complete analysis'
            }
        }
        
        try:
            with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                workbook = writer.book
                
                header_format = workbook.add_format({
                    'bold': True,
                    'bg_color': '#3498db',
                    'font_color': 'white',
                    'border': 1
                })
                
                # 1. Top 100 Stocks
                top_100 = df.nlargest(min(100, len(df)), 'master_score')
                
                if template in templates and templates[template]['columns']:
                    export_cols = [col for col in templates[template]['columns'] if col in top_100.columns]
                else:
                    export_cols = None
                
                if export_cols:
                    top_100_export = top_100[export_cols]
                else:
                    top_100_export = top_100
                
                top_100_export.to_excel(writer, sheet_name='Top 100', index=False)
                
                worksheet = writer.sheets['Top 100']
                for i, col in enumerate(top_100_export.columns):
                    worksheet.write(0, i, col, header_format)
                
                # 2. Market Intelligence
                intel_data = []
                
                regime, regime_metrics = MarketIntelligence.detect_market_regime(df)
                intel_data.append({
                    'Metric': 'Market Regime',
                    'Value': regime,
                    'Details': f"Breadth: {regime_metrics.get('breadth', 0):.1%}"
                })
                
                ad_metrics = MarketIntelligence.calculate_advance_decline_ratio(df)
                intel_data.append({
                    'Metric': 'Advance/Decline',
                    'Value': f"{ad_metrics.get('advancing', 0)}/{ad_metrics.get('declining', 0)}",
                    'Details': f"Ratio: {ad_metrics.get('ad_ratio', 1):.2f}"
                })
                
                intel_df = pd.DataFrame(intel_data)
                intel_df.to_excel(writer, sheet_name='Market Intelligence', index=False)
                
                # 3. Sector Rotation
                sector_rotation = MarketIntelligence.detect_sector_rotation(df)
                if not sector_rotation.empty:
                    sector_rotation.to_excel(writer, sheet_name='Sector Rotation')
                
                # 4. Industry Rotation
                industry_rotation = MarketIntelligence.detect_industry_rotation(df)
                if not industry_rotation.empty:
                    industry_rotation.to_excel(writer, sheet_name='Industry Rotation')
                
                # 5. Pattern Analysis
                pattern_counts = {}
                for patterns in df['patterns'].dropna():
                    if patterns:
                        for p in patterns.split(' | '):
                            pattern_counts[p] = pattern_counts.get(p, 0) + 1
                
                if pattern_counts:
                    pattern_df = pd.DataFrame(
                        list(pattern_counts.items()),
                        columns=['Pattern', 'Count']
                    ).sort_values('Count', ascending=False)
                    pattern_df.to_excel(writer, sheet_name='Pattern Analysis', index=False)
                
                # 6. Wave Radar Signals
                wave_signals = df[
                    (df['momentum_score'] >= 60) & 
                    (df['acceleration_score'] >= 70) &
                    (df['rvol'] >= 2)
                ].head(50)
                
                if len(wave_signals) > 0:
                    wave_cols = ['ticker', 'company_name', 'master_score', 
                                'momentum_score', 'acceleration_score', 'rvol',
                                'market_state', 'patterns', 'category', 'industry']
                    available_wave_cols = [col for col in wave_cols if col in wave_signals.columns]
                    
                    wave_signals[available_wave_cols].to_excel(
                        writer, sheet_name='Wave Radar', index=False
                    )
                
                # 7. Summary Statistics
                summary_stats = {
                    'Total Stocks': len(df),
                    'Average Master Score': df['master_score'].mean() if 'master_score' in df.columns else 0,
                    'Stocks with Patterns': (df['patterns'] != '').sum() if 'patterns' in df.columns else 0,
                    'High RVOL (>2x)': (df['rvol'] > 2).sum() if 'rvol' in df.columns else 0,
                    'Positive 30D Returns': (df['ret_30d'] > 0).sum() if 'ret_30d' in df.columns else 0,
                    'Template Used': template,
                    'Export Date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                
                summary_df = pd.DataFrame(list(summary_stats.items()), columns=['Metric', 'Value'])
                summary_df.to_excel(writer, sheet_name='Summary', index=False)
                
                logger.info(f"Excel report created successfully with {len(writer.sheets)} sheets")
                
        except Exception as e:
            logger.error(f"Error creating Excel report: {str(e)}")
            raise
        
        output.seek(0)
        return output
    
    @staticmethod
    def create_csv_export(df: pd.DataFrame) -> str:
        """Create CSV export efficiently"""
        
        export_cols = [
            'rank', 'ticker', 'company_name', 'master_score',
            'position_score', 'volume_score', 'momentum_score',
            'acceleration_score', 'breakout_score', 'rvol_score',
            'trend_quality', 'price', 'pe', 'eps_current', 'eps_change_pct',
            'from_low_pct', 'from_high_pct',
            'ret_1d', 'ret_7d', 'ret_30d', 'ret_3m', 'ret_6m', 'ret_1y',
            'rvol', 'vmi', 'money_flow_mm', 'position_tension',
            'momentum_harmony', 'market_state', 'patterns', 
            'category', 'sector', 'industry', 'eps_tier', 'pe_tier', 'overall_market_strength'
        ]
        
        available_cols = [col for col in export_cols if col in df.columns]
        
        export_df = df[available_cols].copy()
        
        # Convert volume ratios back to percentage
        vol_ratio_cols = [col for col in export_df.columns if 'vol_ratio' in col]
        for col in vol_ratio_cols:
            with np.errstate(divide='ignore', invalid='ignore'):
                export_df[col] = (export_df[col] - 1) * 100
                export_df[col] = export_df[col].replace([np.inf, -np.inf], 0).fillna(0)
        
        return export_df.to_csv(index=False)

# ============================================
# UI COMPONENTS
# ============================================

class UIComponents:
    """Reusable UI components with proper tooltips"""
    
    @staticmethod
    def render_metric_card(label: str, value: Any, delta: Optional[str] = None, 
                          help_text: Optional[str] = None) -> None:
        """Render a styled metric card with tooltips"""
        # Add tooltip from CONFIG if available
        metric_key = label.lower().replace(' ', '_')
        if not help_text and metric_key in CONFIG.METRIC_TOOLTIPS:
            help_text = CONFIG.METRIC_TOOLTIPS[metric_key]
        
        if help_text:
            st.metric(label, value, delta, help=help_text)
        else:
            st.metric(label, value, delta)
    
    @staticmethod
    def render_summary_section(df: pd.DataFrame) -> None:
        """🚀 ALL TIME BEST REVOLUTIONARY SUMMARY DASHBOARD - INSTITUTIONAL GRADE INTELLIGENCE 🚀"""
        
        if df.empty:
            st.warning("No data available for summary")
            return
        
        # ================================================================================================
        # 🎯 EXECUTIVE COMMAND CENTER - REAL-TIME MARKET INTELLIGENCE
        # ================================================================================================
        
        # Market Regime Detection
        regime, regime_metrics = MarketIntelligence.detect_market_regime(df)
        
        # ================================================================================================
        # 💎 TIER 1: CRITICAL MARKET PULSE INDICATORS
        # ================================================================================================
        
        st.markdown("### 📊 Market Pulse")
        
        pulse_col1, pulse_col2, pulse_col3, pulse_col4, pulse_col5 = st.columns(5)
        
        with pulse_col1:
            # Advanced A/D Ratio with Institutional Logic
            ad_metrics = MarketIntelligence.calculate_advance_decline_ratio(df)
            ad_ratio = ad_metrics.get('ad_ratio', 1.0)
            advancing = ad_metrics.get('advancing', 0)
            declining = ad_metrics.get('declining', 0)
            
            if ad_ratio == float('inf'):
                ad_status = "🚀💥 EXPLOSIVE"
                ad_display = "∞"
                market_signal = "EXTREME BULLISH"
            elif ad_ratio > 3:
                ad_status = "⚡ POWERFUL"
                ad_display = f"{ad_ratio:.1f}"
                market_signal = "STRONG BULLISH"
            elif ad_ratio > 2:
                ad_status = "🔥 STRONG"
                ad_display = f"{ad_ratio:.1f}"
                market_signal = "BULLISH"
            elif ad_ratio > 1.5:
                ad_status = "📈 POSITIVE"
                ad_display = f"{ad_ratio:.1f}"
                market_signal = "MILD BULLISH"
            elif ad_ratio > 0.8:
                ad_status = "⚖️ NEUTRAL"
                ad_display = f"{ad_ratio:.1f}"
                market_signal = "MIXED"
            else:
                ad_status = "📉 WEAK"
                ad_display = f"{ad_ratio:.1f}"
                market_signal = "BEARISH"
            
            UIComponents.render_metric_card(
                "Market Breadth",
                f"{ad_status}",
                f"A/D: {ad_display} • {market_signal}",
                f"Advance/Decline Ratio: {advancing} advancing vs {declining} declining stocks. Critical market breadth indicator."
            )
        
        with pulse_col2:
            # Institutional Momentum Intelligence
            if 'momentum_score' in df.columns:
                momentum_stats = df['momentum_score'].describe()
                high_momentum = len(df[df['momentum_score'] >= 70])  # Lowered from 80
                elite_momentum = len(df[df['momentum_score'] >= 85])  # Lowered from 90
                momentum_pct = (high_momentum / len(df) * 100) if len(df) > 0 else 0
                
                if momentum_pct > 15:  # Lowered from 25
                    momentum_status = "🚀 EXPLOSIVE"
                    momentum_quality = "INSTITUTIONAL GRADE"
                elif momentum_pct > 8:  # Lowered from 15
                    momentum_status = "🔥 STRONG"
                    momentum_quality = "PROFESSIONAL GRADE"
                elif momentum_pct > 4:  # Lowered from 8
                    momentum_status = "📈 BUILDING"
                    momentum_quality = "DEVELOPING"
                else:
                    momentum_status = "📉 WEAK"
                    momentum_quality = "CONSOLIDATING"
                
                UIComponents.render_metric_card(
                    "Momentum Engine",
                    f"{momentum_status}",
                    f"{momentum_pct:.1f}% • {elite_momentum} Elite • {momentum_quality}",
                    f"Momentum analysis: {high_momentum} stocks with strong momentum (≥70). Avg: {momentum_stats['mean']:.1f} | Max: {momentum_stats['max']:.1f}"
                )
            else:
                UIComponents.render_metric_card(
                    "Momentum Engine", 
                    "⚠️ NO DATA",
                    "Momentum data unavailable",
                    "Momentum scoring requires momentum_score column in dataset"
                )
        
        with pulse_col3:
            # Advanced Volume Intelligence
            if 'rvol' in df.columns:
                volume_stats = df['rvol'].describe()
                avg_rvol = df['rvol'].median()
                volume_surge = len(df[df['rvol'] > 2.0])  # Lowered from 3
                extreme_volume = len(df[df['rvol'] > 3.0])  # Lowered from 5
                institutional_volume = len(df[df['rvol'] > 1.5])  # Lowered from 2
                
                if avg_rvol > 1.4:  # Lowered from 2.0
                    vol_status = "🌊🌊 TSUNAMI"
                    vol_quality = "EXTREME ACTIVITY"
                elif avg_rvol > 1.2:  # Lowered from 1.5
                    vol_status = "🌊 SURGE"
                    vol_quality = "HIGH ACTIVITY"
                elif avg_rvol > 1.05:  # Lowered from 1.2
                    vol_status = "💧 ELEVATED"
                    vol_quality = "ACTIVE"
                else:
                    vol_status = "🏜️ QUIET"
                    vol_quality = "LOW ACTIVITY"
                
                UIComponents.render_metric_card(
                    "Volume Intelligence",
                    f"{vol_status}",
                    f"{avg_rvol:.2f}x • {volume_surge} Surges • {vol_quality}",
                    f"Volume analysis: {institutional_volume} stocks with elevated volume (>1.5x). {extreme_volume} extreme spikes (>3x)."
                )
            else:
                UIComponents.render_metric_card(
                    "Volume Intelligence",
                    "⚠️ NO DATA",
                    "Volume data unavailable",
                    "Volume analysis requires rvol column in dataset"
                )
        
        with pulse_col4:
            # Professional Risk Assessment
            risk_score = 0
            risk_factors = []
            
            # Factor 1: Overextension Risk
            if 'from_high_pct' in df.columns and 'momentum_score' in df.columns:
                overextended = len(df[(df['from_high_pct'] >= -2) & (df['momentum_score'] < 50)])
                if overextended > len(df) * 0.10:  # Lowered from 0.15
                    risk_score += 15  # Lowered from 25
                    risk_factors.append(f"{overextended} overextended")
            
            # Factor 2: Pump & Dump Risk
            if 'rvol' in df.columns and 'master_score' in df.columns:
                pump_risk = len(df[(df['rvol'] > 5) & (df['master_score'] < 60)])  # Lowered from 8
                if pump_risk > 5:  # Lowered from 10
                    risk_score += 15  # Lowered from 20
                    risk_factors.append(f"{pump_risk} pump suspects")
            
            # Factor 3: Trend Deterioration
            if 'trend_quality' in df.columns:
                weak_trends = len(df[df['trend_quality'] < 40])
                if weak_trends > len(df) * 0.20:  # Lowered from 0.25
                    risk_score += 10  # Lowered from 15
                    risk_factors.append(f"{weak_trends} weak trends")
            
            # Factor 4: Pattern Quality
            if 'patterns' in df.columns:
                warning_patterns = len(df[df['patterns'].str.contains('🪤|📉|🔉|📊', na=False)])
                if warning_patterns > len(df) * 0.08:  # Lowered from 0.1
                    risk_score += 15  # Lowered from 20
                    risk_factors.append(f"{warning_patterns} warning patterns")
            
            # Factor 5: Market Regime Risk
            regime_risk = 0
            if "RISK-OFF DEFENSIVE" in regime:
                regime_risk = 25
            elif "RANGE-BOUND" in regime:
                regime_risk = 10
            elif "VOLATILE OPPORTUNITY" in regime:
                regime_risk = 15
            elif "MIXED SIGNALS" in regime:
                regime_risk = 12
            elif "RISK-ON BULL" in regime:
                regime_risk = 0
            
            risk_score += regime_risk
            
            # More realistic Risk Classification thresholds
            if risk_score >= 65:  # Lowered from 70
                risk_status = "🔴 EXTREME"
                risk_action = "DEFENSIVE MODE"
            elif risk_score >= 40:  # Lowered from 50
                risk_status = "🟠 HIGH"
                risk_action = "REDUCE EXPOSURE"
            elif risk_score >= 20:  # Lowered from 30
                risk_status = "🟡 MODERATE"
                risk_action = "SELECTIVE"
            else:
                risk_status = "🟢 LOW"
                risk_action = "OPPORTUNITY MODE"
            
            risk_detail = " • ".join(risk_factors[:2]) if risk_factors else "Minimal risks detected"
            
            UIComponents.render_metric_card(
                "Risk Assessment",
                f"{risk_status}",
                f"Score: {risk_score}/100 • {risk_action}",
                f"Professional risk analysis: {risk_detail}. Market regime: {regime}"
            )
        
        with pulse_col5:
            # Market Regime Intelligence
            regime_strength = regime_metrics.get('breadth', 0.5) * 100  # Use actual breadth as strength
            regime_confidence = min(95, max(60, abs(regime_metrics.get('category_spread', 0)) * 5 + 70))  # Confidence based on category spread
            
            if "RISK-ON BULL" in regime:
                regime_action = "AGGRESSIVE LONG"
            elif "VOLATILE OPPORTUNITY" in regime:
                regime_action = "SELECTIVE LONG"
            elif "RANGE-BOUND" in regime:
                regime_action = "RANGE TRADE"
            elif "MIXED SIGNALS" in regime:
                regime_action = "WAIT & WATCH"
            elif "RISK-OFF DEFENSIVE" in regime:
                regime_action = "DEFENSIVE"
            else:
                regime_action = "CAUTIOUS"
            
            UIComponents.render_metric_card(
                "Market Regime",
                f"{regime}",  # Just show the regime as-is (already has emoji)
                f"Strength: {regime_strength:.0f}% • {regime_action}",
                f"Institutional market regime analysis. Confidence: {regime_confidence:.0f}%. Strategic positioning: {regime_action}"
            )
        
        # ================================================================================================
        # 🎯 TIER 2: TODAY'S ELITE OPPORTUNITIES - INSTITUTIONAL GRADE
        # ================================================================================================
        
        st.markdown("---")
        st.markdown("### 🎯 Today's Best Opportunities")
        
        opp_col1, opp_col2, opp_col3, opp_col4 = st.columns(4)
        
        with opp_col1:
            # 🌋 Institutional Tsunami - Enhanced Detection
            if 'patterns' in df.columns:
                institutional_tsunami = df[df['patterns'].str.contains('🌋 INSTITUTIONAL TSUNAMI', na=False)]
                if len(institutional_tsunami) > 0:
                    top_tsunami = institutional_tsunami.nlargest(3, 'master_score')
                    
                    st.markdown("**🌋 INSTITUTIONAL TSUNAMI**")
                    for _, stock in top_tsunami.iterrows():
                        company_name = stock.get('company_name', 'N/A')[:20] + "..." if len(stock.get('company_name', '')) > 20 else stock.get('company_name', 'N/A')
                        rvol_val = stock.get('rvol', 0)
                        momentum_val = stock.get('momentum_score', 0)
                        
                        # Enhanced display with multiple metrics
                        st.write(f"🎯 **{stock['ticker']}** - {company_name}")
                        st.caption(f"Score: {stock['master_score']:.1f} | RVOL: {rvol_val:.1f}x | Mom: {momentum_val:.0f}")
                    
                    # Add tsunami strength indicator
                    avg_strength = top_tsunami['master_score'].mean()
                    tsunami_strength = "🔥🔥 MEGA" if avg_strength > 85 else "🔥 STRONG" if avg_strength > 75 else "📈 MODERATE"
                    st.info(f"**Tsunami Strength:** {tsunami_strength}")
                else:
                    st.markdown("**🌋 INSTITUTIONAL TSUNAMI**")
                    st.info("⚖️ No tsunami patterns detected")
                    st.caption("Monitor for institutional accumulation")
            else:
                st.markdown("**🌋 INSTITUTIONAL TSUNAMI**")
                st.warning("Pattern data unavailable")
        
        with opp_col2:
            # 🕰️ Information Decay Arbitrage - Advanced
            if 'patterns' in df.columns:
                info_decay = df[df['patterns'].str.contains('🕰️ INFORMATION DECAY ARBITRAGE', na=False)]
                if len(info_decay) > 0:
                    top_decay = info_decay.nlargest(3, 'master_score')
                    
                    st.markdown("**🕰️ INFO DECAY ARBITRAGE**")
                    for _, stock in top_decay.iterrows():
                        company_name = stock.get('company_name', 'N/A')[:20] + "..." if len(stock.get('company_name', '')) > 20 else stock.get('company_name', 'N/A')
                        vol_score = stock.get('volume_score', 0)
                        position = stock.get('position_score', 0)
                        
                        st.write(f"🎯 **{stock['ticker']}** - {company_name}")
                        st.caption(f"Score: {stock['master_score']:.1f} | Vol: {vol_score:.0f} | Pos: {position:.0f}")
                    
                    # Add decay efficiency indicator  
                    avg_efficiency = top_decay['master_score'].mean()
                    decay_efficiency = "⚡ OPTIMAL" if avg_efficiency > 80 else "📈 GOOD" if avg_efficiency > 70 else "⚖️ FAIR"
                    st.info(f"**Decay Efficiency:** {decay_efficiency}")
                else:
                    st.markdown("**🕰️ INFO DECAY ARBITRAGE**")
                    st.info("⚖️ No decay opportunities")
                    st.caption("Awaiting information asymmetries")
            else:
                st.markdown("**🕰️ INFO DECAY ARBITRAGE**")
                st.warning("Pattern data unavailable")
        
        with opp_col3:
            # 🎆 Earnings Surprise Leaders - Enhanced
            if 'patterns' in df.columns:
                earnings_surprise = df[df['patterns'].str.contains('🎆 EARNINGS SURPRISE LEADER', na=False)]
                if len(earnings_surprise) > 0:
                    top_earnings = earnings_surprise.nlargest(3, 'master_score')
                    
                    st.markdown("**🎆 EARNINGS ROCKETS**")
                    for _, stock in top_earnings.iterrows():
                        company_name = stock.get('company_name', 'N/A')[:20] + "..." if len(stock.get('company_name', '')) > 20 else stock.get('company_name', 'N/A')
                        eps_growth = stock.get('eps_change_pct', 0)
                        trend_quality = stock.get('trend_quality', 0)
                        
                        st.write(f"🎯 **{stock['ticker']}** - {company_name}")
                        st.caption(f"EPS: {eps_growth:.0f}% | Score: {stock['master_score']:.1f} | Trend: {trend_quality:.0f}")
                    
                    # Add earnings momentum indicator
                    avg_eps_growth = top_earnings['eps_change_pct'].mean() if 'eps_change_pct' in top_earnings.columns and len(top_earnings) > 0 else 0
                    earnings_power = "🚀 EXPLOSIVE" if avg_eps_growth > 100 else "🔥 STRONG" if avg_eps_growth > 50 else "📈 SOLID"
                    st.info(f"**Earnings Power:** {earnings_power}")
                else:
                    st.markdown("**🎆 EARNINGS ROCKETS**")
                    st.info("⚖️ No earnings surprises")
                    st.caption("Monitor upcoming earnings")
            else:
                st.markdown("**🎆 EARNINGS ROCKETS**")
                st.warning("Pattern data unavailable")
        
        with opp_col4:
            # 🐦 Phoenix Rising - Turnaround Stories
            if 'patterns' in df.columns:
                phoenix_rising = df[df['patterns'].str.contains('🐦 PHOENIX RISING', na=False)]
                if len(phoenix_rising) > 0:
                    top_phoenix = phoenix_rising.nlargest(3, 'master_score')
                    
                    st.markdown("**🐦 PHOENIX RISING**")
                    for _, stock in top_phoenix.iterrows():
                        company_name = stock.get('company_name', 'N/A')[:20] + "..." if len(stock.get('company_name', '')) > 20 else stock.get('company_name', 'N/A')
                        from_low = stock.get('from_low_pct', 0)
                        acceleration = stock.get('acceleration_score', 0)
                        
                        st.write(f"🎯 **{stock['ticker']}** - {company_name}")
                        st.caption(f"From Low: {from_low:.0f}% | Accel: {acceleration:.0f} | Score: {stock['master_score']:.1f}")
                    
                    # Add transformation strength
                    avg_score = top_phoenix['master_score'].mean()
                    transformation_power = "🔥🔥 MEGA" if avg_score > 85 else "🔥 STRONG" if avg_score > 75 else "📈 EMERGING"
                    st.info(f"**Transformation:** {transformation_power}")
                else:
                    st.markdown("**🐦 PHOENIX RISING**")
                    st.info("⚖️ No phoenix patterns")
                    st.caption("Scanning for turnarounds")
            else:
                st.markdown("**🐦 PHOENIX RISING**")
                st.warning("Pattern data unavailable")
        
        # ================================================================================================
        # ADVANCED MARKET INTELLIGENCE & SECTOR ROTATION
        # ================================================================================================
        
        st.markdown("---")
        st.markdown("### 🧠 **Market Intelligence**")
        
        intel_col1, intel_col2 = st.columns([3, 2])
        
        with intel_col1:
            # Smart Context-Aware Rotation Intelligence
            # Switches between SECTOR and INDUSTRY rotation based on filter context
            selected_sectors = st.session_state.filter_state.get('sectors', [])
            
            # Determine rotation mode based on filter context
            if len(selected_sectors) == 1:
                # Single sector selected → Focus on INDUSTRY rotation within that sector
                rotation_mode = "INDUSTRY"
                rotation_title = "🔄 INDUSTRY ROTATION INTELLIGENCE"
                sector_name = selected_sectors[0]
                rotation_subtitle = f"📍 **{sector_name} Sector** - Industry Performance Analysis"
                chart_title = f"🎯 INDUSTRY ROTATION MAP - {sector_name}"
                x_axis_title = "Industry"
                hot_label = "HOT INDUSTRIES"
                avoid_label = "AVOID INDUSTRIES"
                rotation_data = MarketIntelligence.detect_industry_rotation(df)
            else:
                # Multiple sectors or no filter → Standard SECTOR rotation
                rotation_mode = "SECTOR"
                rotation_title = "🔄 SECTOR ROTATION INTELLIGENCE"
                if len(selected_sectors) > 1:
                    rotation_subtitle = f"📍 **{len(selected_sectors)} Selected Sectors** - Cross-Sector Analysis"
                else:
                    rotation_subtitle = "📍 **Market-Wide Analysis** - All Sectors"
                chart_title = "🎯 INSTITUTIONAL SECTOR ROTATION MAP"
                x_axis_title = "Sector"
                hot_label = "HOT SECTORS"
                avoid_label = "AVOID SECTORS"
                rotation_data = MarketIntelligence.detect_sector_rotation(df)
            
            # Display dynamic title
            st.markdown(f"**{rotation_title}**")
            
            if not rotation_data.empty:
                # Enhanced visualization with institutional perspective
                fig = go.Figure()
                
                top_12 = rotation_data.head(12)  # Show top analysis items
                
                # Create color mapping based on flow score and LDI
                colors = []
                for score in top_12['flow_score']:
                    if score > 75:
                        colors.append('#00ff00')  # Bright green for hot items
                    elif score > 60:
                        colors.append('#32cd32')  # Green for strong items
                    elif score > 45:
                        colors.append('#ffd700')  # Gold for neutral
                    elif score > 30:
                        colors.append('#ff8c00')  # Orange for weak
                    else:
                        colors.append('#ff4500')  # Red for very weak
                
                fig.add_trace(go.Bar(
                    x=top_12.index,
                    y=top_12['flow_score'],
                    text=[f"{val:.1f}" for val in top_12['flow_score']],
                    textposition='outside',
                    marker_color=colors,
                    marker_line=dict(color='rgba(0,0,0,0.3)', width=1),
                    hovertemplate=(
                        '<b>%{x}</b><br>'
                        'Flow Score: %{y:.1f}<br>'
                        'LDI Score: %{customdata[0]:.1f}%<br>'
                        'Market Leaders: %{customdata[1]} of %{customdata[2]}<br>'
                        'Leadership Density: %{customdata[3]}<br>'
                        'Elite Avg Score: %{customdata[4]:.1f}<br>'
                        'Intelligence: %{customdata[5]}<extra></extra>'
                    ) if all(col in top_12.columns for col in ['ldi_score', 'elite_avg_score', 'ldi_quality']) else (
                        '<b>%{x}</b><br>'
                        'Flow Score: %{y:.1f}<br>'
                        'Stocks Analyzed: %{customdata[0]} of %{customdata[1]}<br>'
                        'Coverage: %{customdata[2]:.1f}%<br>'
                        'Avg Score: %{customdata[3]:.1f}<extra></extra>'
                    ),
                    customdata=np.column_stack((
                        top_12['ldi_score'] if 'ldi_score' in top_12.columns else [0] * len(top_12),
                        top_12['analyzed_stocks'],
                        top_12['total_stocks'],
                        top_12['leadership_density'] if 'leadership_density' in top_12.columns else ['N/A'] * len(top_12),
                        top_12['elite_avg_score'] if 'elite_avg_score' in top_12.columns else top_12['avg_score'],
                        top_12['ldi_quality'] if 'ldi_quality' in top_12.columns else ['Traditional'] * len(top_12)
                    )) if all(col in top_12.columns for col in ['ldi_score', 'elite_avg_score']) else np.column_stack((
                        top_12['analyzed_stocks'],
                        top_12['total_stocks'],
                        top_12['sampling_pct'] if 'sampling_pct' in top_12.columns else [100] * len(top_12),
                        top_12['avg_score']
                    ))
                ))
                
                # Enhanced layout with context-aware styling
                fig.update_layout(
                    title=chart_title,
                    xaxis_title=x_axis_title,
                    yaxis_title="Enhanced Flow Score",
                    height=450,
                    template='plotly_white',
                    showlegend=False,
                    plot_bgcolor='rgba(0,0,0,0)',
                    paper_bgcolor='rgba(0,0,0,0)',
                    font=dict(size=12, color='#2c3e50'),
                    title_font=dict(size=16, color='#2c3e50')
                )
                
                # Add horizontal reference lines
                fig.add_hline(y=75, line_dash="dash", line_color="green", annotation_text="Hot Zone")
                fig.add_hline(y=50, line_dash="dash", line_color="orange", annotation_text="Neutral Zone")
                fig.add_hline(y=25, line_dash="dash", line_color="red", annotation_text="Cold Zone")
                
                st.plotly_chart(fig, width="stretch", theme="streamlit")
                
                # Add context-aware insights
                hot_items = top_12[top_12['flow_score'] > 75]
                if len(hot_items) > 0:
                    st.success(f"🔥 **{hot_label}**: {', '.join(hot_items.index[:3])}")
                
                cold_items = top_12[top_12['flow_score'] < 35]
                if len(cold_items) > 0:
                    st.warning(f"❄️ **{avoid_label}**: {', '.join(cold_items.index[-2:])}")
                    
            else:
                processing_message = f"📊 {rotation_mode.title()} rotation data processing..."
                data_requirement = f"Ensure {rotation_mode.lower()} data is available in your dataset"
                st.info(processing_message)
                st.caption(data_requirement)
        
        with intel_col2:
            # Enhanced Market Regime Detection with Action Items
            regime, regime_metrics = MarketIntelligence.detect_market_regime(df)
            
            st.markdown("**🎯 MARKET REGIME ANALYSIS**")
            
            # Regime display with enhanced styling
            regime_colors = {
                'RISK-ON BULL': '🚀',
                'RISK-OFF DEFENSIVE': '🛡️',
                'VOLATILE OPPORTUNITY': '⚡',
                'RANGE-BOUND': '😴'
            }
            
            # Extract emoji and text from regime
            regime_key = regime.split(' ', 1)[-1] if ' ' in regime else regime
            # Just display the regime as-is since it already contains emojis  
            st.markdown(f"### {regime}")
            
            # Show regime metrics for debugging
            if st.checkbox("🔍 Show Regime Metrics", key="regime_debug"):
                st.json({
                    'category_spread': round(regime_metrics.get('category_spread', 0), 2),
                    'breadth': round(regime_metrics.get('breadth', 0), 3),
                    'avg_rvol': round(regime_metrics.get('avg_rvol', 0), 2),
                    'micro_small_avg': round(regime_metrics.get('micro_small_avg', 0), 2),
                    'large_mega_avg': round(regime_metrics.get('large_mega_avg', 0), 2)
                })
            
            # Detailed regime metrics
            st.markdown("**📡 INTELLIGENCE SIGNALS**")
            
            signals = []
            signal_strength = 0
            
            # Breadth Analysis
            breadth = regime_metrics.get('breadth', 0.5)
            if breadth > 0.7:
                signals.append("✅ Exceptional breadth")
                signal_strength += 3
            elif breadth > 0.6:
                signals.append("✅ Strong breadth")
                signal_strength += 2
            elif breadth > 0.4:
                signals.append("⚖️ Moderate breadth")
                signal_strength += 1
            else:
                signals.append("⚠️ Weak breadth")
            
            # Category Leadership Analysis
            category_spread = regime_metrics.get('category_spread', 0)
            if category_spread > 15:
                signals.append("🔄 Small caps LEADING")
                signal_strength += 2
            elif category_spread > 5:
                signals.append("🔄 Small caps active")
                signal_strength += 1
            elif category_spread < -15:
                signals.append("🛡️ Large caps DEFENSIVE")
            elif category_spread < -5:
                signals.append("🛡️ Large caps preferred")
                signal_strength += 1
            
            # Volume Analysis
            avg_rvol = regime_metrics.get('avg_rvol', 1.0)
            if avg_rvol > 2.0:
                signals.append("🌊🌊 MASSIVE volume")
                signal_strength += 3
            elif avg_rvol > 1.5:
                signals.append("🌊 High volume activity")
                signal_strength += 2
            elif avg_rvol > 1.2:
                signals.append("💧 Normal volume")
                signal_strength += 1
            else:
                signals.append("🏜️ Low volume")
            
            # Pattern Emergence
            if 'patterns' in df.columns:
                pattern_count = (df['patterns'] != '').sum()
                pattern_density = pattern_count / len(df) if len(df) > 0 else 0
                if pattern_density > 0.3:
                    signals.append("🎯 MANY patterns emerging")
                    signal_strength += 2
                elif pattern_density > 0.2:
                    signals.append("🎯 Patterns developing")
                    signal_strength += 1
                elif pattern_density > 0.1:
                    signals.append("⚖️ Few patterns")
                else:
                    signals.append("📉 Pattern scarcity")
            
            for signal in signals:
                st.write(signal)
            
            # Market Strength Meter
            st.markdown("**💪 MARKET STRENGTH COMPOSITE**")
            
            strength_score = min((
                (breadth * 40) +
                (min(avg_rvol, 3) / 3 * 30) +
                (max(0, category_spread) / 20 * 15) +
                ((pattern_count / len(df)) * 15 if 'patterns' in df.columns and len(df) > 0 else 0)
            ), 100)
            
            # Enhanced strength visualization
            if strength_score > 85:
                strength_meter = "🟢🟢🟢🟢🟢"
                strength_label = "EXCEPTIONAL"
            elif strength_score > 70:
                strength_meter = "🟢🟢🟢🟢⚪"
                strength_label = "STRONG"
            elif strength_score > 55:
                strength_meter = "🟢🟢🟢⚪⚪"
                strength_label = "MODERATE"
            elif strength_score > 40:
                strength_meter = "🟢🟢⚪⚪⚪"
                strength_label = "WEAK"
            else:
                strength_meter = "🟢⚪⚪⚪⚪"
                strength_label = "VERY WEAK"
            
            st.write(f"{strength_meter}")
            st.write(f"**{strength_label}** ({strength_score:.0f}/100)")
            
            # Action Items based on regime
            st.markdown("**🎯 ACTION ITEMS**")
            if "RISK-ON BULL" in regime or "VOLATILE OPPORTUNITY" in regime:
                st.info("• Focus on momentum leaders\n• Increase position sizes\n• Look for breakout patterns")
            elif "RANGE-BOUND" in regime:
                st.info("• Trade range-bound setups\n• Focus on earnings plays\n• Neutral position sizing")
            elif "RISK-OFF DEFENSIVE" in regime:
                st.error("• Defensive positioning\n• Cash preservation\n• Avoid new positions")
            else:
                st.warning("• Monitor market conditions\n• Wait for clearer signals\n• Prepare for regime shift")

        # ================================================================================================
        # PERFORMANCE ATTRIBUTION & ADVANCED METRICS
        # ================================================================================================
        
        st.markdown("---")
        st.markdown("### 🏆PERFORMANCE ATTRIBUTION ANALYSIS")
        
        perf_col1, perf_col2, perf_col3 = st.columns(3)
        
        with perf_col1:
            st.markdown("**📊 SCORE COMPONENT BREAKDOWN**")
            
            # Advanced score component analysis
            score_components = {}
            if 'momentum_score' in df.columns:
                score_components['Momentum'] = df['momentum_score'].mean()
            if 'acceleration_score' in df.columns:
                score_components['Acceleration'] = df['acceleration_score'].mean()
            if 'breakout_score' in df.columns:
                score_components['Breakout'] = df['breakout_score'].mean()
            if 'position_score' in df.columns:
                score_components['Position'] = df['position_score'].mean()
            if 'volume_score' in df.columns:
                score_components['Volume'] = df['volume_score'].mean()
            if 'rvol_score' in df.columns:
                score_components['RVOL'] = df['rvol_score'].mean()
            
            if score_components:
                # Create component performance dataframe
                component_df = pd.DataFrame(list(score_components.items()), columns=['Component', 'Score'])
                component_df['Quality'] = component_df['Score'].apply(
                    lambda x: '🔥 Elite' if x >= 75 else '📈 Strong' if x >= 65 else '⚖️ Average' if x >= 50 else '📉 Weak'
                )
                component_df = component_df.sort_values('Score', ascending=False)
                
                st.dataframe(
                    component_df,
                    width='stretch',
                    hide_index=True,
                    column_config={
                        'Component': st.column_config.TextColumn("Component", width="medium"),
                        'Score': st.column_config.ProgressColumn("Average Score", min_value=0, max_value=100, format="%.1f"),
                        'Quality': st.column_config.TextColumn("Quality", width="small")
                    }
                )
                
                # Highlight best component
                best_component = component_df.iloc[0]['Component']
                best_score = component_df.iloc[0]['Score']
                st.success(f"🏆 **Strongest Factor**: {best_component} ({best_score:.1f})")
            else:
                st.info("📊 Score breakdown unavailable")
        
        with perf_col2:
            st.markdown("**💰 RETURN ATTRIBUTION MATRIX**")
            
            # Multi-timeframe return analysis
            return_periods = {
                '1D': 'ret_1d',
                '7D': 'ret_7d', 
                '30D': 'ret_30d',
                '3M': 'ret_3m',
                '1Y': 'ret_1y'
            }
            
            return_analysis = {}
            for period, column in return_periods.items():
                if column in df.columns:
                    positive_count = len(df[df[column] > 0])
                    total_count = len(df[df[column].notna()])
                    win_rate = (positive_count / total_count * 100) if total_count > 0 else 0
                    avg_return = df[column].mean()
                    
                    return_analysis[period] = {
                        'Win Rate': win_rate,
                        'Avg Return': avg_return,
                        'Quality': '🔥' if win_rate > 60 and avg_return > 2 else '📈' if win_rate > 50 else '⚖️' if win_rate > 40 else '📉'
                    }
            
            if return_analysis:
                return_df = pd.DataFrame(return_analysis).T
                
                st.dataframe(
                    return_df,
                    width='stretch',
                    column_config={
                        'Win Rate': st.column_config.ProgressColumn("Win Rate %", min_value=0, max_value=100, format="%.1f"),
                        'Avg Return': st.column_config.NumberColumn("Avg Return %", format="%.2f%%"),
                        'Quality': st.column_config.TextColumn("Signal", width="small")
                    }
                )
                
                # Find best performing timeframe
                if len(return_df) > 0:
                    best_period = return_df['Avg Return'].idxmax()
                    best_return = return_df.loc[best_period, 'Avg Return']
                    st.success(f"🎯 **Best Timeframe**: {best_period} ({best_return:.2f}%)")
            else:
                st.info("📊 Return data unavailable")
        
        with perf_col3:
            st.markdown("**🎯 PATTERN EFFECTIVENESS**")
            
            # Advanced pattern analysis
            if 'patterns' in df.columns and 'master_score' in df.columns:
                pattern_performance = {}
                
                # Extract and analyze individual patterns
                for _, row in df.iterrows():
                    if pd.notna(row['patterns']) and row['patterns']:
                        patterns = row['patterns'].split(' | ')
                        for pattern in patterns:
                            pattern = pattern.strip()
                            if pattern:
                                if pattern not in pattern_performance:
                                    pattern_performance[pattern] = {'scores': [], 'count': 0}
                                pattern_performance[pattern]['scores'].append(row['master_score'])
                                pattern_performance[pattern]['count'] += 1
                
                # Calculate effectiveness metrics
                pattern_stats = []
                for pattern, data in pattern_performance.items():
                    if data['count'] >= 3:  # Only patterns with sufficient sample size
                        avg_score = np.mean(data['scores'])
                        pattern_stats.append({
                            'Pattern': pattern[:20] + '...' if len(pattern) > 20 else pattern,
                            'Count': data['count'],
                            'Avg Score': avg_score,
                            'Effectiveness': '🔥 Elite' if avg_score > 80 else '📈 Strong' if avg_score > 70 else '⚖️ Average'
                        })
                
                if pattern_stats:
                    pattern_df = pd.DataFrame(pattern_stats).sort_values('Avg Score', ascending=False).head(8)
                    
                    st.dataframe(
                        pattern_df,
                        width='stretch',
                        hide_index=True,
                        column_config={
                            'Pattern': st.column_config.TextColumn("Pattern", width="medium"),
                            'Count': st.column_config.NumberColumn("Count", width="small"),
                            'Avg Score': st.column_config.ProgressColumn("Avg Score", min_value=0, max_value=100, format="%.1f"),
                            'Effectiveness': st.column_config.TextColumn("Quality", width="small")
                        }
                    )
                    
                    # Highlight most effective pattern
                    best_pattern = pattern_df.iloc[0]['Pattern']
                    best_effectiveness = pattern_df.iloc[0]['Avg Score']
                    st.success(f"🏆 **Most Effective**: {best_pattern} ({best_effectiveness:.1f})")
                else:
                    st.info("📊 Insufficient pattern data")
            else:
                st.info("📊 Pattern analysis unavailable")

# ============================================
# SESSION STATE MANAGER
# ============================================

class SessionStateManager:
    """
    Unified session state manager for Streamlit.
    This class ensures all state variables are properly initialized,
    preventing runtime errors and managing filter states consistently.
    """

    @staticmethod
    def initialize():
        """
        Initializes all necessary session state variables with explicit defaults.
        This prevents KeyErrors when accessing variables for the first time.
        """
        defaults = {
            # Core Application State
            'search_query': "",
            'last_refresh': datetime.now(timezone.utc),
            'data_source': "sheet",
            'user_preferences': {
                'default_top_n': CONFIG.DEFAULT_TOP_N,
                'display_mode': 'Hybrid (Technical + Fundamentals)',
                'last_filters': {}
            },
            'active_filter_count': 0,
            'quick_filter': None,
            'quick_filter_applied': False,
            'show_debug': False,
            'performance_metrics': {},
            'data_quality': {},
            
            # Legacy filter keys (for backward compatibility)
            'display_count': CONFIG.DEFAULT_TOP_N,
            'sort_by': 'Rank',
            'export_template': 'Full Analysis (All Data)',
            'category_filter': [],
            'sector_filter': [],
            'industry_filter': [],
            'min_score': 0,
            'patterns': [],
            'trend_filter': "All Trends",
            'eps_tier_filter': [],
            'pe_tier_filter': [],
            'price_tier_filter': [],
            'eps_change_tiers': [],
            'position_tiers': [],
            'position_range': (0, 100),
            'performance_tiers': [],
            'performance_custom_range': (-100, 500),
            'min_eps_change': "",
            'min_pe': None,
            'max_pe': None,
            'require_fundamental_data': False,
            
            # Wave Radar specific filters
            'market_states_filter': [],
            'market_strength_range_slider': (0, 100),
            'long_term_strength_range_slider': (0, 100),
            'show_sensitivity_details': False,
            'show_market_regime': True,
            'wave_timeframe_select': "All Waves",
            'wave_sensitivity': "Balanced",
            
            # Sheet configuration
            'sheet_id': '',
            'gid': CONFIG.DEFAULT_GID
        }
        
        # Initialize default values
        for key, default_value in defaults.items():
            if key not in st.session_state:
                st.session_state[key] = default_value
        
        # Initialize centralized filter state (NEW)
        if 'filter_state' not in st.session_state:
            st.session_state.filter_state = {
                'categories': [],
                'sectors': [],
                'industries': [],
                'min_score': 0,
                'patterns': [],
                'trend_filter': "All Trends",
                'trend_range': (0, 100),
                'eps_tiers': [],
                'pe_tiers': [],
                'price_tiers': [],
                'eps_change_tiers': [],
                'position_tiers': [],
                'position_range': (0, 100),
                'turnover_tiers': [],
                'turnover_30d_tiers': [],
                'turnover_90d_tiers': [],
                'turnover_180d_tiers': [],
                'growth_trend_selection': "All Trends",
                'growth_trend_custom_range': (0.0, 100.0),
                'growth_trends': [],
                'growth_quality_tiers': [],
                'smart_money_flows': [],
                'vqs_grades': [],  # VQS Grade filter
                'advanced_ver_grades': [],  # Advanced VER Grade filter
                'momentum_selection': "All Momentum",
                'momentum_custom_range': (-100.0, 100.0),
                'momentum_range': (-100.0, 100.0),
                'consistency_selection': "All Consistency",
                'consistency_custom_range': (0.0, 100.0),
                'consistency_range': (0.0, 100.0),
                'performance_tiers': [],
                'performance_custom_range': (-100, 500),
                'volume_tiers': [],
                'rvol_range': (0.1, 20.0),
                'vmi_tiers': [],
                'custom_vmi_range': (0.5, 3.0),
                'momentum_harmony_tiers': [],
                'ret_1d_range': (2.0, 25.0),
                'ret_3d_range': (3.0, 50.0),
                'ret_7d_range': (5.0, 75.0),
                'ret_30d_range': (10.0, 150.0),
                'ret_3m_range': (15.0, 200.0),
                'ret_6m_range': (20.0, 500.0),
                'ret_1y_range': (25.0, 1000.0),
                'ret_3y_range': (50.0, 2000.0),
                'ret_5y_range': (75.0, 5000.0),
                'min_eps_change': None,
                'min_pe': None,
                'max_pe': None,
                'require_fundamental_data': False,
                'market_states': [],
                'market_strength_range': (0, 100),
                'long_term_strength_range': (0, 100),
                'position_score_range': (0, 100),
                'volume_score_range': (0, 100),
                'momentum_score_range': (0, 100),
                'acceleration_score_range': (0, 100),
                'breakout_score_range': (0, 100),
                'rvol_score_range': (0, 100),
                'position_score_selection': "All Scores",
                'volume_score_selection': "All Scores",
                'momentum_score_selection': "All Scores",
                'acceleration_score_selection': "All Scores",
                'breakout_score_selection': "All Scores",
                'rvol_score_selection': "All Scores",
                'ret_1d_selection': "All Returns",
                'ret_3d_selection': "All Returns",
                'ret_7d_selection': "All Returns",
                'ret_30d_selection': "All Returns",
                'ret_3m_selection': "All Returns",
                'ret_6m_selection': "All Returns",
                'ret_1y_selection': "All Returns",
                'ret_3y_selection': "All Returns",
                'ret_5y_selection': "All Returns",
                'quick_filter': None,
                'quick_filter_applied': False,
                # Two-Stage Pattern Filtering System
                'exclude_patterns': [],
                'include_patterns': [],
                'combination_patterns': []
            }
        
        # CRITICAL FIX: Clean up any corrupted performance_tiers values
        # This prevents the "default value not in options" error
        valid_performance_options = [
            "🚀 Strong Gainers (>3% 1D)",
            "⚡ Power Moves (>7% 1D)",
            "💥 Explosive (>15% 1D)",
            "🌟 3-Day Surge (>6% 3D)",
            "📈 Weekly Winners (>12% 7D)",
            "🏆 Monthly Champions (>25% 30D)",
            "🎯 Quarterly Stars (>40% 3M)",
            "💎 Half-Year Heroes (>60% 6M)",
            "🌙 Annual Winners (>80% 1Y)",
            "👑 Multi-Year Champions (>150% 3Y)",
            "🏛️ Long-Term Legends (>250% 5Y)",
            "🎯 Custom Range"
        ]
        
        # Clean up performance_tiers in both filter_state and legacy state
        for state_key in ['filter_state', 'performance_tiers']:
            if state_key == 'filter_state' and 'filter_state' in st.session_state:
                if 'performance_tiers' in st.session_state.filter_state:
                    current_tiers = st.session_state.filter_state['performance_tiers']
                    if isinstance(current_tiers, list):
                        cleaned_tiers = [tier for tier in current_tiers if tier in valid_performance_options]
                        st.session_state.filter_state['performance_tiers'] = cleaned_tiers
            elif state_key == 'performance_tiers' and 'performance_tiers' in st.session_state:
                current_tiers = st.session_state['performance_tiers']
                if isinstance(current_tiers, list):
                    cleaned_tiers = [tier for tier in current_tiers if tier in valid_performance_options]
                    st.session_state['performance_tiers'] = cleaned_tiers

    @staticmethod
    def build_filter_dict() -> Dict[str, Any]:
        """
        Builds a comprehensive filter dictionary from the current session state.
        This centralizes filter data for easy consumption by the FilterEngine.
        
        Returns:
            Dict[str, Any]: A dictionary of all active filter settings.
        """
        filters = {}
        
        # Use centralized filter state if available
        if 'filter_state' in st.session_state:
            state = st.session_state.filter_state
            
            # Map centralized state to filter dict
            if state.get('categories'):
                filters['categories'] = state['categories']
            if state.get('sectors'):
                filters['sectors'] = state['sectors']
            if state.get('industries'):
                filters['industries'] = state['industries']
            if state.get('min_score', 0) > 0:
                filters['min_score'] = state['min_score']
            if state.get('patterns'):
                filters['patterns'] = state['patterns']
            if state.get('trend_filter') != "All Trends":
                filters['trend_filter'] = state['trend_filter']
                filters['trend_range'] = state.get('trend_range', (0, 100))
            if state.get('eps_tiers'):
                filters['eps_tiers'] = state['eps_tiers']
            if state.get('pe_tiers'):
                filters['pe_tiers'] = state['pe_tiers']
            if state.get('price_tiers'):
                filters['price_tiers'] = state['price_tiers']
            if state.get('eps_change_tiers'):
                filters['eps_change_tiers'] = state['eps_change_tiers']
            if state.get('min_pe') is not None:
                filters['min_pe'] = state['min_pe']
            if state.get('max_pe') is not None:
                filters['max_pe'] = state['max_pe']
            if state.get('require_fundamental_data'):
                filters['require_fundamental_data'] = True
            if state.get('market_states'):
                filters['market_states'] = state['market_states']
            if state.get('market_strength_range') != (0, 100):
                filters['market_strength_range'] = state['market_strength_range']
            if state.get('long_term_strength_range') != (0, 100):
                filters['long_term_strength_range'] = state['long_term_strength_range']
            if state.get('performance_tiers'):
                filters['performance_tiers'] = state['performance_tiers']
            if state.get('position_tiers'):
                filters['position_tiers'] = state['position_tiers']
            if state.get('turnover_tiers'):
                filters['turnover_tiers'] = state['turnover_tiers']
            if state.get('volume_tiers'):
                filters['volume_tiers'] = state['volume_tiers']
            if state.get('vmi_tiers'):
                filters['vmi_tiers'] = state['vmi_tiers']
            if state.get('momentum_harmony_tiers'):
                filters['momentum_harmony_tiers'] = state['momentum_harmony_tiers']
            if state.get('custom_vmi_range') != (0.5, 3.0):
                filters['custom_vmi_range'] = state['custom_vmi_range']
            if state.get('position_score_range') != (0, 100):
                filters['position_score_range'] = state['position_score_range']
            if state.get('volume_score_range') != (0, 100):
                filters['volume_score_range'] = state['volume_score_range']
            if state.get('momentum_score_range') != (0, 100):
                filters['momentum_score_range'] = state['momentum_score_range']
            if state.get('acceleration_score_range') != (0, 100):
                filters['acceleration_score_range'] = state['acceleration_score_range']
            if state.get('breakout_score_range') != (0, 100):
                filters['breakout_score_range'] = state['breakout_score_range']
            if state.get('rvol_score_range') != (0, 100):
                filters['rvol_score_range'] = state['rvol_score_range']
            
            # Individual Performance Return Period Filters
            # Define default ranges to compare against
            default_ranges = {
                'ret_1d_range': (2.0, 25.0),
                'ret_3d_range': (3.0, 50.0),
                'ret_7d_range': (5.0, 75.0),
                'ret_30d_range': (10.0, 150.0),
                'ret_3m_range': (15.0, 200.0),
                'ret_6m_range': (20.0, 500.0),
                'ret_1y_range': (25.0, 1000.0),
                'ret_3y_range': (50.0, 2000.0),
                'ret_5y_range': (75.0, 5000.0)
            }
            
            # Add individual return period filters if they differ from defaults
            for ret_col in ['ret_1d', 'ret_3d', 'ret_7d', 'ret_30d', 'ret_3m', 'ret_6m', 'ret_1y', 'ret_3y', 'ret_5y']:
                range_key = f'{ret_col}_range'
                if state.get(range_key) and state[range_key] != default_ranges.get(range_key):
                    filters[range_key] = state[range_key]
                
        else:
            # Fallback to legacy individual keys
            # Categorical filters
            for key, filter_name in [
                ('category_filter', 'categories'), 
                ('sector_filter', 'sectors'), 
                ('industry_filter', 'industries')
            ]:
                if st.session_state.get(key) and st.session_state[key]:
                    filters[filter_name] = st.session_state[key]
            
            # Numeric filters
            if st.session_state.get('min_score', 0) > 0:
                filters['min_score'] = st.session_state['min_score']
            
            # PE filters
            if st.session_state.get('min_pe'):
                value = st.session_state['min_pe']
                if isinstance(value, str) and value.strip():
                    try:
                        filters['min_pe'] = float(value)
                    except ValueError:
                        pass
                elif isinstance(value, (int, float)):
                    filters['min_pe'] = float(value)
            
            if st.session_state.get('max_pe'):
                value = st.session_state['max_pe']
                if isinstance(value, str) and value.strip():
                    try:
                        filters['max_pe'] = float(value)
                    except ValueError:
                        pass
                elif isinstance(value, (int, float)):
                    filters['max_pe'] = float(value)

            # Multi-select filters
            if st.session_state.get('patterns') and st.session_state['patterns']:
                filters['patterns'] = st.session_state['patterns']
            
            # Tier filters
            for key, filter_name in [
                ('eps_tier_filter', 'eps_tiers'),
                ('pe_tier_filter', 'pe_tiers'),
                ('price_tier_filter', 'price_tiers'),
                ('eps_change_tier_filter', 'eps_change_tiers')
            ]:
                if st.session_state.get(key) and st.session_state[key]:
                    filters[filter_name] = st.session_state[key]
            
            # Trend filter
            if st.session_state.get('trend_filter') != "All Trends":
                trend_options = {
                    "🔥 Exceptional (85+)": (85, 100),
                    "🚀 Strong (70-84)": (70, 84),
                    "✅ Good (55-69)": (55, 69),
                    "➡️ Neutral (40-54)": (40, 54),
                    "⚠️ Weak (25-39)": (25, 39),
                    "🔻 Poor (<25)": (0, 24)
                }
                filters['trend_filter'] = st.session_state['trend_filter']
                filters['trend_range'] = trend_options.get(st.session_state['trend_filter'], (0, 100))
            
            # Market filters
            if st.session_state.get('market_strength_range_slider') != (0, 100):
                filters['market_strength_range'] = st.session_state['market_strength_range_slider']
            
            if st.session_state.get('long_term_strength_range_slider') != (0, 100):
                filters['long_term_strength_range'] = st.session_state['long_term_strength_range_slider']
            
            if st.session_state.get('market_states_filter') and st.session_state['market_states_filter']:
                filters['market_states'] = st.session_state['market_states_filter']
            
            # Checkbox filters
            if st.session_state.get('require_fundamental_data', False):
                filters['require_fundamental_data'] = True
            
        return filters

    @staticmethod
    def clear_filters():
        """
        Resets all filter-related session state keys to their default values.
        This provides a clean slate for the user.
        FIXED: Now properly cleans ALL dynamic widget keys.
        """
        # Clear the centralized filter state
        if 'filter_state' in st.session_state:
            st.session_state.filter_state = {
                'categories': [],
                'sectors': [],
                'industries': [],
                'min_score': 0,
                'patterns': [],
                'trend_filter': "All Trends",
                'trend_range': (0, 100),
                'trend_custom_range': (0, 100),
                'eps_tiers': [],
                'pe_tiers': [],
                'price_tiers': [],
                'eps_change_tiers': [],
                'position_tiers': [],
                'position_range': (0, 100),
                'turnover_tiers': [],
                'turnover_30d_tiers': [],
                'turnover_90d_tiers': [],
                'turnover_180d_tiers': [],
                'growth_trend_selection': "All Trends",
                'growth_trend_custom_range': (0.0, 100.0),
                'growth_trends': [],
                'growth_quality_tiers': [],
                'smart_money_flows': [],
                'vqs_grades': [],  # VQS Grade filter
                'advanced_ver_grades': [],  # Advanced VER Grade filter
                'momentum_selection': "All Momentum",
                'momentum_custom_range': (-100.0, 100.0),
                'momentum_range': (-100.0, 100.0),
                'consistency_selection': "All Consistency",
                'consistency_custom_range': (0.0, 100.0),
                'consistency_range': (0.0, 100.0),
                # Timeframe-Specific Growth Filters
                'growth_daily_30_selection': "All Growth",
                'growth_daily_30_range': None,
                'growth_daily_30_custom_range': (-100.0, 200.0),
                'growth_30_90_selection': "All Growth",
                'growth_30_90_range': None,
                'growth_30_90_custom_range': (-100.0, 200.0),
                'growth_90_180_selection': "All Growth",
                'growth_90_180_range': None,
                'growth_90_180_custom_range': (-100.0, 200.0),
                'performance_tiers': [],
                'performance_custom_range': (-100, 500),
                'volume_tiers': [],
                'rvol_range': (0.1, 20.0),
                'min_eps_change': None,
                'min_pe': None,
                'max_pe': None,
                'require_fundamental_data': False,
                'market_states': [],
                'market_strength_range': (0, 100),
                'long_term_strength_range': (0, 100),
                'position_score_range': (0, 100),
                'volume_score_range': (0, 100),
                'momentum_score_range': (0, 100),
                'acceleration_score_range': (0, 100),
                'breakout_score_range': (0, 100),
                'rvol_score_range': (0, 100),
                'position_score_selection': "All Scores",
                'volume_score_selection': "All Scores",
                'momentum_score_selection': "All Scores",
                'acceleration_score_selection': "All Scores",
                'breakout_score_selection': "All Scores",
                'rvol_score_selection': "All Scores",
                # Performance filter selections
                'ret_1d_selection': "All Returns",
                'ret_3d_selection': "All Returns", 
                'ret_7d_selection': "All Returns",
                'ret_30d_selection': "All Returns",
                'ret_3m_selection': "All Returns",
                'ret_6m_selection': "All Returns",
                'ret_1y_selection': "All Returns",
                'ret_3y_selection': "All Returns",
                'ret_5y_selection': "All Returns",
                # Two-Stage Pattern Filtering System
                'exclude_patterns': [],
                'include_patterns': [],
                'combination_patterns': [],
                # Performance filter ranges
                'ret_1d_range': (2.0, 25.0),
                'ret_3d_range': (3.0, 50.0),
                'ret_7d_range': (5.0, 75.0),
                'ret_30d_range': (10.0, 150.0),
                'ret_3m_range': (15.0, 200.0),
                'ret_6m_range': (20.0, 500.0),
                'ret_1y_range': (25.0, 1000.0),
                'ret_3y_range': (50.0, 2000.0),
                'ret_5y_range': (75.0, 5000.0),
                'quick_filter': None,
                'quick_filter_applied': False
            }
        
        # Clear individual legacy filter keys
        filter_keys = [
            'category_filter', 'sector_filter', 'industry_filter', 'eps_tier_filter',
            'pe_tier_filter', 'price_tier_filter', 'eps_change_tier_filter', 'patterns', 'min_score', 'trend_filter',
            'min_pe', 'max_pe', 'require_fundamental_data',
            'quick_filter', 'quick_filter_applied', 'market_states_filter',
            'market_strength_range_slider', 'long_term_strength_range_slider', 'show_sensitivity_details', 'show_market_regime',
            'wave_timeframe_select', 'wave_sensitivity'
        ]
        
        for key in filter_keys:
            if key in st.session_state:
                if isinstance(st.session_state[key], list):
                    st.session_state[key] = []
                elif isinstance(st.session_state[key], bool):
                    st.session_state[key] = False
                elif isinstance(st.session_state[key], str):
                    if key == 'trend_filter':
                        st.session_state[key] = "All Trends"
                    elif key == 'wave_timeframe_select':
                        st.session_state[key] = "All Waves"
                    elif key == 'wave_sensitivity':
                        st.session_state[key] = "Balanced"
                    else:
                        st.session_state[key] = ""
                elif isinstance(st.session_state[key], tuple):
                    if key == 'market_strength_range_slider':
                        st.session_state[key] = (0, 100)
                elif isinstance(st.session_state[key], (int, float)):
                    if key == 'min_score':
                        st.session_state[key] = 0
                    else:
                        st.session_state[key] = None if key in ['min_pe', 'max_pe'] else 0
                else:
                    st.session_state[key] = None
        
        # CRITICAL FIX: Delete all widget keys to force UI reset
        widget_keys_to_delete = [
            # Multiselect widgets
            'category_multiselect', 'sector_multiselect', 'industry_multiselect',
            'patterns_multiselect', 'market_states_multiselect',
            'eps_tier_multiselect', 'pe_tier_multiselect', 'price_tier_multiselect',
            'eps_change_tiers_widget', 'performance_tier_multiselect', 'position_tier_multiselect',
            'volume_tier_multiselect',
            'performance_tier_multiselect_intelligence', 'volume_tier_multiselect_intelligence',
            'position_tier_multiselect_intelligence', 'turnover_tier_multiselect_dedicated',
            'turnover_30d_tier_multiselect_dedicated', 'turnover_90d_tier_multiselect_dedicated', 'turnover_180d_tier_multiselect_dedicated',
            
            # Slider widgets
            'min_score_slider', 'market_strength_slider', 'performance_custom_range_slider',
            'trend_custom_range_slider',
            'ret_1d_range_slider', 'ret_3d_range_slider', 'ret_7d_range_slider', 'ret_30d_range_slider',
            'ret_3m_range_slider', 'ret_6m_range_slider', 'ret_1y_range_slider', 'ret_3y_range_slider', 'ret_5y_range_slider',
            'position_range_slider', 'rvol_range_slider',
            'position_score_slider', 'volume_score_slider', 'momentum_score_slider',
            'acceleration_score_slider', 'breakout_score_slider', 'rvol_score_slider',
            
            # Score dropdown widgets
            'position_score_dropdown', 'volume_score_dropdown', 'momentum_score_dropdown',
            'acceleration_score_dropdown', 'breakout_score_dropdown', 'rvol_score_dropdown',
            
            # Selectbox widgets
            'trend_selectbox', 'wave_timeframe_select', 'display_mode_toggle',
            
            # Text input widgets
            'eps_change_input', 'min_pe_input', 'max_pe_input',
            
            # Checkbox widgets
            'require_fundamental_checkbox', 'show_sensitivity_details', 'show_market_regime',
            
            # Additional keys
            'display_count_select', 'sort_by_select', 'export_template_radio',
            'wave_sensitivity', 'search_input', 'sheet_input', 'gid_input'
        ]
        
        # Delete each widget key if it exists
        deleted_count = 0
        for key in widget_keys_to_delete:
            if key in st.session_state:
                del st.session_state[key]
                deleted_count += 1
        
        # ==== MEMORY LEAK FIX - START ====
        # Clean up ANY dynamically created widget keys that weren't in the predefined list
        # This catches widgets created on the fly or with dynamic keys
        
        all_widget_patterns = [
            '_multiselect', '_slider', '_selectbox', '_checkbox', 
            '_input', '_radio', '_button', '_expander', '_toggle',
            '_number_input', '_text_area', '_date_input', '_time_input',
            '_color_picker', '_file_uploader', '_camera_input'
        ]
        
        # Collect keys to delete (can't modify dict during iteration)
        dynamic_keys_to_delete = []
        
        for key in list(st.session_state.keys()):
            # Check if this key ends with any widget pattern
            for pattern in all_widget_patterns:
                if pattern in key and key not in widget_keys_to_delete:
                    dynamic_keys_to_delete.append(key)
                    break
        
        # Delete the dynamic keys
        for key in dynamic_keys_to_delete:
            try:
                del st.session_state[key]
                deleted_count += 1
                logger.debug(f"Deleted dynamic widget key: {key}")
            except KeyError:
                # Key might have been deleted already
                pass
        
        # Also clean up any keys that start with 'FormSubmitter'
        form_keys_to_delete = [key for key in st.session_state.keys() if key.startswith('FormSubmitter')]
        for key in form_keys_to_delete:
            try:
                del st.session_state[key]
                deleted_count += 1
            except KeyError:
                pass
        
        # ==== MEMORY LEAK FIX - END ====
        
        # Also clear legacy filter keys for backward compatibility
        legacy_keys = [
            'category_filter', 'sector_filter', 'industry_filter',
            'min_score', 'patterns', 'trend_filter',
            'eps_tier_filter', 'pe_tier_filter', 'price_tier_filter',
            'min_eps_change', 'min_pe', 'max_pe',
            'require_fundamental_data', 'market_states_filter',
            'market_strength_range_slider'
        ]
        
        for key in legacy_keys:
            if key in st.session_state:
                if isinstance(st.session_state[key], list):
                    st.session_state[key] = []
                elif isinstance(st.session_state[key], bool):
                    st.session_state[key] = False
                elif isinstance(st.session_state[key], str):
                    if key == 'trend_filter':
                        st.session_state[key] = "All Trends"
                    else:
                        st.session_state[key] = ""
                elif isinstance(st.session_state[key], tuple):
                    if key == 'market_strength_range_slider':
                        st.session_state[key] = (0, 100)
                elif isinstance(st.session_state[key], (int, float)):
                    if key == 'min_score':
                        st.session_state[key] = 0
                    else:
                        st.session_state[key] = None
                else:
                    st.session_state[key] = None
        
        # Reset active filter count
        st.session_state.active_filter_count = 0
        
        # Clear quick filter
        st.session_state.quick_filter = None
        st.session_state.quick_filter_applied = False
        
        # Clear any cached filter results
        if 'user_preferences' in st.session_state:
            st.session_state.user_preferences['last_filters'] = {}
        
        logger.info(f"All filters and widget states cleared successfully. Deleted {deleted_count} widget keys.")
    
    @staticmethod
    def sync_filter_states():
        """
        Synchronizes legacy individual filter keys with centralized filter state.
        This ensures backward compatibility during transition.
        """
        if 'filter_state' not in st.session_state:
            return
        
        state = st.session_state.filter_state
        
        # Sync from centralized to individual (for widgets that still use old keys)
        mappings = [
            ('categories', 'category_filter'),
            ('sectors', 'sector_filter'),
            ('industries', 'industry_filter'),
            ('min_score', 'min_score'),
            ('patterns', 'patterns'),
            ('trend_filter', 'trend_filter'),
            ('eps_tiers', 'eps_tier_filter'),
            ('pe_tiers', 'pe_tier_filter'),
            ('price_tiers', 'price_tier_filter'),
            ('eps_change_tiers', 'eps_change_tier_filter'),
            ('min_pe', 'min_pe'),
            ('max_pe', 'max_pe'),
            ('require_fundamental_data', 'require_fundamental_data'),
            ('market_states', 'market_states_filter'),
            ('market_strength_range', 'market_strength_range_slider'),
        ]
        
        for state_key, session_key in mappings:
            if state_key in state:
                st.session_state[session_key] = state[state_key]
    
    @staticmethod
    def get_active_filter_count() -> int:
        """
        Counts the number of active filters.
        
        Returns:
            int: Number of active filters.
        """
        count = 0
        
        if 'filter_state' in st.session_state:
            state = st.session_state.filter_state
            
            if state.get('categories'): count += 1
            if state.get('sectors'): count += 1
            if state.get('industries'): count += 1
            if state.get('min_score', 0) > 0: count += 1
            if state.get('patterns'): count += 1
            if state.get('trend_filter') != "All Trends": count += 1
            if state.get('eps_tiers'): count += 1
            if state.get('pe_tiers'): count += 1
            if state.get('price_tiers'): count += 1
            if state.get('eps_change_tiers'): count += 1
            if state.get('min_pe') is not None: count += 1
            if state.get('max_pe') is not None: count += 1
            if state.get('require_fundamental_data'): count += 1
            if state.get('market_states'): count += 1
            if state.get('market_strength_range') != (0, 100): count += 1
            if state.get('long_term_strength_range') != (0, 100): count += 1
        else:
            # Fallback to old method
            filter_checks = [
                ('category_filter', lambda x: x and len(x) > 0),
                ('sector_filter', lambda x: x and len(x) > 0),
                ('industry_filter', lambda x: x and len(x) > 0),
                ('min_score', lambda x: x > 0),
                ('patterns', lambda x: x and len(x) > 0),
                ('trend_filter', lambda x: x != 'All Trends'),
                ('eps_tier_filter', lambda x: x and len(x) > 0),
                ('pe_tier_filter', lambda x: x and len(x) > 0),
                ('price_tier_filter', lambda x: x and len(x) > 0),
                ('eps_change_tier_filter', lambda x: x and len(x) > 0),
                ('min_pe', lambda x: x is not None and str(x).strip() != ''),
                ('max_pe', lambda x: x is not None and str(x).strip() != ''),
                ('require_fundamental_data', lambda x: x),
                ('market_states_filter', lambda x: x and len(x) > 0),
                ('market_strength_range_slider', lambda x: x != (0, 100)),
                ('long_term_strength_range_slider', lambda x: x != (0, 100))
            ]
            
            for key, check_func in filter_checks:
                value = st.session_state.get(key)
                if value is not None and check_func(value):
                    count += 1
        
        return count
    
    @staticmethod
    def safe_get(key: str, default: Any = None) -> Any:
        """
        Safely get a session state value with fallback.
        
        Args:
            key (str): The session state key.
            default (Any): Default value if key doesn't exist.
            
        Returns:
            Any: The value from session state or default.
        """
        if key not in st.session_state:
            st.session_state[key] = default
        return st.session_state[key]
    
    @staticmethod
    def safe_set(key: str, value: Any) -> None:
        """
        Safely set a session state value.
        
        Args:
            key (str): The session state key.
            value (Any): The value to set.
        """
        st.session_state[key] = value
    
    @staticmethod
    def reset_quick_filters():
        """Reset quick filter states"""
        st.session_state.quick_filter = None
        st.session_state.quick_filter_applied = False
        
        if 'filter_state' in st.session_state:
            st.session_state.filter_state['quick_filter'] = None
            st.session_state.filter_state['quick_filter_applied'] = False
        
# ============================================
# MARKET STATE FILTER VALIDATION
# ============================================

def validate_market_state_filters():
    """
    Quick validation test for all market state filter presets.
    Ensures configuration is properly set up and filters work correctly.
    """
    try:
        logger.info("="*60)
        logger.info("VALIDATING MARKET STATE FILTERS")
        logger.info("="*60)
        
        # Check configuration exists
        if not hasattr(CONFIG, 'MARKET_STATE_FILTERS'):
            logger.error("MARKET_STATE_FILTERS not found in CONFIG")
            return False
        
        # Check default filter
        default_filter = getattr(CONFIG, 'DEFAULT_MARKET_FILTER', None)
        if not default_filter:
            logger.error("DEFAULT_MARKET_FILTER not set in CONFIG")
            return False
        
        # Test each filter preset
        filters = CONFIG.MARKET_STATE_FILTERS
        all_states = ['STRONG_UPTREND', 'UPTREND', 'PULLBACK', 'ROTATION', 
                     'SIDEWAYS', 'DOWNTREND', 'STRONG_DOWNTREND', 'BOUNCE']
        
        for filter_name, filter_config in filters.items():
            logger.info(f"Testing {filter_name} filter...")
            
            # Check allowed_states exists and is valid
            allowed_states = filter_config.get('allowed_states', [])
            if not allowed_states:
                logger.error(f"  ✗ {filter_name}: No allowed_states defined")
                return False
            
            # Check all states are valid
            invalid_states = [s for s in allowed_states if s not in all_states]
            if invalid_states:
                logger.error(f"  ✗ {filter_name}: Invalid states: {invalid_states}")
                return False
            
            # Check description exists
            description = filter_config.get('description', '')
            if not description:
                logger.warning(f"  ⚠ {filter_name}: No description provided")
            
            # Test strategy focus
            state_count = len(allowed_states)
            if filter_name == 'MOMENTUM' and state_count < 2:
                logger.warning(f"  ⚠ {filter_name}: May be too restrictive ({state_count} states)")
            elif filter_name == 'ALL' and state_count != len(all_states):
                logger.warning(f"  ⚠ {filter_name}: Should include all states ({state_count}/{len(all_states)})")
            
            logger.info(f"  ✓ {filter_name}: {state_count} allowed states - {', '.join(allowed_states)}")
        
        # Test default filter exists
        if default_filter not in filters:
            logger.error(f"DEFAULT_MARKET_FILTER '{default_filter}' not found in MARKET_STATE_FILTERS")
            return False
        
        logger.info(f"✓ Default filter: {default_filter}")
        logger.info(f"✓ All {len(filters)} filter presets validated successfully")
        logger.info("="*60)
        
        return True
        
    except Exception as e:
        logger.error(f"Market state filter validation failed: {e}")
        return False

# ============================================
# MAIN APPLICATION
# ============================================

def main():
    """Main Streamlit application - Final Perfected Production Version"""
    
    # Page configuration
    st.set_page_config(
        page_title="Wave Detection 3.0",
        page_icon="🌊",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # Initialize robust session state
    SessionStateManager.initialize()
    
    # Custom CSS for production UI
    st.markdown("""
    <style>
    /* Production-ready CSS */
    .main {padding: 0rem 1rem;}
    .stTabs [data-baseweb="tab-list"] {gap: 8px;}
    .stTabs [data-baseweb="tab"] {
        height: 50px;
        padding-left: 20px;
        padding-right: 20px;
    }
    div[data-testid="metric-container"] {
        background-color: rgba(28, 131, 225, 0.1);
        border: 1px solid rgba(28, 131, 225, 0.2);
        padding: 5% 5% 5% 10%;
        border-radius: 5px;
        overflow-wrap: break-word;
    }
    .stAlert {
        padding: 1rem;
        border-radius: 5px;
    }
    /* Button styling */
    div.stButton > button {
        width: 100%;
        transition: all 0.3s ease;
    }
    div.stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 5px 10px rgba(0,0,0,0.2);
    }
    /* Mobile responsive */
    @media (max-width: 768px) {
        .stDataFrame {font-size: 12px;}
        div[data-testid="metric-container"] {padding: 3%;}
        .main {padding: 0rem 0.5rem;}
    }
    /* Table optimization */
    .stDataFrame > div {overflow-x: auto;}
    /* Loading animation */
    .stSpinner > div {
        border-color: #3498db;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # Header
    st.markdown("""
    <div style="
        text-align: center;
        padding: 2rem 0;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border-radius: 10px;
        margin-bottom: 2rem;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    ">
        <h1 style="margin: 0; font-size: 2.5rem;">🌊 Wave Detection 3.0</h1>
        <p style="margin: 0.5rem 0 0 0; opacity: 0.9;">
            Professional Stock Ranking System
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    # Sidebar configuration
    with st.sidebar:
        st.markdown("### 🎯 Quick Actions")
        
        # Control buttons
        col1, col2 = st.columns(2)
        with col1:
            if st.button("🔄 Refresh Data", type="primary", width="stretch"):
                st.cache_data.clear()
                st.session_state.last_refresh = datetime.now(timezone.utc)
                st.rerun()
        
        with col2:
            if st.button("🧹 Clear Cache", width="stretch"):
                st.cache_data.clear()
                gc.collect()  # Force garbage collection
                st.success("Cache cleared!")
                time.sleep(0.5)
                st.rerun()
        
        # Data source selection
        st.markdown("---")
        st.markdown("### 📂 Data Source")
        
        data_source_col1, data_source_col2 = st.columns(2)
        
        with data_source_col1:
            if st.button("📊 Google Sheets", 
                        type="primary" if st.session_state.data_source == "sheet" else "secondary", 
                        width="stretch"):
                st.session_state.data_source = "sheet"
                st.rerun()
        
        with data_source_col2:
            if st.button("📁 Upload CSV", 
                        type="primary" if st.session_state.data_source == "upload" else "secondary", 
                        width="stretch"):
                st.session_state.data_source = "upload"
                st.rerun()

        uploaded_file = None
        sheet_id = None
        gid = None
        
        if st.session_state.data_source == "upload":
            uploaded_file = st.file_uploader(
                "Choose CSV file", 
                type="csv",
                help="Upload a CSV file with stock data. Must contain 'ticker' and 'price' columns."
            )
            if uploaded_file is None:
                st.info("Please upload a CSV file to continue")
        else:
            # Google Sheets input
            st.markdown("#### 📊 Google Sheets Configuration")
            
            sheet_input = st.text_input(
                "Google Sheets ID/URL or Google Drive CSV File URL",
                value=st.session_state.get('sheet_id', ''),
                placeholder="Enter Sheet ID, Google Sheets URL, or Google Drive CSV file URL",
                help="Supports:\n• Google Sheets: 1OEQ_qxL4lzlO9LlKnDGlDku2yQC1iYvOYeXF0mTQlJM\n• Google Drive CSV: https://drive.google.com/file/d/ID/view"
            )
            
            # Always re-detect the URL type from current input (don't rely on session state)
            sheet_id = None
            url_source_type = "sheets"  # Default
            
            if sheet_input:
                sheet_id, url_source_type = extract_spreadsheet_id(sheet_input)
                # Store in session state for persistence
                st.session_state.sheet_id = sheet_id
                st.session_state.url_source_type = url_source_type
            else:
                # If no input, get from session state as fallback
                sheet_id = st.session_state.get('sheet_id', '')
                url_source_type = st.session_state.get('url_source_type', 'sheets')
            
            # Show different GID input based on source type
            if url_source_type == "drive":
                st.success("📁 Google Drive CSV file detected - GID not needed")
                gid = CONFIG.DEFAULT_GID
            elif url_source_type == "sheets":
                gid_input = st.text_input(
                    "Sheet Tab GID (Optional)",
                    value=st.session_state.get('gid', CONFIG.DEFAULT_GID),
                    placeholder=f"Default: {CONFIG.DEFAULT_GID}",
                    help="The GID identifies specific sheet tab. Found in URL after #gid="
                )
                
                if gid_input:
                    gid = gid_input.strip()
                else:
                    gid = CONFIG.DEFAULT_GID
            else:
                st.warning(f"⚠️ Unknown URL type detected: {url_source_type}")
                gid = CONFIG.DEFAULT_GID
            
            if not sheet_id:
                st.warning("Please enter a Google Sheets ID or Google Drive CSV file URL to continue")
        
        # Data quality indicator
        data_quality = st.session_state.get('data_quality', {})
        if data_quality:
            with st.expander("📊 Data Quality", expanded=True):
                col1, col2 = st.columns(2)
                with col1:
                    completeness = data_quality.get('completeness', 0)
                    if completeness > 80:
                        emoji = "🟢"
                    elif completeness > 60:
                        emoji = "🟡"
                    else:
                        emoji = "🔴"
                    
                    st.metric("Completeness", f"{emoji} {completeness:.1f}%")
                    st.metric("Total Stocks", f"{data_quality.get('total_rows', 0):,}")
                
                with col2:
                    if 'timestamp' in data_quality:
                        age = datetime.now(timezone.utc) - data_quality['timestamp']
                        hours = age.total_seconds() / 3600
                        
                        if hours < 1:
                            freshness = "🟢 Fresh"
                        elif hours < 24:
                            freshness = "🟡 Recent"
                        else:
                            freshness = "🔴 Stale"
                        
                        st.metric("Data Age", freshness)
                    
                    duplicates = data_quality.get('duplicate_tickers', 0)
                    if duplicates > 0:
                        st.metric("Duplicates", f"⚠️ {duplicates}")
        
        # Performance metrics
        perf_metrics = st.session_state.get('performance_metrics', {})
        if perf_metrics:
            with st.expander("⚡ Performance"):
                total_time = sum(perf_metrics.values())
                if total_time < 3:
                    perf_emoji = "🟢"
                elif total_time < 5:
                    perf_emoji = "🟡"
                else:
                    perf_emoji = "🔴"
                
                st.metric("Load Time", f"{perf_emoji} {total_time:.1f}s")
                
                # Show slowest operations
                if len(perf_metrics) > 0:
                    slowest = sorted(perf_metrics.items(), key=lambda x: x[1], reverse=True)[:3]
                    for func_name, elapsed in slowest:
                        if elapsed > 0.001:
                            st.caption(f"{func_name}: {elapsed:.4f}s")
        
        st.markdown("---")
        st.markdown("### 🔍 Smart Filters")
        
        active_filter_count = 0
        
        if st.session_state.get('quick_filter_applied', False):
            active_filter_count += 1
        
        filter_checks = [
            ('category_filter', lambda x: x and len(x) > 0),
            ('sector_filter', lambda x: x and len(x) > 0),
            ('industry_filter', lambda x: x and len(x) > 0),
            ('min_score', lambda x: x > 0),
            ('patterns', lambda x: x and len(x) > 0),
            ('trend_filter', lambda x: x != 'All Trends'),
            ('eps_tier_filter', lambda x: x and len(x) > 0),
            ('pe_tier_filter', lambda x: x and len(x) > 0),
            ('price_tier_filter', lambda x: x and len(x) > 0),
            ('min_eps_change', lambda x: x is not None and str(x).strip() != ''),
            ('min_pe', lambda x: x is not None and str(x).strip() != ''),
            ('max_pe', lambda x: x is not None and str(x).strip() != ''),
            ('require_fundamental_data', lambda x: x),
            ('market_states_filter', lambda x: x and len(x) > 0),
            ('market_strength_range_slider', lambda x: x != (0, 100)),
            ('long_term_strength_range_slider', lambda x: x != (0, 100))
        ]
        
        for key, check_func in filter_checks:
            value = st.session_state.get(key)
            if value is not None and check_func(value):
                active_filter_count += 1
        
        st.session_state.active_filter_count = active_filter_count
        
        if active_filter_count > 0:
            st.info(f"🔍 **{active_filter_count} filter{'s' if active_filter_count > 1 else ''} active**")
        
        if st.button("🗑️ Clear All Filters", 
                    width="stretch", 
                    type="primary" if active_filter_count > 0 else "secondary"):
            SessionStateManager.clear_filters()
            st.success("✅ All filters cleared!")
            st.rerun()
        
        st.markdown("---")
        show_debug = st.checkbox("🐛 Show Debug Info", 
                               value=st.session_state.get('show_debug', False),
                               key="show_debug")
    
    try:
        if st.session_state.data_source == "upload" and uploaded_file is None:
            st.warning("Please upload a CSV file to continue")
            st.stop()
        
        if st.session_state.data_source == "sheet" and not sheet_id:
            st.warning("Please enter a Google Sheets ID or Google Drive CSV file URL to continue")
            st.stop()
        
        with st.spinner("📥 Loading and processing data..."):
            try:
                if st.session_state.data_source == "upload" and uploaded_file is not None:
                    ranked_df, data_timestamp, metadata = load_and_process_data(
                        "upload", file_data=uploaded_file
                    )
                else:
                    # Use the current detected url_source_type, not session state
                    ranked_df, data_timestamp, metadata = load_and_process_data(
                        "sheet", 
                        sheet_id=sheet_id,
                        gid=gid,
                        url_source_type=url_source_type
                    )
                
                st.session_state.ranked_df = ranked_df
                st.session_state.data_timestamp = data_timestamp
                st.session_state.last_refresh = datetime.now(timezone.utc)
                
                if metadata.get('warnings'):
                    for warning in metadata['warnings']:
                        st.warning(warning)
                
                if metadata.get('errors'):
                    for error in metadata['errors']:
                        st.error(error)
                
            except Exception as e:
                logger.error(f"Failed to load data: {str(e)}")
                
                last_good_data = st.session_state.get('last_good_data')
                if last_good_data:
                    ranked_df, data_timestamp, metadata = last_good_data
                    st.warning("Failed to load fresh data, using cached version")
                else:
                    st.error(f"❌ Error: {str(e)}")
                    st.info("Common issues:\n- Invalid Google Sheets ID\n- Sheet not publicly accessible\n- Network connectivity\n- Invalid CSV format")
                    st.stop()
        
    except Exception as e:
        st.error(f"❌ Critical Error: {str(e)}")
        with st.expander("🔍 Error Details"):
            st.code(str(e))
        st.stop()
    
    # Quick Action Buttons
    st.markdown("### ⚡ Quick Actions")
    qa_col1, qa_col2, qa_col3, qa_col4, qa_col5 = st.columns(5)
    
    quick_filter_applied = st.session_state.get('quick_filter_applied', False)
    quick_filter = st.session_state.get('quick_filter', None)
    
    with qa_col1:
        if st.button("📈 Top Gainers", width="stretch"):
            st.session_state['quick_filter'] = 'top_gainers'
            st.session_state['quick_filter_applied'] = True
            st.rerun()
    
    with qa_col2:
        if st.button("🔥 Volume Surges", width="stretch"):
            st.session_state['quick_filter'] = 'volume_surges'
            st.session_state['quick_filter_applied'] = True
            st.rerun()
    
    with qa_col3:
        if st.button("🚀 High Velocity", width="stretch"):
            st.session_state['quick_filter'] = 'velocity_breakout'
            st.session_state['quick_filter_applied'] = True
            st.rerun()
    
    with qa_col4:
        if st.button("🌋 Tsunami", width="stretch"):
            st.session_state['quick_filter'] = 'institutional_tsunami'
            st.session_state['quick_filter_applied'] = True
            st.rerun()
    
    with qa_col5:
        if st.button("🌊 Show All", width="stretch"):
            st.session_state['quick_filter'] = None
            st.session_state['quick_filter_applied'] = False
            st.rerun()
    
    # Get ranked_df from session state
    ranked_df = st.session_state.get('ranked_df')
    if ranked_df is None:
        st.error("No data available. Please check your data source configuration.")
        st.stop()
    
    # Initialize ranked_df_display with a safe default
    ranked_df_display = ranked_df
    
    if quick_filter:
        if quick_filter == 'top_gainers':
            ranked_df_display = ranked_df[ranked_df['momentum_score'] >= 80]
            st.info(f"Showing {len(ranked_df_display)} stocks with momentum score ≥ 80")
        elif quick_filter == 'volume_surges':
            ranked_df_display = ranked_df[ranked_df['rvol'] >= 3]
            st.info(f"Showing {len(ranked_df_display)} stocks with RVOL ≥ 3x")
        elif quick_filter == 'velocity_breakout':
            ranked_df_display = ranked_df[ranked_df['patterns'].str.contains('VELOCITY BREAKOUT', na=False)]
            st.info(f"Showing {len(ranked_df_display)} stocks with Velocity Breakout pattern")
        elif quick_filter == 'institutional_tsunami':
            ranked_df_display = ranked_df[ranked_df['patterns'].str.contains('🌋 INSTITUTIONAL TSUNAMI', na=False)]
            st.info(f"Showing {len(ranked_df_display)} stocks with Institutional Tsunami pattern")
    
    # Sidebar filters
    with st.sidebar:
        # Initialize centralized filter state
        FilterEngine.initialize_filters()
        
        # Initialize filters dict for current frame
        filters = {}
        
        # Display Mode
        st.markdown("### 📊 Display Mode")
        
        # Ensure user_preferences exists and safe access to display_mode with fallback
        if 'user_preferences' not in st.session_state:
            st.session_state.user_preferences = {'display_mode': 'Hybrid (Technical + Fundamentals)'}
        current_display_mode = st.session_state.user_preferences.get('display_mode', 'Hybrid (Technical + Fundamentals)')
        
        display_mode = st.radio(
            "Choose your view:",
            options=["Technical", "Hybrid (Technical + Fundamentals)"],
            index=0 if current_display_mode == 'Technical' else 1,
            help="Technical: Pure momentum analysis | Hybrid: Adds PE & EPS data",
            key="display_mode_toggle"
        )
        
        st.session_state.user_preferences['display_mode'] = display_mode
        show_fundamentals = (display_mode == "Hybrid (Technical + Fundamentals)")
        
        # Show available fundamental columns when in Hybrid mode for user transparency
        if show_fundamentals and 'ranked_df' in st.session_state:
            available_fund_cols = [col for col in ['pe', 'eps_current', 'eps_change_pct'] 
                                 if col in st.session_state.ranked_df.columns]
            if available_fund_cols:
                st.sidebar.success(f"✅ Fundamental data available: {', '.join(available_fund_cols)}")
            else:
                st.sidebar.warning("⚠️ No fundamental data found in current dataset")
        
        st.markdown("---")
        
        # CRITICAL: Define callback functions BEFORE widgets
        def sync_categories():
            if 'category_multiselect' in st.session_state:
                st.session_state.filter_state['categories'] = st.session_state.category_multiselect
        
        def sync_sectors():
            if 'sector_multiselect' in st.session_state:
                st.session_state.filter_state['sectors'] = st.session_state.sector_multiselect
        
        def sync_industries():
            if 'industry_multiselect' in st.session_state:
                st.session_state.filter_state['industries'] = st.session_state.industry_multiselect
        
        def sync_min_score():
            if 'min_score_slider' in st.session_state:
                st.session_state.filter_state['min_score'] = st.session_state.min_score_slider
        
        def sync_position_tier():
            if 'position_tier_multiselect_intelligence' in st.session_state:
                st.session_state.filter_state['position_tiers'] = st.session_state.position_tier_multiselect_intelligence
        
        def sync_position_range():
            if 'position_range_slider' in st.session_state:
                st.session_state.filter_state['position_range'] = st.session_state.position_range_slider
        
        def sync_performance_tier():
            if 'performance_tier_multiselect_intelligence' in st.session_state:
                st.session_state.filter_state['performance_tiers'] = st.session_state.performance_tier_multiselect_intelligence
        
        def sync_performance_custom_range():
            # Sync individual range sliders for all timeframes
            if 'ret_1d_range_slider' in st.session_state:
                st.session_state.filter_state['ret_1d_range'] = st.session_state.ret_1d_range_slider
            if 'ret_3d_range_slider' in st.session_state:
                st.session_state.filter_state['ret_3d_range'] = st.session_state.ret_3d_range_slider
            if 'ret_7d_range_slider' in st.session_state:
                st.session_state.filter_state['ret_7d_range'] = st.session_state.ret_7d_range_slider
            if 'ret_30d_range_slider' in st.session_state:
                st.session_state.filter_state['ret_30d_range'] = st.session_state.ret_30d_range_slider
            if 'ret_3m_range_slider' in st.session_state:
                st.session_state.filter_state['ret_3m_range'] = st.session_state.ret_3m_range_slider
            if 'ret_6m_range_slider' in st.session_state:
                st.session_state.filter_state['ret_6m_range'] = st.session_state.ret_6m_range_slider
            if 'ret_1y_range_slider' in st.session_state:
                st.session_state.filter_state['ret_1y_range'] = st.session_state.ret_1y_range_slider
            if 'ret_3y_range_slider' in st.session_state:
                st.session_state.filter_state['ret_3y_range'] = st.session_state.ret_3y_range_slider
            if 'ret_5y_range_slider' in st.session_state:
                st.session_state.filter_state['ret_5y_range'] = st.session_state.ret_5y_range_slider
            # Legacy support
            if 'performance_custom_range_slider' in st.session_state:
                st.session_state.filter_state['performance_custom_range'] = st.session_state.performance_custom_range_slider
        
        def sync_volume_tier():
            if 'volume_tier_multiselect_intelligence' in st.session_state:
                st.session_state.filter_state['volume_tiers'] = st.session_state.volume_tier_multiselect_intelligence
        
        def sync_vmi_tier():
            if 'vmi_tier_multiselect_intelligence' in st.session_state:
                st.session_state.filter_state['vmi_tiers'] = st.session_state.vmi_tier_multiselect_intelligence
        
        def sync_momentum_harmony_tier():
            if 'momentum_harmony_tier_multiselect_intelligence' in st.session_state:
                st.session_state.filter_state['momentum_harmony_tiers'] = st.session_state.momentum_harmony_tier_multiselect_intelligence
        
        def sync_rvol_range():
            if 'rvol_range_slider' in st.session_state:
                st.session_state.filter_state['rvol_range'] = st.session_state.rvol_range_slider
        
        # Performance Filter Sync Functions
        def sync_performance_dropdowns():
            # Sync all performance dropdown selections
            for ret_col in ['ret_1d', 'ret_3d', 'ret_7d', 'ret_30d', 'ret_3m', 'ret_6m', 'ret_1y', 'ret_3y', 'ret_5y']:
                dropdown_key = f'{ret_col}_dropdown'
                if dropdown_key in st.session_state:
                    st.session_state.filter_state[f'{ret_col}_selection'] = st.session_state[dropdown_key]
        
        def sync_performance_sliders():
            # Sync all performance range sliders
            for ret_col in ['ret_1d', 'ret_3d', 'ret_7d', 'ret_30d', 'ret_3m', 'ret_6m', 'ret_1y', 'ret_3y', 'ret_5y']:
                slider_key = f'{ret_col}_range_slider'
                if slider_key in st.session_state:
                    st.session_state.filter_state[f'{ret_col}_range'] = st.session_state[slider_key]
        
        def sync_patterns():
            if 'patterns_multiselect' in st.session_state:
                st.session_state.filter_state['patterns'] = st.session_state.patterns_multiselect

        # NEW: Two-stage pattern filtering sync functions
        def sync_exclude_patterns():
            if 'exclude_patterns_multiselect' in st.session_state:
                st.session_state.filter_state['exclude_patterns'] = st.session_state.exclude_patterns_multiselect

        def sync_include_patterns():
            if 'include_patterns_multiselect' in st.session_state:
                st.session_state.filter_state['include_patterns'] = st.session_state.include_patterns_multiselect

        def sync_combination_patterns():
            if 'combination_patterns_multiselect' in st.session_state:
                st.session_state.filter_state['combination_patterns'] = st.session_state.combination_patterns_multiselect

        def sync_market_states():
            if 'market_states_multiselect' in st.session_state:
                st.session_state.filter_state['market_states'] = st.session_state.market_states_multiselect
        
        # Intelligence Score Dropdown and Slider Sync Functions
        def sync_position_score_dropdown():
            if 'position_score_dropdown' in st.session_state:
                st.session_state.filter_state['position_score_selection'] = st.session_state.position_score_dropdown
        
        def sync_position_score_slider():
            if 'position_score_slider' in st.session_state:
                st.session_state.filter_state['position_score_range'] = st.session_state.position_score_slider
        
        def sync_volume_score_dropdown():
            if 'volume_score_dropdown' in st.session_state:
                st.session_state.filter_state['volume_score_selection'] = st.session_state.volume_score_dropdown
        
        def sync_volume_score_slider():
            if 'volume_score_slider' in st.session_state:
                st.session_state.filter_state['volume_score_range'] = st.session_state.volume_score_slider
        
        def sync_market_strength():
            if 'market_strength_slider' in st.session_state:
                st.session_state.filter_state['market_strength_range'] = st.session_state.market_strength_slider
        
        def sync_long_term_strength():
            if 'long_term_strength_slider' in st.session_state:
                st.session_state.filter_state['long_term_strength_range'] = st.session_state.long_term_strength_slider
        
        def sync_momentum_score_dropdown():
            if 'momentum_score_dropdown' in st.session_state:
                st.session_state.filter_state['momentum_score_selection'] = st.session_state.momentum_score_dropdown
        
        def sync_momentum_score_slider():
            if 'momentum_score_slider' in st.session_state:
                st.session_state.filter_state['momentum_score_range'] = st.session_state.momentum_score_slider
        
        def sync_acceleration_score_dropdown():
            if 'acceleration_score_dropdown' in st.session_state:
                st.session_state.filter_state['acceleration_score_selection'] = st.session_state.acceleration_score_dropdown
        
        def sync_acceleration_score_slider():
            if 'acceleration_score_slider' in st.session_state:
                st.session_state.filter_state['acceleration_score_range'] = st.session_state.acceleration_score_slider
        
        def sync_breakout_score_dropdown():
            if 'breakout_score_dropdown' in st.session_state:
                st.session_state.filter_state['breakout_score_selection'] = st.session_state.breakout_score_dropdown
        
        def sync_breakout_score_slider():
            if 'breakout_score_slider' in st.session_state:
                st.session_state.filter_state['breakout_score_range'] = st.session_state.breakout_score_slider
        
        def sync_rvol_score_dropdown():
            if 'rvol_score_dropdown' in st.session_state:
                st.session_state.filter_state['rvol_score_selection'] = st.session_state.rvol_score_dropdown
        
        def sync_rvol_score_slider():
            if 'rvol_score_slider' in st.session_state:
                st.session_state.filter_state['rvol_score_range'] = st.session_state.rvol_score_slider
        
        # BIDIRECTIONAL SMART INTERCONNECTED FILTERS: Category ↔ Sector ↔ Industry
        # Any selection affects the other two filters bidirectionally
        st.markdown("#### 🏢 Company Classification")
        
        # Safety check to ensure we have valid data before proceeding with filters
        if ranked_df_display is None or ranked_df_display.empty:
            st.warning("No data available for filtering. Please check your data source.")
            return
        
        # Category filter with callback
        categories = FilterEngine.get_filter_options(ranked_df_display, 'category', filters)
        
        # Clean default values to only include available options (INTERCONNECTION FIX)
        stored_categories = st.session_state.filter_state.get('categories', [])
        valid_category_defaults = [cat for cat in stored_categories if cat in categories]
        
        selected_categories = st.multiselect(
            f"Market Cap Category ({len(categories)} available)",
            options=categories,
            default=valid_category_defaults,
            placeholder="Select categories (empty = All)",
            help="📊 Filter by market cap category. Updates based on selected sectors and industries.",
            key="category_multiselect",
            on_change=sync_categories  # SYNC ON CHANGE
        )
        
        if selected_categories:
            filters['categories'] = selected_categories
        
        # Sector filter with callback
        sectors = FilterEngine.get_filter_options(ranked_df_display, 'sector', filters)
        
        # Clean default values to only include available options (INTERCONNECTION FIX)
        stored_sectors = st.session_state.filter_state.get('sectors', [])
        valid_sector_defaults = [sec for sec in stored_sectors if sec in sectors]
        
        selected_sectors = st.multiselect(
            f"Sector ({len(sectors)} available)",
            options=sectors,
            default=valid_sector_defaults,
            placeholder="Select sectors (empty = All)",
            help="🏭 Filter by business sector. Updates based on selected categories and industries.",
            key="sector_multiselect",
            on_change=sync_sectors  # SYNC ON CHANGE
        )
        
        if selected_sectors:
            filters['sectors'] = selected_sectors
        
        # Industry filter with callback
        industries = FilterEngine.get_filter_options(ranked_df_display, 'industry', filters)
        
        # Clean default values to only include available options (INTERCONNECTION FIX)
        stored_industries = st.session_state.filter_state.get('industries', [])
        valid_industry_defaults = [ind for ind in stored_industries if ind in industries]
        
        selected_industries = st.multiselect(
            f"Industry ({len(industries)} available)",
            options=industries,
            default=valid_industry_defaults,
            placeholder="Select industries (empty = All)",
            help="🏢 Filter by specific industry. Updates categories and sectors to show only relevant options.",
            key="industry_multiselect",
            on_change=sync_industries  # SYNC ON CHANGE
        )
        
        if selected_industries:
            filters['industries'] = selected_industries

        st.markdown("#### ✨ Pattern Detector")
        
        # Get all available patterns
        all_patterns = set()
        for patterns in ranked_df_display['patterns'].dropna():
            if patterns:
                all_patterns.update(patterns.split(' | '))
        
        if all_patterns:
            sorted_patterns = sorted(all_patterns)
            
            # STAGE 1: EXCLUDE PATTERNS
            
            # Get stored exclude patterns
            stored_exclude_patterns = st.session_state.filter_state.get('exclude_patterns', [])
            valid_exclude_defaults = [pat for pat in stored_exclude_patterns if pat in sorted_patterns]
            
            excluded_patterns = st.multiselect(
                f"🚫 Exclude Patterns ({len(sorted_patterns)} available)",
                options=sorted_patterns,
                default=valid_exclude_defaults,
                placeholder="Select patterns to exclude (empty = exclude none)",
                help="Stocks containing ANY of these patterns will be REMOVED from results",
                key="exclude_patterns_multiselect",
                on_change=sync_exclude_patterns
            )
            
            if excluded_patterns:
                filters['exclude_patterns'] = excluded_patterns
            
            # STAGE 2: INCLUDE PATTERNS
            
            # Get stored include patterns
            stored_include_patterns = st.session_state.filter_state.get('include_patterns', [])
            valid_include_defaults = [pat for pat in stored_include_patterns if pat in sorted_patterns]
            
            included_patterns = st.multiselect(
                f"✅ Include Patterns ({len(sorted_patterns)} available)",
                options=sorted_patterns,
                default=valid_include_defaults,
                placeholder="Select patterns to include (empty = include all remaining)",
                help="From stocks remaining after exclusion, show only those with ANY of these patterns",
                key="include_patterns_multiselect", 
                on_change=sync_include_patterns
            )
            
            if included_patterns:
                filters['include_patterns'] = included_patterns
            
            # STAGE 3: COMBINATION PATTERNS (AND Logic)
            
            # Get stored combination patterns
            stored_combination_patterns = st.session_state.filter_state.get('combination_patterns', [])
            valid_combination_defaults = [pat for pat in stored_combination_patterns if pat in sorted_patterns]
            
            combination_patterns = st.multiselect(
                f"🔗 Combination Patterns ({len(sorted_patterns)} available)",
                options=sorted_patterns,
                default=valid_combination_defaults,
                placeholder="Select patterns for AND logic (empty = no combination filter)",
                help="Show only stocks that have ALL of these patterns simultaneously (AND logic)",
                key="combination_patterns_multiselect",
                on_change=sync_combination_patterns
            )
            
            if combination_patterns:
                filters['combination_patterns'] = combination_patterns
        
        else:
            st.info("📊 No patterns detected in current dataset")
        
        # Trend filter with callback and custom range
        st.markdown("#### 📈 Trend Strength")
        trend_options = {
            "All Trends": (0, 100),
            "🔥 Exceptional (85+)": (85, 100),
            "🚀 Strong (70-84)": (70, 84),
            "✅ Good (55-69)": (55, 69),
            "➡️ Neutral (40-54)": (40, 54),
            "⚠️ Weak (25-39)": (25, 39),
            "🔻 Poor (<25)": (0, 24),
            "🎯 Custom Range": None  # Special option for custom range
        }
        
        current_trend = st.session_state.filter_state.get('trend_filter', "All Trends")
        if current_trend not in trend_options:
            current_trend = "All Trends"
        
        # Custom sync function for trend with custom range support
        def sync_trend_with_custom():
            if 'trend_selectbox' in st.session_state:
                selected = st.session_state.trend_selectbox
                st.session_state.filter_state['trend_filter'] = selected
                
                if selected == "🎯 Custom Range":
                    # Don't set range here, will be set by slider
                    pass
                else:
                    st.session_state.filter_state['trend_range'] = trend_options[selected]
        
        def sync_trend_custom_slider():
            if 'trend_custom_range_slider' in st.session_state:
                st.session_state.filter_state['trend_custom_range'] = st.session_state.trend_custom_range_slider
        
        selected_trend = st.selectbox(
            "Trend Quality",
            options=list(trend_options.keys()),
            index=list(trend_options.keys()).index(current_trend),
            help="Filter stocks by trend strength based on SMA alignment. Choose Custom Range for precise control.",
            key="trend_selectbox",
            on_change=sync_trend_with_custom
        )
        
        # Show custom range slider when Custom Range is selected
        if selected_trend == "🎯 Custom Range":
            # Get current custom range from session state, default to (0, 100)
            current_custom_range = st.session_state.filter_state.get('trend_custom_range', (0, 100))
            
            custom_range = st.slider(
                "Custom Trend Quality Range",
                min_value=0,
                max_value=100,
                value=current_custom_range,
                step=1,
                help="🔥 85+: Exceptional | 🚀 70-84: Strong | ✅ 55-69: Good | ➡️ 40-54: Neutral | ⚠️ 25-39: Weak | 🔻 <25: Poor",
                key="trend_custom_range_slider",
                on_change=sync_trend_custom_slider
            )
            
            # Set the filter range to custom slider value
            filters['trend_filter'] = selected_trend
            filters['trend_range'] = custom_range
            # Also update session state for consistency
            st.session_state.filter_state['trend_range'] = custom_range
            
            # Show indicator for selected range
            def get_trend_indicator_for_range(min_val, max_val):
                """Get appropriate indicator for the selected range"""
                if min_val >= 85:
                    return "🔥"
                elif min_val >= 70:
                    return "🚀"
                elif min_val >= 55:
                    return "✅"
                elif min_val >= 40:
                    return "➡️"
                elif min_val >= 25:
                    return "⚠️"
                else:
                    return "🔻"
            
            range_indicator = get_trend_indicator_for_range(custom_range[0], custom_range[1])
            st.caption(f"{range_indicator} **Selected Range**: {custom_range[0]}-{custom_range[1]} | Filtering stocks with trend quality in this range")
            
        elif selected_trend != "All Trends":
            filters['trend_filter'] = selected_trend
            filters['trend_range'] = trend_options[selected_trend]
        
        # Market State filters with callbacks
        st.markdown("#### 📈 Market State Filters")
        
        # Add filter presets and custom selection option (no individual states in main multiselect)
        preset_options = ["🎯 MOMENTUM (Default)", "⚡ AGGRESSIVE", "💎 VALUE", "🛡️ DEFENSIVE", "🌍 ALL"]
        market_state_with_presets = preset_options + ["📊 Custom Selection"]
        
        selected_market_states = st.multiselect(
            "Market State",
            options=market_state_with_presets,
            default=st.session_state.filter_state.get('market_states', []),
            placeholder="Select market states or use preset strategy",
            help="Filter by market momentum state. Use presets for different trading strategies or select Custom Selection for individual states",
            key="market_states_multiselect",
            on_change=sync_market_states  # SYNC ON CHANGE
        )
        
        # Show custom market states dropdown when Custom Selection is active
        custom_selection_active = "📊 Custom Selection" in selected_market_states
        custom_states_selection = []
        
        if custom_selection_active:
            # Define the 8 specific market states for custom selection
            custom_market_states = [
                "BOUNCE",
                "DOWNTREND", 
                "PULLBACK",
                "ROTATION",
                "SIDEWAYS",
                "STRONG_DOWNTREND",
                "STRONG_UPTREND",
                "UPTREND"
            ]
            
            # Add sync function for custom states
            def sync_custom_market_states():
                if 'custom_market_states_multiselect' in st.session_state:
                    st.session_state.filter_state['custom_market_states'] = st.session_state.custom_market_states_multiselect
            
            custom_states_selection = st.multiselect(
                "Select Individual Market States",
                options=custom_market_states,
                default=st.session_state.filter_state.get('custom_market_states', []),
                placeholder="Choose specific market states to filter",
                help="Select one or more market states to include in your filter",
                key="custom_market_states_multiselect",
                on_change=sync_custom_market_states
            )
            
            # Long Term Strength Slider - Professional Implementation
            st.markdown("**🏆 Long Term Strength Filter**")
            long_term_strength_range = st.slider(
                "Long Term Strength Range",
                min_value=0,
                max_value=100,
                value=st.session_state.filter_state.get('long_term_strength_range', (0, 100)),
                step=5,
                help="Filter by long term strength score (0-100). Higher values indicate stronger long-term trend consistency, momentum harmony, and sustained performance characteristics.",
                key="long_term_strength_slider",
                on_change=sync_long_term_strength
            )
            
            # Market Strength Slider - Professional Implementation
            st.markdown("**📊 Market Strength Filter**")
            market_strength_range = st.slider(
                "Market Strength Range",
                min_value=0,
                max_value=100,
                value=st.session_state.filter_state.get('market_strength_range', (0, 100)),
                step=5,
                help="Filter by overall market strength score (0-100). Higher values indicate stronger market conditions with better momentum, acceleration, and volume characteristics.",
                key="market_strength_slider",
                on_change=sync_market_strength
            )
            
            # Combine selections for filtering
            if custom_states_selection:
                # Use custom states selection when available
                filters['market_states'] = ["📊 Custom Selection"] + custom_states_selection
            else:
                # Just custom selection flag without individual states
                filters['market_states'] = selected_market_states
        else:
            # Regular preset selection
            if selected_market_states:
                filters['market_states'] = selected_market_states
        
        # Add Market Strength filter when Custom Selection is active
        if custom_selection_active:
            if st.session_state.filter_state.get('market_strength_range', (0, 100)) != (0, 100):
                filters['market_strength_range'] = st.session_state.filter_state['market_strength_range']
            if st.session_state.filter_state.get('long_term_strength_range', (0, 100)) != (0, 100):
                filters['long_term_strength_range'] = st.session_state.filter_state['long_term_strength_range']
        
        # 🎯 Score Component - Professional Expandable Section
        with st.expander("🎯 Score Component", expanded=False):
            
            # Minimum Master Score
            min_score = st.slider(
                "Minimum Master Score",
                min_value=0,
                max_value=100,
                value=st.session_state.filter_state.get('min_score', 0),
                step=5,
                help="Filter stocks by minimum master score (0-100). Higher scores indicate better overall ranking.",
                key="min_score_slider",
                on_change=sync_min_score
            )
            
            if min_score > 0:
                filters['min_score'] = min_score
            
            # Position Score Dropdown with Custom Range
            if 'position_score' in ranked_df_display.columns:
                position_score_options = [
                    "All Scores",
                    "🟢 Strong (>= 80)",
                    "🟡 Good (>= 60)",
                    "🟠 Fair (>= 40)",
                    "🔴 Weak (< 40)",
                    "🎯 Custom Range"
                ]
                
                current_position_selection = st.session_state.filter_state.get('position_score_selection', "All Scores")
                if current_position_selection not in position_score_options:
                    current_position_selection = "All Scores"
                
                position_score_selection = st.selectbox(
                    "Position Score",
                    options=position_score_options,
                    index=position_score_options.index(current_position_selection),
                    help="Filter stocks by position score strength",
                    key="position_score_dropdown",
                    on_change=sync_position_score_dropdown
                )
                
                # Show custom range slider when "🎯 Custom Range" is selected
                if position_score_selection == "🎯 Custom Range":
                    position_score_range = st.slider(
                        "Position Score Custom Range",
                        min_value=0,
                        max_value=100,
                        value=st.session_state.filter_state.get('position_score_range', (0, 100)),
                        step=5,
                        help="Filter stocks by position score custom range (0-100)",
                        key="position_score_slider",
                        on_change=sync_position_score_slider
                    )
                    
                    if position_score_range != (0, 100):
                        filters['position_score_range'] = position_score_range
                elif position_score_selection != "All Scores":
                    # Map selection to range
                    if position_score_selection == "🟢 Strong (>= 80)":
                        filters['position_score_range'] = (80, 100)
                    elif position_score_selection == "🟡 Good (>= 60)":
                        filters['position_score_range'] = (60, 100)
                    elif position_score_selection == "🟠 Fair (>= 40)":
                        filters['position_score_range'] = (40, 100)
                    elif position_score_selection == "🔴 Weak (< 40)":
                        filters['position_score_range'] = (0, 39)
            
            # Volume Score Dropdown with Custom Range
            if 'volume_score' in ranked_df_display.columns:
                volume_score_options = [
                    "All Scores",
                    "🟢 Strong (>= 80)",
                    "🟡 Good (>= 60)",
                    "🟠 Fair (>= 40)",
                    "🔴 Weak (< 40)",
                    "🎯 Custom Range"
                ]
                
                current_volume_selection = st.session_state.filter_state.get('volume_score_selection', "All Scores")
                if current_volume_selection not in volume_score_options:
                    current_volume_selection = "All Scores"
                
                volume_score_selection = st.selectbox(
                    "Volume Score",
                    options=volume_score_options,
                    index=volume_score_options.index(current_volume_selection),
                    help="Filter stocks by volume score strength",
                    key="volume_score_dropdown",
                    on_change=sync_volume_score_dropdown
                )
                
                # Show custom range slider when "🎯 Custom Range" is selected
                if volume_score_selection == "🎯 Custom Range":
                    volume_score_range = st.slider(
                        "Volume Score Custom Range",
                        min_value=0,
                        max_value=100,
                        value=st.session_state.filter_state.get('volume_score_range', (0, 100)),
                        step=5,
                        help="Filter stocks by volume score custom range (0-100)",
                        key="volume_score_slider",
                        on_change=sync_volume_score_slider
                    )
                    
                    if volume_score_range != (0, 100):
                        filters['volume_score_range'] = volume_score_range
                elif volume_score_selection != "All Scores":
                    # Map selection to range
                    if volume_score_selection == "🟢 Strong (>= 80)":
                        filters['volume_score_range'] = (80, 100)
                    elif volume_score_selection == "🟡 Good (>= 60)":
                        filters['volume_score_range'] = (60, 100)
                    elif volume_score_selection == "🟠 Fair (>= 40)":
                        filters['volume_score_range'] = (40, 100)
                    elif volume_score_selection == "🔴 Weak (< 40)":
                        filters['volume_score_range'] = (0, 39)
            
            # Momentum Score Dropdown with Custom Range
            if 'momentum_score' in ranked_df_display.columns:
                momentum_score_options = [
                    "All Scores",
                    "🟢 Strong (>= 80)",
                    "🟡 Good (>= 60)",
                    "🟠 Fair (>= 40)",
                    "🔴 Weak (< 40)",
                    "🎯 Custom Range"
                ]
                
                current_momentum_selection = st.session_state.filter_state.get('momentum_score_selection', "All Scores")
                if current_momentum_selection not in momentum_score_options:
                    current_momentum_selection = "All Scores"
                
                momentum_score_selection = st.selectbox(
                    "Momentum Score",
                    options=momentum_score_options,
                    index=momentum_score_options.index(current_momentum_selection),
                    help="Filter stocks by momentum score strength",
                    key="momentum_score_dropdown",
                    on_change=sync_momentum_score_dropdown
                )
                
                # Show custom range slider when "🎯 Custom Range" is selected
                if momentum_score_selection == "🎯 Custom Range":
                    momentum_score_range = st.slider(
                        "Momentum Score Custom Range",
                        min_value=0,
                        max_value=100,
                        value=st.session_state.filter_state.get('momentum_score_range', (0, 100)),
                        step=5,
                        help="Filter stocks by momentum score custom range (0-100)",
                        key="momentum_score_slider",
                        on_change=sync_momentum_score_slider
                    )
                    
                    if momentum_score_range != (0, 100):
                        filters['momentum_score_range'] = momentum_score_range
                elif momentum_score_selection != "All Scores":
                    # Map selection to range
                    if momentum_score_selection == "🟢 Strong (>= 80)":
                        filters['momentum_score_range'] = (80, 100)
                    elif momentum_score_selection == "🟡 Good (>= 60)":
                        filters['momentum_score_range'] = (60, 100)
                    elif momentum_score_selection == "🟠 Fair (>= 40)":
                        filters['momentum_score_range'] = (40, 100)
                    elif momentum_score_selection == "🔴 Weak (< 40)":
                        filters['momentum_score_range'] = (0, 39)
            
            # Acceleration Score Dropdown with Custom Range
            if 'acceleration_score' in ranked_df_display.columns:
                acceleration_score_options = [
                    "All Scores",
                    "🟢 Strong (>= 80)",
                    "🟡 Good (>= 60)",
                    "🟠 Fair (>= 40)",
                    "🔴 Weak (< 40)",
                    "🎯 Custom Range"
                ]
                
                current_acceleration_selection = st.session_state.filter_state.get('acceleration_score_selection', "All Scores")
                if current_acceleration_selection not in acceleration_score_options:
                    current_acceleration_selection = "All Scores"
                
                acceleration_score_selection = st.selectbox(
                    "Acceleration Score",
                    options=acceleration_score_options,
                    index=acceleration_score_options.index(current_acceleration_selection),
                    help="Filter stocks by acceleration score strength",
                    key="acceleration_score_dropdown",
                    on_change=sync_acceleration_score_dropdown
                )
                
                # Show custom range slider when "🎯 Custom Range" is selected
                if acceleration_score_selection == "🎯 Custom Range":
                    acceleration_score_range = st.slider(
                        "Acceleration Score Custom Range",
                        min_value=0,
                        max_value=100,
                        value=st.session_state.filter_state.get('acceleration_score_range', (0, 100)),
                        step=5,
                        help="Filter stocks by acceleration score custom range (0-100)",
                        key="acceleration_score_slider",
                        on_change=sync_acceleration_score_slider
                    )
                    
                    if acceleration_score_range != (0, 100):
                        filters['acceleration_score_range'] = acceleration_score_range
                elif acceleration_score_selection != "All Scores":
                    # Map selection to range
                    if acceleration_score_selection == "🟢 Strong (>= 80)":
                        filters['acceleration_score_range'] = (80, 100)
                    elif acceleration_score_selection == "🟡 Good (>= 60)":
                        filters['acceleration_score_range'] = (60, 100)
                    elif acceleration_score_selection == "🟠 Fair (>= 40)":
                        filters['acceleration_score_range'] = (40, 100)
                    elif acceleration_score_selection == "🔴 Weak (< 40)":
                        filters['acceleration_score_range'] = (0, 39)
            
            # Breakout Score Dropdown with Custom Range
            if 'breakout_score' in ranked_df_display.columns:
                breakout_score_options = [
                    "All Scores",
                    "🟢 Strong (>= 80)",
                    "🟡 Good (>= 60)",
                    "🟠 Fair (>= 40)",
                    "🔴 Weak (< 40)",
                    "🎯 Custom Range"
                ]
                
                current_breakout_selection = st.session_state.filter_state.get('breakout_score_selection', "All Scores")
                if current_breakout_selection not in breakout_score_options:
                    current_breakout_selection = "All Scores"
                
                breakout_score_selection = st.selectbox(
                    "Breakout Score",
                    options=breakout_score_options,
                    index=breakout_score_options.index(current_breakout_selection),
                    help="Filter stocks by breakout score strength",
                    key="breakout_score_dropdown",
                    on_change=sync_breakout_score_dropdown
                )
                
                # Show custom range slider when "🎯 Custom Range" is selected
                if breakout_score_selection == "🎯 Custom Range":
                    breakout_score_range = st.slider(
                        "Breakout Score Custom Range",
                        min_value=0,
                        max_value=100,
                        value=st.session_state.filter_state.get('breakout_score_range', (0, 100)),
                        step=5,
                        help="Filter stocks by breakout score custom range (0-100)",
                        key="breakout_score_slider",
                        on_change=sync_breakout_score_slider
                    )
                    
                    if breakout_score_range != (0, 100):
                        filters['breakout_score_range'] = breakout_score_range
                elif breakout_score_selection != "All Scores":
                    # Map selection to range
                    if breakout_score_selection == "🟢 Strong (>= 80)":
                        filters['breakout_score_range'] = (80, 100)
                    elif breakout_score_selection == "🟡 Good (>= 60)":
                        filters['breakout_score_range'] = (60, 100)
                    elif breakout_score_selection == "🟠 Fair (>= 40)":
                        filters['breakout_score_range'] = (40, 100)
                    elif breakout_score_selection == "🔴 Weak (< 40)":
                        filters['breakout_score_range'] = (0, 39)
            
            # RVOL Score Dropdown with Custom Range
            if 'rvol_score' in ranked_df_display.columns:
                rvol_score_options = [
                    "All Scores",
                    "🟢 Strong (>= 80)",
                    "🟡 Good (>= 60)",
                    "🟠 Fair (>= 40)",
                    "🔴 Weak (< 40)",
                    "🎯 Custom Range"
                ]
                
                current_rvol_selection = st.session_state.filter_state.get('rvol_score_selection', "All Scores")
                if current_rvol_selection not in rvol_score_options:
                    current_rvol_selection = "All Scores"
                
                rvol_score_selection = st.selectbox(
                    "RVOL Score",
                    options=rvol_score_options,
                    index=rvol_score_options.index(current_rvol_selection),
                    help="Filter stocks by RVOL score strength",
                    key="rvol_score_dropdown",
                    on_change=sync_rvol_score_dropdown
                )
                
                # Show custom range slider when "🎯 Custom Range" is selected
                if rvol_score_selection == "🎯 Custom Range":
                    rvol_score_range = st.slider(
                        "RVOL Score Custom Range",
                        min_value=0,
                        max_value=100,
                        value=st.session_state.filter_state.get('rvol_score_range', (0, 100)),
                        step=5,
                        help="Filter stocks by RVOL score custom range (0-100)",
                        key="rvol_score_slider",
                        on_change=sync_rvol_score_slider
                    )
                    
                    if rvol_score_range != (0, 100):
                        filters['rvol_score_range'] = rvol_score_range
                elif rvol_score_selection != "All Scores":
                    # Map selection to range
                    if rvol_score_selection == "🟢 Strong (>= 80)":
                        filters['rvol_score_range'] = (80, 100)
                    elif rvol_score_selection == "🟡 Good (>= 60)":
                        filters['rvol_score_range'] = (60, 100)
                    elif rvol_score_selection == "🟠 Fair (>= 40)":
                        filters['rvol_score_range'] = (40, 100)
                    elif rvol_score_selection == "🔴 Weak (< 40)":
                        filters['rvol_score_range'] = (0, 39)
        
        # 📈 Performance Filter - Professional Expandable Section
        with st.expander("📈 Performance Filter", expanded=False):
            
            # Check for available return columns
            available_return_cols = [col for col in ['ret_1d', 'ret_3d', 'ret_7d', 'ret_30d', 'ret_3m', 'ret_6m', 'ret_1y', 'ret_3y', 'ret_5y'] if col in ranked_df_display.columns]
            
            if available_return_cols:
                # Individual Return Period Filters
                st.markdown("**📊 Individual Return Period Filters**")
                
                # Define performance configuration with your exact specifications
                performance_config = {
                    'ret_1d': {
                        'name': '1 Day Return',
                        'presets': [
                            {'label': '💥 Explosive', 'min': 10, 'max': None, 'description': '>10% 1D'},
                            {'label': '🚀 Strong Rise', 'min': 5, 'max': 10, 'description': '5-10% 1D'},
                            {'label': '📈 Positive', 'min': 2, 'max': 5, 'description': '2-5% 1D'},
                            {'label': '➡️ Flat', 'min': -2, 'max': 2, 'description': '-2% to 2% 1D'},
                            {'label': '📉 Negative', 'min': -5, 'max': -2, 'description': '-5% to -2% 1D'},
                            {'label': '💣 Crash', 'min': None, 'max': -5, 'description': '<-5% 1D'}
                        ],
                        'slider_range': (-50, 50),
                        'slider_step': 0.5,
                        'default_min': -50,
                        'default_max': 50
                    },
                    'ret_3d': {
                        'name': '3 Day Return',
                        'presets': [
                            {'label': '🌟 3-Day Surge', 'min': 15, 'max': None, 'description': '>15% 3D'},
                            {'label': '⚡ Strong Momentum', 'min': 8, 'max': 15, 'description': '8-15% 3D'},
                            {'label': '📊 Steady Rise', 'min': 3, 'max': 8, 'description': '3-8% 3D'},
                            {'label': '⏸️ Consolidating', 'min': -3, 'max': 3, 'description': '-3% to 3% 3D'},
                            {'label': '⚠️ Weakening', 'min': -8, 'max': -3, 'description': '-8% to -3% 3D'},
                            {'label': '🔻 Sharp Decline', 'min': None, 'max': -8, 'description': '<-8% 3D'}
                        ],
                        'slider_range': (-75, 75),
                        'slider_step': 1,
                        'default_min': -75,
                        'default_max': 75
                    },
                    'ret_7d': {
                        'name': 'Weekly Return',
                        'presets': [
                            {'label': '📈 Weekly Winners', 'min': 20, 'max': None, 'description': '>20% 7D'},
                            {'label': '💪 Strong Week', 'min': 12, 'max': 20, 'description': '12-20% 7D'},
                            {'label': '✅ Good Week', 'min': 5, 'max': 12, 'description': '5-12% 7D'},
                            {'label': '😐 Flat Week', 'min': -5, 'max': 5, 'description': '-5% to 5% 7D'},
                            {'label': '😟 Bad Week', 'min': -12, 'max': -5, 'description': '-12% to -5% 7D'},
                            {'label': '💔 Terrible Week', 'min': None, 'max': -12, 'description': '<-12% 7D'}
                        ],
                        'slider_range': (-100, 100),
                        'slider_step': 1,
                        'default_min': -100,
                        'default_max': 100
                    },
                    'ret_30d': {
                        'name': 'Monthly Return',
                        'presets': [
                            {'label': '🏆 Monthly Champions', 'min': 30, 'max': None, 'description': '>30% 30D'},
                            {'label': '🎯 Top Performers', 'min': 20, 'max': 30, 'description': '20-30% 30D'},
                            {'label': '📈 Outperformers', 'min': 10, 'max': 20, 'description': '10-20% 30D'},
                            {'label': '🔄 Market Performers', 'min': -10, 'max': 10, 'description': '-10% to 10% 30D'},
                            {'label': '📉 Underperformers', 'min': -20, 'max': -10, 'description': '-20% to -10% 30D'},
                            {'label': '☠️ Monthly Losers', 'min': None, 'max': -20, 'description': '<-20% 30D'}
                        ],
                        'slider_range': (-100, 200),
                        'slider_step': 2,
                        'default_min': -100,
                        'default_max': 200
                    },
                    'ret_3m': {
                        'name': '3 Month Return',
                        'presets': [
                            {'label': '🎯 Quarterly Stars', 'min': 50, 'max': None, 'description': '>50% 3M'},
                            {'label': '⭐ Strong Quarter', 'min': 30, 'max': 50, 'description': '30-50% 3M'},
                            {'label': '👍 Good Quarter', 'min': 15, 'max': 30, 'description': '15-30% 3M'},
                            {'label': '➖ Flat Quarter', 'min': -15, 'max': 15, 'description': '-15% to 15% 3M'},
                            {'label': '👎 Weak Quarter', 'min': -30, 'max': -15, 'description': '-30% to -15% 3M'},
                            {'label': '🚨 Quarterly Disaster', 'min': None, 'max': -30, 'description': '<-30% 3M'}
                        ],
                        'slider_range': (-100, 300),
                        'slider_step': 5,
                        'default_min': -100,
                        'default_max': 300
                    },
                    'ret_6m': {
                        'name': '6 Month Return',
                        'presets': [
                            {'label': '💎 Half-Year Heroes', 'min': 80, 'max': None, 'description': '>80% 6M'},
                            {'label': '🌟 Semi-Annual Stars', 'min': 60, 'max': 80, 'description': '60-80% 6M'},
                            {'label': '📈 Strong Half', 'min': 30, 'max': 60, 'description': '30-60% 6M'},
                            {'label': '〰️ Sideways', 'min': -20, 'max': 20, 'description': '-20% to 20% 6M'},
                            {'label': '📉 Weak Half', 'min': -50, 'max': -20, 'description': '-50% to -20% 6M'},
                            {'label': '💀 Half-Year Collapse', 'min': None, 'max': -50, 'description': '<-50% 6M'}
                        ],
                        'slider_range': (-100, 500),
                        'slider_step': 10,
                        'default_min': -100,
                        'default_max': 500
                    },
                    'ret_1y': {
                        'name': '1 Year Return',
                        'presets': [
                            {'label': '🌙 Annual Winners', 'min': 100, 'max': None, 'description': '>100% 1Y'},
                            {'label': '🏅 Year Stars', 'min': 80, 'max': 100, 'description': '80-100% 1Y'},
                            {'label': '💪 Strong Year', 'min': 50, 'max': 80, 'description': '50-80% 1Y'},
                            {'label': '📊 Market Year', 'min': -30, 'max': 30, 'description': '-30% to 30% 1Y'},
                            {'label': '😔 Disappointing Year', 'min': -60, 'max': -30, 'description': '-60% to -30% 1Y'},
                            {'label': '🔴 Annual Disasters', 'min': None, 'max': -60, 'description': '<-60% 1Y'}
                        ],
                        'slider_range': (-100, 1000),
                        'slider_step': 10,
                        'default_min': -100,
                        'default_max': 1000
                    },
                    'ret_3y': {
                        'name': '3 Year Return',
                        'presets': [
                            {'label': '👑 Multi-Year Champions', 'min': 200, 'max': None, 'description': '>200% 3Y'},
                            {'label': '🚀 3Y Rockets', 'min': 150, 'max': 200, 'description': '150-200% 3Y'},
                            {'label': '⭐ 3Y Stars', 'min': 100, 'max': 150, 'description': '100-150% 3Y'},
                            {'label': '📈 3Y Growth', 'min': 0, 'max': 50, 'description': '0-50% 3Y'},
                            {'label': '📉 3Y Decline', 'min': -50, 'max': 0, 'description': '-50% to 0% 3Y'},
                            {'label': '💣 3Y Destruction', 'min': None, 'max': -50, 'description': '<-50% 3Y'}
                        ],
                        'slider_range': (-100, 2000),
                        'slider_step': 25,
                        'default_min': -100,
                        'default_max': 2000
                    },
                    'ret_5y': {
                        'name': '5 Year Return',
                        'presets': [
                            {'label': '🏛️ Long-Term Legends', 'min': 300, 'max': None, 'description': '>300% 5Y'},
                            {'label': '💎 5Y Diamonds', 'min': 250, 'max': 300, 'description': '250-300% 5Y'},
                            {'label': '🌟 5Y Winners', 'min': 150, 'max': 250, 'description': '150-250% 5Y'},
                            {'label': '📊 5Y Average', 'min': 0, 'max': 100, 'description': '0-100% 5Y'},
                            {'label': '⚠️ 5Y Laggards', 'min': -75, 'max': 0, 'description': '-75% to 0% 5Y'},
                            {'label': '☠️ 5Y Wipeout', 'min': None, 'max': -75, 'description': '<-75% 5Y'}
                        ],
                        'slider_range': (-100, 5000),
                        'slider_step': 50,
                        'default_min': -100,
                        'default_max': 5000
                    }
                }
                
                # Create single column layout for sequential order
                # Process each available return column in proper time sequence
                for ret_col in available_return_cols:
                    if ret_col in performance_config:
                        config = performance_config[ret_col]
                        
                        # Create preset options
                        preset_options = ["All Returns"] + [preset['label'] for preset in config['presets']] + ["🎯 Custom Range"]
                        
                        # Get current selection
                        current_selection = st.session_state.filter_state.get(f'{ret_col}_selection', "All Returns")
                        if current_selection not in preset_options:
                            current_selection = "All Returns"
                        
                        # Performance period dropdown
                        selection = st.selectbox(
                            config['name'],
                            options=preset_options,
                            index=preset_options.index(current_selection),
                            help=f"Filter stocks by {config['name'].lower()}",
                            key=f"{ret_col}_dropdown",
                            on_change=sync_performance_dropdowns
                        )
                        
                        # Update session state with current selection
                        st.session_state.filter_state[f'{ret_col}_selection'] = selection
                        
                        # Handle custom range selection
                        if selection == "🎯 Custom Range":
                            # Ensure proper type conversion for slider value parameter
                            default_value = (float(config['default_min']), float(config['default_max']))
                            current_value = st.session_state.filter_state.get(f'{ret_col}_range', default_value)
                            
                            # Ensure current_value is also a tuple of floats
                            if isinstance(current_value, tuple) and len(current_value) == 2:
                                current_value = (float(current_value[0]), float(current_value[1]))
                            else:
                                current_value = default_value
                            
                            range_value = st.slider(
                                f"{config['name']} Range (%)",
                                min_value=float(config['slider_range'][0]),
                                max_value=float(config['slider_range'][1]),
                                value=current_value,
                                step=float(config['slider_step']),
                                help=f"Custom range for {config['name'].lower()}",
                                key=f"{ret_col}_range_slider",
                                on_change=sync_performance_sliders
                            )
                            filters[f'{ret_col}_range'] = range_value
                        elif selection != "All Returns":
                            # Handle preset selection
                            for preset in config['presets']:
                                if preset['label'] == selection:
                                    min_val = preset['min'] if preset['min'] is not None else config['default_min']
                                    max_val = preset['max'] if preset['max'] is not None else config['default_max']
                                    filters[f'{ret_col}_range'] = (min_val, max_val)
                                    break
            else:
                st.info("📊 No return data available for performance filtering")
        
        # 🧠 Intelligence Filter - Combined Section
        with st.expander("🧠 Intelligence Filter", expanded=False):
            # VMI (Volume Momentum Index) Filter
            if 'vmi_tier' in ranked_df_display.columns or 'vmi' in ranked_df_display.columns:
                vmi_tier_options = list(CONFIG.TIERS['vmi_tiers'].keys()) + ["🎯 Custom VMI Range"]
                vmi_tiers = st.multiselect(
                    "VMI (Volume Momentum Index) Tiers",
                    options=vmi_tier_options,
                    default=st.session_state.filter_state.get('vmi_tiers', []),
                    key='vmi_tier_multiselect_intelligence',
                    on_change=sync_vmi_tier,
                    help="Volume Momentum Index: Weighted volume trend score classification"
                )
                
                if vmi_tiers:
                    filters['vmi_tiers'] = vmi_tiers
                
                # Show custom VMI range slider when "🎯 Custom VMI Range" is selected
                custom_vmi_range_selected = any("Custom VMI Range" in tier for tier in vmi_tiers) if vmi_tiers else False
                if custom_vmi_range_selected:
                    st.write("📊 **Custom VMI Range Filter**")
                    
                    vmi_range = st.slider(
                        "VMI Range",
                        min_value=0.0,
                        max_value=5.0,
                        value=(0.5, 3.0),
                        step=0.1,
                        key='custom_vmi_range_intelligence',
                        help="VMI typically ranges from 0.1 (very low volume) to 3.0+ (very high volume)"
                    )
                    filters['custom_vmi_range'] = vmi_range
            
            # Momentum Harmony Filter  
            if 'momentum_harmony_tier' in ranked_df_display.columns or 'momentum_harmony' in ranked_df_display.columns:
                momentum_harmony_tier_options = list(CONFIG.TIERS['momentum_harmony_tiers'].keys())
                momentum_harmony_tiers = st.multiselect(
                    "Momentum Harmony Tiers",
                    options=momentum_harmony_tier_options,
                    default=st.session_state.filter_state.get('momentum_harmony_tiers', []),
                    key='momentum_harmony_tier_multiselect_intelligence',
                    on_change=sync_momentum_harmony_tier,
                    help="Multi-timeframe momentum alignment: 0-4 score showing consistency across periods"
                )
                
                if momentum_harmony_tiers:
                    filters['momentum_harmony_tiers'] = momentum_harmony_tiers
            
            #  Volume Intelligence
            if 'volume_tier' in ranked_df_display.columns or 'rvol' in ranked_df_display.columns:
                # Volume tier multiselect with custom range option
                volume_tier_options = list(CONFIG.TIERS['volume_tiers'].keys()) + ["🎯 Custom RVOL Range"]
                volume_tiers = st.multiselect(
                    "Volume Activity Tiers",
                    options=volume_tier_options,
                    default=st.session_state.filter_state.get('volume_tiers', []),
                    key='volume_tier_multiselect_intelligence',
                    on_change=sync_volume_tier,
                    help="Select volume activity tiers or use Custom RVOL Range for precise control"
                )
                
                if volume_tiers:
                    filters['volume_tiers'] = volume_tiers
                
                # Show custom RVOL range slider when "🎯 Custom RVOL Range" is selected
                custom_rvol_range_selected = any("Custom RVOL Range" in tier for tier in volume_tiers) if volume_tiers else False
                if custom_rvol_range_selected:
                    st.write("📊 **Custom RVOL Range Filter**")
                    
                    rvol_range = st.slider(
                        "RVOL Range",
                        min_value=0.1,
                        max_value=20.0,
                        value=st.session_state.filter_state.get('rvol_range', (0.1, 20.0)),
                        step=0.1,
                        help="Filter by Relative Volume (RVOL) range",
                        key="rvol_range_slider",
                        on_change=sync_rvol_range
                    )
                    if rvol_range != (0.1, 20.0):
                        filters['rvol_range'] = rvol_range
            
            # 🎯 Position Intelligence
            if 'position_tier' in ranked_df_display.columns:
                # Position tier multiselect with custom range option
                position_tier_options = list(CONFIG.TIERS['position_tiers'].keys()) + ["🎯 Custom Position Range"]
                position_tiers = st.multiselect(
                    "Position Tiers",
                    options=position_tier_options,
                    default=st.session_state.filter_state.get('position_tiers', []),
                    key='position_tier_multiselect_intelligence',
                    on_change=sync_position_tier,
                    help="Select position tiers or use Custom Position Range for precise control"
                )
                
                if position_tiers:
                    filters['position_tiers'] = position_tiers
                
                # Show custom position range slider when "🎯 Custom Position Range" is selected
                custom_position_range_selected = any("Custom Position Range" in tier for tier in position_tiers) if position_tiers else False
                if custom_position_range_selected:
                    st.write("📊 **Custom Position Range Filter**")
                    
                    position_range = st.slider(
                        "Position Range (%)",
                        min_value=0,
                        max_value=100,
                        value=st.session_state.filter_state.get('position_range', (0, 100)),
                        step=1,
                        help="Filter by position percentage range (distance from 52-week low)",
                        key="position_range_slider",
                        on_change=sync_position_range
                    )
                    if position_range != (0, 100):
                        filters['position_range'] = position_range
            
        
        # 💧 Turnover Filter - Dedicated Section
        with st.expander("💧 Turnover Filter", expanded=False):
            
            # Daily Turnover
            if 'daily_turnover_tier' in ranked_df_display.columns:
                turnover_tier_options = list(CONFIG.TIERS['daily_turnover_tiers'].keys())
                turnover_tiers = st.multiselect(
                    "📊 Daily Turnover Tiers",
                    options=turnover_tier_options,
                    default=st.session_state.filter_state.get('turnover_tiers', []),
                    key='turnover_tier_multiselect_dedicated',
                    on_change=lambda: st.session_state.filter_state.update({'turnover_tiers': st.session_state.turnover_tier_multiselect_dedicated}),
                    help="📊 Filter by daily turnover tiers"
                )
                
                if turnover_tiers:
                    filters['turnover_tiers'] = turnover_tiers
            
            # 30-Day Turnover
            if 'turnover_30d_tier' in ranked_df_display.columns:
                turnover_30d_tier_options = list(CONFIG.TIERS['daily_turnover_tiers'].keys())
                turnover_30d_tiers = st.multiselect(
                    "📈 30-Day Turnover Tiers",
                    options=turnover_30d_tier_options,
                    default=st.session_state.filter_state.get('turnover_30d_tiers', []),
                    key='turnover_30d_tier_multiselect_dedicated',
                    on_change=lambda: st.session_state.filter_state.update({'turnover_30d_tiers': st.session_state.turnover_30d_tier_multiselect_dedicated}),
                    help="📈 Filter by 30-day turnover tiers"
                )
                
                if turnover_30d_tiers:
                    filters['turnover_30d_tiers'] = turnover_30d_tiers
            
            # 90-Day Turnover
            if 'turnover_90d_tier' in ranked_df_display.columns:
                turnover_90d_tier_options = list(CONFIG.TIERS['daily_turnover_tiers'].keys())
                turnover_90d_tiers = st.multiselect(
                    "📊 90-Day Turnover Tiers",
                    options=turnover_90d_tier_options,
                    default=st.session_state.filter_state.get('turnover_90d_tiers', []),
                    key='turnover_90d_tier_multiselect_dedicated',
                    on_change=lambda: st.session_state.filter_state.update({'turnover_90d_tiers': st.session_state.turnover_90d_tier_multiselect_dedicated}),
                    help="📊 Filter by 90-day turnover tiers"
                )
                
                if turnover_90d_tiers:
                    filters['turnover_90d_tiers'] = turnover_90d_tiers
            
            # 180-Day Turnover
            if 'turnover_180d_tier' in ranked_df_display.columns:
                turnover_180d_tier_options = list(CONFIG.TIERS['daily_turnover_tiers'].keys())
                turnover_180d_tiers = st.multiselect(
                    "📉 180-Day Turnover Tiers",
                    options=turnover_180d_tier_options,
                    default=st.session_state.filter_state.get('turnover_180d_tiers', []),
                    key='turnover_180d_tier_multiselect_dedicated',
                    on_change=lambda: st.session_state.filter_state.update({'turnover_180d_tiers': st.session_state.turnover_180d_tier_multiselect_dedicated}),
                    help="📉 Filter by 180-day turnover tiers"
                )
                
                if turnover_180d_tiers:
                    filters['turnover_180d_tiers'] = turnover_180d_tiers
        
        
        # 🌊 LIQUIDITY GROWTH ANALYTICS FILTER - Advanced Multi-Period Analysis
        with st.expander("🌊 Liquidity Growth Analytics", expanded=False):
            
            # 🏆 Advanced VQS Grade Filter (Multi-Dimensional Volume Quality)
            if 'vqs_grade' in ranked_df_display.columns:
                st.markdown("**🏆 Advanced Volume Quality Score (VQS) Grade**")
                
                vqs_grade_options = [
                    "A+ - 👑 Elite Quality (90+)",
                    "A - 🌟 Excellent Quality (80-89)",
                    "B - ✅ Strong Quality (70-79)",
                    "C - ⚪ Average Quality (60-69)",
                    "D - ⚠️ Below Average (50-59)",
                    "F - ❌ Poor Quality (<50)"
                ]
                
                vqs_grades = st.multiselect(
                    "Filter by Advanced VQS Grade",
                    options=vqs_grade_options,
                    default=st.session_state.filter_state.get('vqs_grades', []),
                    key='vqs_grade_multiselect',
                    on_change=lambda: st.session_state.filter_state.update({'vqs_grades': st.session_state.vqs_grade_multiselect}),
                    help="🏆 Filter by Advanced VQS grade. Multi-dimensional assessment: Base Score (multi-timeframe) × Quality Multiplier (smart money) × Market Adjustment (regime) × Risk Factor (liquidity) × Confidence. Uses 30+ data columns."
                )
                
                if vqs_grades:
                    # Extract letter grades from selected options (e.g., "A+ - 👑 Elite Quality (90+)" -> "A+")
                    selected_letter_grades = [grade.split(" - ")[0] for grade in vqs_grades]
                    filters['vqs_grades'] = selected_letter_grades
                
                st.caption("💡 **Tip:** Combine Advanced VQS A+/A with Advanced VER for institutional-grade stocks")
                st.markdown("---")
            
            # ⚡ Advanced VER Grade Filter (Multi-Dimensional Volume Efficiency)
            if 'advanced_ver_grade' in ranked_df_display.columns:
                st.markdown("**⚡ Advanced Volume Efficiency Ratio (VER) Grade**")
                
                advanced_ver_grade_options = [
                    "A+ - 🚀 Extreme (>10.0)",
                    "A - 🔥 Elite+ (>5.0)",
                    "B - ✅ Strong (>2.5)",
                    "C - ⚪ Good (>1.5)",
                    "D - ⚠️ Fair (>0.8)",
                    "F - 🚨 Poor (<0.8)"
                ]
                
                advanced_ver_grades = st.multiselect(
                    "Filter by Advanced VER Grade",
                    options=advanced_ver_grade_options,
                    default=st.session_state.filter_state.get('advanced_ver_grades', []),
                    key='advanced_ver_grade_multiselect',
                    on_change=lambda: st.session_state.filter_state.update({'advanced_ver_grades': st.session_state.advanced_ver_grade_multiselect}),
                    help="⚡ Filter by Advanced Volume Efficiency Ratio grade. Advanced VER is a multi-dimensional metric combining Base VER (multi-timeframe) × Quality Multiplier (consistency, VMI, harmony) × Regime Adjustment (market state, volatility) × Risk Factor (money flow, smart money, liquidity). This sophisticated algorithm uses 30+ data columns for institutional-grade volume efficiency assessment."
                )
                
                if advanced_ver_grades:
                    # Extract letter grades from selected options (e.g., "A+ - 🚀 Extreme (>10.0)" -> "A+")
                    selected_letter_grades = [grade.split(" - ")[0] for grade in advanced_ver_grades]
                    filters['advanced_ver_grades'] = selected_letter_grades
                
                st.caption("💡 **Tip:** A+ and A grades indicate institutional-quality efficiency with supply shock potential")
                st.caption("🎯 **Pro Tip:** Combine Advanced VER A+/A with VQS A/B and STRONG_UPTREND for elite opportunities")
                st.markdown("---")
            
            # Growth Trend Classification Filter with Custom Range
            if 'growth_trend' in ranked_df_display.columns:
                st.markdown("**📈 Growth Trend Classification**")
                
                growth_trend_options = {
                    "All Trends": None,
                    "🌊💎 Elite Momentum": "🌊💎 Elite Momentum",
                    "🚀 Explosive Growth": "🚀 Explosive Growth",
                    "📈 Strong Growth": "📈 Strong Growth",
                    "✅ Steady Growth": "✅ Steady Growth",
                    "⚡ Rapidly Accelerating": "⚡ Rapidly Accelerating",
                    "⚡ Accelerating": "⚡ Accelerating",
                    "➡️ Stable Growth": "➡️ Stable Growth",
                    "📉 Strong Decline": "📉 Strong Decline",
                    "📉 Declining": "📉 Declining",
                    "⚠️ Weak": "⚠️ Weak",
                    "⚪ Mixed": "⚪ Mixed",
                    "🎯 Custom Range": None  # Special option for custom range
                }
                
                current_trend_selection = st.session_state.filter_state.get('growth_trend_selection', "All Trends")
                if current_trend_selection not in growth_trend_options:
                    current_trend_selection = "All Trends"
                
                # Custom sync function for growth trend with custom range support
                def sync_growth_trend_with_custom():
                    if 'growth_trend_selectbox' in st.session_state:
                        selected = st.session_state.growth_trend_selectbox
                        st.session_state.filter_state['growth_trend_selection'] = selected
                
                def sync_growth_trend_custom_slider():
                    if 'growth_trend_custom_range_slider' in st.session_state:
                        st.session_state.filter_state['growth_trend_custom_range'] = st.session_state.growth_trend_custom_range_slider
                
                selected_growth_trend = st.selectbox(
                    "📈 Growth Trend Quality",
                    options=list(growth_trend_options.keys()),
                    index=list(growth_trend_options.keys()).index(current_trend_selection),
                    help="Filter stocks by liquidity growth trend patterns. Choose Custom Range for precise growth score control.",
                    key="growth_trend_selectbox",
                    on_change=sync_growth_trend_with_custom
                )
                
                # Show custom range slider when Custom Range is selected
                if selected_growth_trend == "🎯 Custom Range":
                    # Get current custom range from session state, default to (0, 100)
                    current_custom_range = st.session_state.filter_state.get('growth_trend_custom_range', (0.0, 100.0))
                    
                    custom_range = st.slider(
                        "Custom Growth Score Range",
                        min_value=0.0,
                        max_value=100.0,
                        value=current_custom_range,
                        step=5.0,
                        help="🚀 85+: Explosive | 📈 70-84: Strong | ✅ 55-69: Steady | ➡️ 40-54: Stable | 📉 25-39: Declining | ⚪ <25: Weak",
                        key="growth_trend_custom_range_slider",
                        on_change=sync_growth_trend_custom_slider
                    )
                    filters['growth_trend_custom_range'] = custom_range
                elif selected_growth_trend != "All Trends":
                    # Store the selected preset trend
                    filters['growth_trends'] = [growth_trend_options[selected_growth_trend]]
            
            # Growth Quality Tier Filter
            if 'growth_quality_tier' in ranked_df_display.columns:
                growth_quality_options = [
                    "💎 Elite",
                    "🌟 Excellent", 
                    "✅ Good",
                    "⚪ Average",
                    "⚠️ Weak"
                ]
                growth_quality_tiers = st.multiselect(
                    "💎 Growth Quality Tiers",
                    options=growth_quality_options,
                    default=st.session_state.filter_state.get('growth_quality_tiers', []),
                    key='growth_quality_tier_multiselect',
                    on_change=lambda: st.session_state.filter_state.update({'growth_quality_tiers': st.session_state.growth_quality_tier_multiselect}),
                    help="⭐ Filter by composite growth quality rating"
                )
                
                if growth_quality_tiers:
                    filters['growth_quality_tiers'] = growth_quality_tiers
            
            # Smart Money Flow Filter
            if 'smart_money_flow' in ranked_df_display.columns:
                smart_money_options = [
                    "🌊💎 Elite Institutional Accumulation",
                    "🎯 Strong Accumulation",
                    "✅ Moderate Accumulation",
                    "📊 Mild Interest",
                    "⚪ Neutral",
                    "❌ Distribution",
                    "⚠️ Weak Distribution"
                ]
                smart_money_flows = st.multiselect(
                    "💰 Smart Money Flow Detection",
                    options=smart_money_options,
                    default=st.session_state.filter_state.get('smart_money_flows', []),
                    key='smart_money_flow_multiselect',
                    on_change=lambda: st.session_state.filter_state.update({'smart_money_flows': st.session_state.smart_money_flow_multiselect}),
                    help="🎯 Filter by institutional/smart money accumulation signals"
                )
                
                if smart_money_flows:
                    filters['smart_money_flows'] = smart_money_flows
            
            # Growth Momentum Filter with Custom Range
            if 'growth_momentum' in ranked_df_display.columns:
                st.markdown("**⚡ Growth Momentum**")
                
                momentum_options = {
                    "All Momentum": None,
                    "🚀 Explosive Acceleration (50+)": (50.0, 10000.0),
                    "📈 Strong Acceleration (25-50)": (25.0, 50.0),
                    "✅ Moderate Acceleration (10-25)": (10.0, 25.0),
                    "⚡ Mild Acceleration (0-10)": (0.0, 10.0),
                    "➡️ Stable (-10 to 0)": (-10.0, 0.0),
                    "📉 Mild Deceleration (-25 to -10)": (-25.0, -10.0),
                    "⚠️ Strong Deceleration (-50 to -25)": (-50.0, -25.0),
                    "❌ Sharp Deceleration (<-50)": (-100.0, -50.0),
                    "🎯 Custom Range": None  # Special option for custom range
                }
                
                current_momentum_selection = st.session_state.filter_state.get('momentum_selection', "All Momentum")
                if current_momentum_selection not in momentum_options:
                    current_momentum_selection = "All Momentum"
                
                # Custom sync function for momentum with custom range support
                def sync_momentum_with_custom():
                    if 'momentum_selectbox' in st.session_state:
                        selected = st.session_state.momentum_selectbox
                        st.session_state.filter_state['momentum_selection'] = selected
                        
                        if selected == "🎯 Custom Range":
                            # Don't set range here, will be set by slider
                            pass
                        elif selected != "All Momentum":
                            st.session_state.filter_state['momentum_range'] = momentum_options[selected]
                
                def sync_momentum_custom_slider():
                    if 'momentum_custom_range_slider' in st.session_state:
                        st.session_state.filter_state['momentum_custom_range'] = st.session_state.momentum_custom_range_slider
                
                selected_momentum = st.selectbox(
                    "Momentum Quality",
                    options=list(momentum_options.keys()),
                    index=list(momentum_options.keys()).index(current_momentum_selection),
                    help="Filter stocks by growth momentum (acceleration/deceleration). Choose Custom Range for precise control.",
                    key="momentum_selectbox",
                    on_change=sync_momentum_with_custom
                )
                
                # Show custom range slider when Custom Range is selected
                if selected_momentum == "🎯 Custom Range":
                    # Get current custom range from session state, default to (-100, 100)
                    current_custom_range = st.session_state.filter_state.get('momentum_custom_range', (-100.0, 100.0))
                    
                    custom_range = st.slider(
                        "Custom Momentum Range (%)",
                        min_value=-100.0,
                        max_value=100.0,
                        value=current_custom_range,
                        step=5.0,
                        help="🚀 50+: Explosive | 📈 25-50: Strong | ✅ 10-25: Moderate | ⚡ 0-10: Mild | ➡️ -10-0: Stable | 📉 -25--10: Mild Decel | ⚠️ -50--25: Strong Decel | ❌ <-50: Sharp Decel",
                        key="momentum_custom_range_slider",
                        on_change=sync_momentum_custom_slider
                    )
                    filters['momentum_custom_range'] = custom_range
                elif selected_momentum != "All Momentum":
                    # Store the selected preset range
                    filters['momentum_range'] = momentum_options[selected_momentum]
            
            # Growth Consistency Filter with Custom Range
            if 'growth_consistency' in ranked_df_display.columns:
                st.markdown("**🎯 Growth Consistency**")
                
                consistency_options = {
                    "All Consistency": None,
                    "💎 Exceptional (85+)": (85.0, 100.0),
                    "🌟 Excellent (70-84)": (70.0, 84.0),
                    "✅ Good (55-69)": (55.0, 69.0),
                    "⚪ Average (40-54)": (40.0, 54.0),
                    "⚠️ Below Average (25-39)": (25.0, 39.0),
                    "❌ Poor (<25)": (0.0, 24.0),
                    "🎯 Custom Range": None  # Special option for custom range
                }
                
                current_consistency_selection = st.session_state.filter_state.get('consistency_selection', "All Consistency")
                if current_consistency_selection not in consistency_options:
                    current_consistency_selection = "All Consistency"
                
                # Custom sync function for consistency with custom range support
                def sync_consistency_with_custom():
                    if 'consistency_selectbox' in st.session_state:
                        selected = st.session_state.consistency_selectbox
                        st.session_state.filter_state['consistency_selection'] = selected
                        
                        if selected == "🎯 Custom Range":
                            # Don't set range here, will be set by slider
                            pass
                        elif selected != "All Consistency":
                            st.session_state.filter_state['consistency_range'] = consistency_options[selected]
                
                def sync_consistency_custom_slider():
                    if 'consistency_custom_range_slider' in st.session_state:
                        st.session_state.filter_state['consistency_custom_range'] = st.session_state.consistency_custom_range_slider
                
                selected_consistency = st.selectbox(
                    "Consistency Quality",
                    options=list(consistency_options.keys()),
                    index=list(consistency_options.keys()).index(current_consistency_selection),
                    help="Filter stocks by growth pattern consistency across time periods. Choose Custom Range for precise control.",
                    key="consistency_selectbox",
                    on_change=sync_consistency_with_custom
                )
                
                # Show custom range slider when Custom Range is selected
                if selected_consistency == "🎯 Custom Range":
                    # Get current custom range from session state, default to (0, 100)
                    current_custom_range = st.session_state.filter_state.get('consistency_custom_range', (0.0, 100.0))
                    
                    custom_range = st.slider(
                        "Custom Consistency Range (%)",
                        min_value=0.0,
                        max_value=100.0,
                        value=current_custom_range,
                        step=5.0,
                        help="💎 85+: Exceptional | 🌟 70-84: Excellent | ✅ 55-69: Good | ⚪ 40-54: Average | ⚠️ 25-39: Below Average | ❌ <25: Poor",
                        key="consistency_custom_range_slider",
                        on_change=sync_consistency_custom_slider
                    )
                    filters['consistency_custom_range'] = custom_range
                elif selected_consistency != "All Consistency":
                    # Store the selected preset range
                    filters['consistency_range'] = consistency_options[selected_consistency]
            
            st.markdown("---")
            st.markdown("**📊 Timeframe Momentum Filters**")
            st.caption("⚡ Positive % = Recent activity HIGHER than historical (Accelerating) | 📉 Negative % = Recent LOWER than historical (Decelerating)")
            
            # Daily vs 30-Day Turnover Growth Filter
            if 'growth_daily_to_30' in ranked_df_display.columns:
                daily_30_options = {
                    "All Momentum": None,
                    "🌊💎 Elite (500+%)": (500.0, 10000.0),
                    "🚀 Explosive (200-500%)": (200.0, 500.0),
                    "📈 Very Strong (100-200%)": (100.0, 200.0),
                    "✅ Strong (50-100%)": (50.0, 100.0),
                    "⚡ Moderate (25-50%)": (25.0, 50.0),
                    "➡️ Mild (0-25%)": (0.0, 25.0),
                    "📉 Decelerating (< 0%)": (-1000.0, 0.0),
                    "🎯 Custom Range": None
                }
                
                current_daily_30_selection = st.session_state.filter_state.get('growth_daily_30_selection', "All Momentum")
                if current_daily_30_selection not in daily_30_options:
                    current_daily_30_selection = "All Momentum"
                
                def sync_daily_30():
                    if 'growth_daily_30_selectbox' in st.session_state:
                        selected = st.session_state.growth_daily_30_selectbox
                        st.session_state.filter_state['growth_daily_30_selection'] = selected
                        if selected != "All Momentum" and selected != "🎯 Custom Range":
                            st.session_state.filter_state['growth_daily_30_range'] = daily_30_options[selected]
                
                def sync_daily_30_custom():
                    if 'growth_daily_30_custom_slider' in st.session_state:
                        st.session_state.filter_state['growth_daily_30_custom_range'] = st.session_state.growth_daily_30_custom_slider
                
                selected_daily_30 = st.selectbox(
                    "📊 Daily vs 30-Day Growth",
                    options=list(daily_30_options.keys()),
                    index=list(daily_30_options.keys()).index(current_daily_30_selection),
                    help="Filter by momentum: How much higher/lower is daily turnover vs 30-day average. Positive = Current activity exceeds recent average (accelerating)",
                    key="growth_daily_30_selectbox",
                    on_change=sync_daily_30
                )
                
                if selected_daily_30 == "🎯 Custom Range":
                    current_custom_range = st.session_state.filter_state.get('growth_daily_30_custom_range', (-100.0, 200.0))
                    custom_range = st.slider(
                        "Custom Daily→30D Range (%)",
                        min_value=-100.0,
                        max_value=500.0,
                        value=current_custom_range,
                        step=5.0,
                        help="Custom range for Daily vs 30-Day turnover growth",
                        key="growth_daily_30_custom_slider",
                        on_change=sync_daily_30_custom
                    )
                    filters['growth_daily_30_custom_range'] = custom_range
                elif selected_daily_30 != "All Momentum":
                    filters['growth_daily_30_range'] = daily_30_options[selected_daily_30]
            
            # 30-Day vs 90-Day Turnover Growth Filter
            if 'growth_30_to_90' in ranked_df_display.columns:
                thirty_90_options = {
                    "All Momentum": None,
                    "🌊💎 Elite (200+%)": (200.0, 10000.0),
                    "🚀 Explosive (100-200%)": (100.0, 200.0),
                    "📈 Very Strong (50-100%)": (50.0, 100.0),
                    "✅ Strong (25-50%)": (25.0, 50.0),
                    "⚡ Moderate (10-25%)": (10.0, 25.0),
                    "➡️ Mild (0-10%)": (0.0, 10.0),
                    "📉 Decelerating (< 0%)": (-1000.0, 0.0),
                    "🎯 Custom Range": None
                }
                
                current_30_90_selection = st.session_state.filter_state.get('growth_30_90_selection', "All Momentum")
                if current_30_90_selection not in thirty_90_options:
                    current_30_90_selection = "All Momentum"
                
                def sync_30_90():
                    if 'growth_30_90_selectbox' in st.session_state:
                        selected = st.session_state.growth_30_90_selectbox
                        st.session_state.filter_state['growth_30_90_selection'] = selected
                        if selected != "All Momentum" and selected != "🎯 Custom Range":
                            st.session_state.filter_state['growth_30_90_range'] = thirty_90_options[selected]
                
                def sync_30_90_custom():
                    if 'growth_30_90_custom_slider' in st.session_state:
                        st.session_state.filter_state['growth_30_90_custom_range'] = st.session_state.growth_30_90_custom_slider
                
                selected_30_90 = st.selectbox(
                    "📈 30-Day vs 90-Day Momentum",
                    options=list(thirty_90_options.keys()),
                    index=list(thirty_90_options.keys()).index(current_30_90_selection),
                    help="Filter by momentum: How much higher/lower is 30-day average vs 90-day average. Positive = Recent activity exceeds historical (accelerating)",
                    key="growth_30_90_selectbox",
                    on_change=sync_30_90
                )
                
                if selected_30_90 == "🎯 Custom Range":
                    current_custom_range = st.session_state.filter_state.get('growth_30_90_custom_range', (-100.0, 200.0))
                    custom_range = st.slider(
                        "Custom 30D→90D Range (%)",
                        min_value=-100.0,
                        max_value=500.0,
                        value=current_custom_range,
                        step=5.0,
                        help="Custom range for 30-Day vs 90-Day turnover growth",
                        key="growth_30_90_custom_slider",
                        on_change=sync_30_90_custom
                    )
                    filters['growth_30_90_custom_range'] = custom_range
                elif selected_30_90 != "All Momentum":
                    filters['growth_30_90_range'] = thirty_90_options[selected_30_90]
            
            # 90-Day vs 180-Day Turnover Growth Filter
            if 'growth_90_to_180' in ranked_df_display.columns:
                ninety_180_options = {
                    "All Momentum": None,
                    "🌊💎 Elite (150+%)": (150.0, 10000.0),
                    "🚀 Explosive (75-150%)": (75.0, 150.0),
                    "📈 Very Strong (40-75%)": (40.0, 75.0),
                    "✅ Strong (20-40%)": (20.0, 40.0),
                    "⚡ Moderate (10-20%)": (10.0, 20.0),
                    "➡️ Mild (0-10%)": (0.0, 10.0),
                    "📉 Decelerating (< 0%)": (-1000.0, 0.0),
                    "🎯 Custom Range": None
                }
                
                current_90_180_selection = st.session_state.filter_state.get('growth_90_180_selection', "All Momentum")
                if current_90_180_selection not in ninety_180_options:
                    current_90_180_selection = "All Momentum"
                
                def sync_90_180():
                    if 'growth_90_180_selectbox' in st.session_state:
                        selected = st.session_state.growth_90_180_selectbox
                        st.session_state.filter_state['growth_90_180_selection'] = selected
                        if selected != "All Momentum" and selected != "🎯 Custom Range":
                            st.session_state.filter_state['growth_90_180_range'] = ninety_180_options[selected]
                
                def sync_90_180_custom():
                    if 'growth_90_180_custom_slider' in st.session_state:
                        st.session_state.filter_state['growth_90_180_custom_range'] = st.session_state.growth_90_180_custom_slider
                
                selected_90_180 = st.selectbox(
                    "📊 90-Day vs 180-Day Momentum",
                    options=list(ninety_180_options.keys()),
                    index=list(ninety_180_options.keys()).index(current_90_180_selection),
                    help="Filter by momentum: How much higher/lower is 90-day average vs 180-day average. Positive = Medium-term activity exceeds long-term (accelerating)",
                    key="growth_90_180_selectbox",
                    on_change=sync_90_180
                )
                
                if selected_90_180 == "🎯 Custom Range":
                    current_custom_range = st.session_state.filter_state.get('growth_90_180_custom_range', (-100.0, 200.0))
                    custom_range = st.slider(
                        "Custom 90D→180D Range (%)",
                        min_value=-100.0,
                        max_value=500.0,
                        value=current_custom_range,
                        step=5.0,
                        help="Custom range for 90-Day vs 180-Day turnover growth",
                        key="growth_90_180_custom_slider",
                        on_change=sync_90_180_custom
                    )
                    filters['growth_90_180_custom_range'] = custom_range
                elif selected_90_180 != "All Momentum":
                    filters['growth_90_180_range'] = ninety_180_options[selected_90_180]

        
        # Advanced filters with callbacks
        with st.expander("🔧 Advanced Filters"):
            # Define callbacks for advanced filters
            def sync_eps_tier():
                if 'eps_tier_multiselect' in st.session_state:
                    st.session_state.filter_state['eps_tiers'] = st.session_state.eps_tier_multiselect
            
            def sync_pe_tier():
                if 'pe_tier_multiselect' in st.session_state:
                    st.session_state.filter_state['pe_tiers'] = st.session_state.pe_tier_multiselect
            
            def sync_price_tier():
                if 'price_tier_multiselect' in st.session_state:
                    st.session_state.filter_state['price_tiers'] = st.session_state.price_tier_multiselect
            
            def sync_eps_change_tier():
                if 'eps_change_tiers_widget' in st.session_state:
                    st.session_state.filter_state['eps_change_tiers'] = st.session_state.eps_change_tiers_widget
            
            def sync_min_pe():
                if 'min_pe_input' in st.session_state:
                    value = st.session_state.min_pe_input
                    if value.strip():
                        try:
                            st.session_state.filter_state['min_pe'] = float(value)
                        except ValueError:
                            st.session_state.filter_state['min_pe'] = None
                    else:
                        st.session_state.filter_state['min_pe'] = None
            
            def sync_max_pe():
                if 'max_pe_input' in st.session_state:
                    value = st.session_state.max_pe_input
                    if value.strip():
                        try:
                            st.session_state.filter_state['max_pe'] = float(value)
                        except ValueError:
                            st.session_state.filter_state['max_pe'] = None
                    else:
                        st.session_state.filter_state['max_pe'] = None
            
            def sync_fundamental():
                if 'require_fundamental_checkbox' in st.session_state:
                    st.session_state.filter_state['require_fundamental_data'] = st.session_state.require_fundamental_checkbox
            
            # Tier filters
            for tier_type, col_name, filter_key, sync_func in [
                ('eps_tiers', 'eps_tier', 'eps_tiers', sync_eps_tier),
                ('pe_tiers', 'pe_tier', 'pe_tiers', sync_pe_tier),
                ('price_tiers', 'price_tier', 'price_tiers', sync_price_tier)
            ]:
                if col_name in ranked_df_display.columns:
                    tier_options = FilterEngine.get_filter_options(ranked_df_display, col_name, filters)
                    
                    selected_tiers = st.multiselect(
                        f"{col_name.replace('_', ' ').title()}",
                        options=tier_options,
                        default=st.session_state.filter_state.get(filter_key, []),
                        placeholder=f"Select {col_name.replace('_', ' ')}s (empty = All)",
                        key=f"{col_name}_multiselect",
                        on_change=sync_func  # SYNC ON CHANGE
                    )
                    
                    if selected_tiers:
                        filters[tier_type] = selected_tiers
            
            # EPS change tier filter
            if 'eps_change_tier' in ranked_df_display.columns:
                # EPS Change Tier Filter
                eps_change_tiers = st.multiselect(
                    "EPS Change Tier",
                    options=list(CONFIG.TIERS['eps_change_pct'].keys()),
                    default=st.session_state.filter_state.get('eps_change_tiers', []),
                    key='eps_change_tiers_widget',
                    on_change=sync_eps_change_tier,
                    help="Select EPS change tiers to include"
                )
                
                if eps_change_tiers:
                    filters['eps_change_tiers'] = eps_change_tiers
            
            # PE filters (only in hybrid mode)
            if show_fundamentals and 'pe' in ranked_df_display.columns:
                st.markdown("**🔍 Fundamental Filters**")
                
                col1, col2 = st.columns(2)
                with col1:
                    current_min_pe = st.session_state.filter_state.get('min_pe')
                    min_pe_str = str(current_min_pe) if current_min_pe is not None else ""
                    
                    min_pe_input = st.text_input(
                        "Min PE Ratio",
                        value=min_pe_str,
                        placeholder="e.g. 10",
                        key="min_pe_input",
                        on_change=sync_min_pe  # SYNC ON CHANGE
                    )
                    
                    if min_pe_input.strip():
                        try:
                            min_pe_val = float(min_pe_input)
                            filters['min_pe'] = min_pe_val
                        except ValueError:
                            st.error("Invalid Min PE")
                
                with col2:
                    current_max_pe = st.session_state.filter_state.get('max_pe')
                    max_pe_str = str(current_max_pe) if current_max_pe is not None else ""
                    
                    max_pe_input = st.text_input(
                        "Max PE Ratio",
                        value=max_pe_str,
                        placeholder="e.g. 30",
                        key="max_pe_input",
                        on_change=sync_max_pe  # SYNC ON CHANGE
                    )
                    
                    if max_pe_input.strip():
                        try:
                            max_pe_val = float(max_pe_input)
                            filters['max_pe'] = max_pe_val
                        except ValueError:
                            st.error("Invalid Max PE")
                
                # Data completeness filter
                require_fundamental = st.checkbox(
                    "Only show stocks with PE and EPS data",
                    value=st.session_state.filter_state.get('require_fundamental_data', False),
                    key="require_fundamental_checkbox",
                    on_change=sync_fundamental  # SYNC ON CHANGE
                )
                
                if require_fundamental:
                    filters['require_fundamental_data'] = True
        
        # Count active filters using FilterEngine method
        active_filter_count = FilterEngine.get_active_count()
        st.session_state.active_filter_count = active_filter_count
        
        if active_filter_count > 0:
            st.info(f"🔍 **{active_filter_count} filter{'s' if active_filter_count > 1 else ''} active**")
        
        # Clear filters button - ENHANCED VERSION
        if st.button("🗑️ Clear All Filters", 
                    width="stretch", 
                    type="primary" if active_filter_count > 0 else "secondary",
                    key="clear_filters_sidebar_btn"):
            
            # Use both FilterEngine and SessionStateManager clear methods
            FilterEngine.clear_all_filters()
            SessionStateManager.clear_filters()
            
            st.success("✅ All filters cleared!")
            time.sleep(0.3)
            st.rerun()
    
    # Apply filters (outside sidebar)
    if quick_filter_applied:
        filtered_df = FilterEngine.apply_filters(ranked_df_display, filters)
    else:
        filtered_df = FilterEngine.apply_filters(ranked_df, filters)
    
    filtered_df = filtered_df.sort_values('rank')
    
    # Save current filters
    st.session_state.user_preferences['last_filters'] = filters
    
    # Debug info (OPTIONAL)
    if show_debug:
        with st.sidebar.expander("🐛 Debug Info", expanded=True):
            st.write("**Active Filters:**")
            for key, value in filters.items():
                if value is not None and value != [] and value != 0 and \
                   (not (isinstance(value, tuple) and value == (0,100))):
                    st.write(f"• {key}: {value}")
            
            st.write(f"\n**Filter State:**")
            st.write(st.session_state.filter_state)
            
            st.write(f"\n**Filter Result:**")
            st.write(f"Before: {len(ranked_df)} stocks")
            st.write(f"After: {len(filtered_df)} stocks")
            
            if st.session_state.performance_metrics:
                st.write(f"\n**Performance:**")
                for func, time_taken in st.session_state.performance_metrics.items():
                    if time_taken > 0.001:
                        st.write(f"• {func}: {time_taken:.4f}s")
    
    active_filter_count = st.session_state.get('active_filter_count', 0)
    if quick_filter:
        filter_status_col1, filter_status_col2 = st.columns([5, 1])
        with filter_status_col1:
            quick_filter_names = {
                'top_gainers': '📈 Top Gainers',
                'volume_surges': '🔥 Volume Surges',
                'velocity_breakout': '🚀 High Velocity',
                'institutional_tsunami': '🌋 Tsunami'
            }
            filter_display = quick_filter_names.get(quick_filter, 'Filtered')
            
            if active_filter_count > 1:
                st.info(f"**Viewing:** {filter_display} + {active_filter_count - 1} other filter{'s' if active_filter_count > 2 else ''} | **{len(filtered_df):,} stocks** shown")
            else:
                st.info(f"**Viewing:** {filter_display} | **{len(filtered_df):,} stocks** shown")
        
        with filter_status_col2:
            if st.button("Clear Filters", type="secondary", key="clear_filters_main_btn"):
                FilterEngine.clear_all_filters()
                SessionStateManager.clear_filters()
                st.rerun()
    
    col1, col2, col3, col4, col5, col6 = st.columns(6)
    
    with col1:
        total_stocks = len(filtered_df)
        total_original = len(ranked_df)
        pct_of_all = (total_stocks/total_original*100) if total_original > 0 else 0
        
        UIComponents.render_metric_card(
            "Total Stocks",
            f"{total_stocks:,}",
            f"{pct_of_all:.0f}% of {total_original:,}"
        )
    
    with col2:
        if not filtered_df.empty and 'master_score' in filtered_df.columns:
            avg_score = filtered_df['master_score'].mean()
            std_score = filtered_df['master_score'].std()
            UIComponents.render_metric_card(
                "Avg Score",
                f"{avg_score:.1f}",
                f"σ={std_score:.1f}"
            )
        else:
            UIComponents.render_metric_card("Avg Score", "N/A")
    
    with col3:
        if show_fundamentals and 'pe' in filtered_df.columns:
            valid_pe = filtered_df['pe'].notna() & (filtered_df['pe'] > 0) & (filtered_df['pe'] < 10000)
            pe_coverage = valid_pe.sum()
            pe_pct = (pe_coverage / len(filtered_df) * 100) if len(filtered_df) > 0 else 0
            
            if pe_coverage > 0:
                median_pe = filtered_df.loc[valid_pe, 'pe'].median()
                UIComponents.render_metric_card(
                    "Median PE",
                    f"{median_pe:.1f}x",
                    f"{pe_pct:.0f}% have data"
                )
            else:
                UIComponents.render_metric_card("PE Data", "Limited", "No PE data")
        else:
            if not filtered_df.empty and 'master_score' in filtered_df.columns:
                min_score = filtered_df['master_score'].min()
                max_score = filtered_df['master_score'].max()
                score_range = f"{min_score:.1f}-{max_score:.1f}"
            else:
                score_range = "N/A"
            UIComponents.render_metric_card("Score Range", score_range)
    
    with col4:
        if show_fundamentals and 'eps_change_pct' in filtered_df.columns:
            valid_eps_change = filtered_df['eps_change_pct'].notna()
            positive_eps_growth = valid_eps_change & (filtered_df['eps_change_pct'] > 0)
            strong_growth = valid_eps_change & (filtered_df['eps_change_pct'] > 50)
            mega_growth = valid_eps_change & (filtered_df['eps_change_pct'] > 100)
            
            growth_count = positive_eps_growth.sum()
            strong_count = strong_growth.sum()
            
            if mega_growth.sum() > 0:
                UIComponents.render_metric_card(
                    "EPS Growth +ve",
                    f"{growth_count}",
                    f"{strong_count} >50% | {mega_growth.sum()} >100%"
                )
            else:
                UIComponents.render_metric_card(
                    "EPS Growth +ve",
                    f"{growth_count}",
                    f"{valid_eps_change.sum()} have data"
                )
        else:
            if 'acceleration_score' in filtered_df.columns:
                accelerating = (filtered_df['acceleration_score'] >= 80).sum()
            else:
                accelerating = 0
            UIComponents.render_metric_card("Accelerating", f"{accelerating}")
    
    with col5:
        if 'rvol' in filtered_df.columns:
            high_rvol = (filtered_df['rvol'] > 2).sum()
        else:
            high_rvol = 0
        UIComponents.render_metric_card("High RVOL", f"{high_rvol}")
    
    with col6:
        if 'trend_quality' in filtered_df.columns:
            strong_trends = (filtered_df['trend_quality'] >= 70).sum()
            total = len(filtered_df)
            UIComponents.render_metric_card(
                "Strong Trends", 
                f"{strong_trends}",
                f"{strong_trends/total*100:.0f}%" if total > 0 else "0%"
            )
        else:
            if 'patterns' in filtered_df.columns:
                with_patterns = (filtered_df['patterns'] != '').sum()
                UIComponents.render_metric_card("With Patterns", f"{with_patterns}")
            else:
                UIComponents.render_metric_card("With Patterns", "N/A")
    
    tabs = st.tabs([
        "📊 Summary", "🏆 Rankings", "📈 Market Radar", "📊 Analysis", "🔍 Search", "📥 Export", "ℹ️ About"
    ])
    
    with tabs[0]:
        st.markdown("### 📊 Executive Summary Dashboard")
        
        if not filtered_df.empty:
            UIComponents.render_summary_section(filtered_df)
            
            st.markdown("---")
            st.markdown("#### 💾 Download Clean Processed Data")
            
            download_cols = st.columns(3)
            
            with download_cols[0]:
                st.markdown("**📊 Current View Data**")
                st.write(f"Includes {len(filtered_df)} stocks matching current filters")
                
                csv_filtered = ExportEngine.create_csv_export(filtered_df)
                st.download_button(
                    label="📥 Download Filtered Data (CSV)",
                    data=csv_filtered,
                    file_name=f"wave_detection_filtered_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}.csv",
                    mime="text/csv",
                    help="Download currently filtered stocks with all scores and indicators"
                )
            
            with download_cols[1]:
                st.markdown("**🏆 Top 100 Stocks**")
                st.write("Elite stocks ranked by Master Score")
                
                top_100 = filtered_df.nlargest(100, 'master_score')
                csv_top100 = ExportEngine.create_csv_export(top_100)
                st.download_button(
                    label="📥 Download Top 100 (CSV)",
                    data=csv_top100,
                    file_name=f"wave_detection_top100_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}.csv",
                    mime="text/csv",
                    help="Download top 100 stocks by Master Score"
                )
            
            with download_cols[2]:
                st.markdown("**🎯 Pattern Stocks Only**")
                if 'patterns' in filtered_df.columns:
                    pattern_stocks = filtered_df[filtered_df['patterns'] != '']
                    st.write(f"Includes {len(pattern_stocks)} stocks with patterns")
                    
                    if len(pattern_stocks) > 0:
                        csv_patterns = ExportEngine.create_csv_export(pattern_stocks)
                        st.download_button(
                            label="📥 Download Pattern Stocks (CSV)",
                            data=csv_patterns,
                            file_name=f"wave_detection_patterns_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}.csv",
                            mime="text/csv",
                            help="Download stocks with technical patterns"
                        )
                    else:
                        st.info("📊 No pattern stocks in current selection")
                else:
                    st.warning("⚠️ Pattern data not available in dataset")
        
        else:
            st.warning("No data available for summary. Please adjust filters.")
    
    # Tab 1: Rankings - ALL TIME BEST PROFESSIONAL RANKING TAB
    with tabs[1]:
        st.markdown("### 🏆 Top Ranked Stocks")
        st.markdown("*Complete analysis with all critical metrics for professional stock ranking*")
        
        # Enhanced Control Panel
        ranking_controls = st.columns([2, 2, 2, 2, 2])
        
        with ranking_controls[0]:
            display_count = st.selectbox(
                "🔢 Show Top",
                options=CONFIG.AVAILABLE_TOP_N,
                index=CONFIG.AVAILABLE_TOP_N.index(st.session_state.user_preferences['default_top_n']),
                key="display_count_select"
            )
            st.session_state.user_preferences['default_top_n'] = display_count
        
        with ranking_controls[1]:
            sort_options = ['Master Score', 'Momentum Score', 'Volume Score', 'Position Score', 
                          'Acceleration Score', 'Breakout Score', 'RVOL Score', 'Trend Quality',
                          'Long Term Strength', 'Liquidity Score', 'Overall Market Strength']
            
            sort_by = st.selectbox(
                "📊 Primary Sort",
                options=sort_options, 
                index=0,
                key="sort_by_select"
            )
        
        with ranking_controls[2]:
            view_mode = st.selectbox(
                "📋 View Mode",
                options=["Essential", "Technical", "Complete", "Custom"],
                index=0,
                key="ranking_view_mode",
                help="Essential: Core metrics | Technical: All scores | Complete: Everything"
            )
        
        with ranking_controls[3]:
            performance_timeframe = st.selectbox(
                "⏱️ Performance Period",
                options=["1D", "3D", "7D", "30D", "All"],
                index=4,
                key="perf_timeframe",
                help="Focus on specific performance timeframe"
            )
        
        with ranking_controls[4]:
            export_format = st.selectbox(
                "💾 Export Format",
                options=["None", "CSV", "Excel", "JSON"],
                index=0,
                key="export_format"
            )
        
        # 🔧 PERFORMANCE PERIOD FILTERING IMPLEMENTATION
        # Apply performance-based filtering before display
        performance_filtered_df = filtered_df.copy()
        
        if performance_timeframe != "All":
            # Map timeframe to return column
            timeframe_mapping = {
                "1D": "ret_1d",
                "3D": "ret_3d", 
                "7D": "ret_7d",
                "30D": "ret_30d"
            }
            
            primary_return_col = timeframe_mapping.get(performance_timeframe)
            
            if primary_return_col and primary_return_col in performance_filtered_df.columns:
                # Adjust master score based on selected timeframe performance
                # Give higher weight to the selected timeframe
                valid_returns = performance_filtered_df[primary_return_col].notna()
                
                if valid_returns.any():
                    # Create performance-adjusted score
                    timeframe_multiplier = pd.Series(1.0, index=performance_filtered_df.index)
                    
                    # Boost scores for strong performers in selected timeframe
                    strong_performers = (performance_filtered_df[primary_return_col] > 10) & valid_returns
                    moderate_performers = (performance_filtered_df[primary_return_col] > 5) & (performance_filtered_df[primary_return_col] <= 10) & valid_returns
                    weak_performers = (performance_filtered_df[primary_return_col] < -5) & valid_returns
                    
                    timeframe_multiplier[strong_performers] = 1.15  # 15% boost for strong performers
                    timeframe_multiplier[moderate_performers] = 1.05  # 5% boost for moderate performers  
                    timeframe_multiplier[weak_performers] = 0.95    # 5% penalty for weak performers
                    
                    # Apply timeframe adjustment to master score
                    performance_filtered_df['master_score_adjusted'] = (
                        performance_filtered_df['master_score'] * timeframe_multiplier
                    ).clip(0, 100)
                    
                    # Resort by adjusted score for this timeframe
                    performance_filtered_df = performance_filtered_df.sort_values('master_score_adjusted', ascending=False)
                    
                    logger.info(f"Applied {performance_timeframe} performance weighting: "
                              f"{strong_performers.sum()} strong, {moderate_performers.sum()} moderate, "
                              f"{weak_performers.sum()} weak performers")
        
        display_df = performance_filtered_df.head(display_count).copy()
        
        
        # Enhanced sorting logic
        sort_mapping = {
            'Master Score': 'master_score',
            'Momentum Score': 'momentum_score',
            'Volume Score': 'volume_score', 
            'Position Score': 'position_score',
            'Acceleration Score': 'acceleration_score',
            'Breakout Score': 'breakout_score',
            'RVOL Score': 'rvol_score',
            'Trend Quality': 'trend_quality',
            'Long Term Strength': 'long_term_strength',
            'Liquidity Score': 'liquidity_score',
            'Overall Market Strength': 'overall_market_strength'
        }
        
        if sort_by in sort_mapping and sort_mapping[sort_by] in display_df.columns:
            display_df = display_df.sort_values(sort_mapping[sort_by], ascending=False)
        else:
            display_df = display_df.sort_values('master_score', ascending=False)
        
        if not display_df.empty:
            
            # PROFESSIONAL COLUMN CONFIGURATION BASED ON VIEW MODE
            # Use adjusted score if performance timeframe filtering is applied
            score_column = 'master_score_adjusted' if (performance_timeframe != "All" and 'master_score_adjusted' in display_df.columns) else 'master_score'
            score_label = f'{performance_timeframe} Score' if performance_timeframe != "All" and 'master_score_adjusted' in display_df.columns else 'Score'
            
            if view_mode == "Essential":
                # Core metrics every trader needs
                display_cols = {
                    'rank': 'Rank',
                    'ticker': 'Ticker',
                    'company_name': 'Company',
                    score_column: score_label,
                    'market_state': 'State',
                    'price': 'Price',
                    'from_low_pct': 'Low%',
                    'from_high_pct': 'High%',
                    'ret_1d': '1D%',
                    'ret_7d': '7D%',
                    'ret_30d': '30D%',
                    'rvol': 'RVOL',
                    'vmi': 'VMI',
                    'volume_1d': 'Vol(Cr)',
                    'patterns': 'Patterns',
                    'category': 'Category'
                }
                
                # Add fundamentals if in Hybrid mode
                if show_fundamentals:
                    display_cols.update({
                        'pe': 'PE',
                        'eps_current': 'EPS',
                        'eps_change_pct': 'EPS Δ%'
                    })
                
            elif view_mode == "Technical":
                # All technical scores and indicators
                display_cols = {
                    'rank': 'Rank',
                    'ticker': 'Ticker',
                    'company_name': 'Company',
                    score_column: 'Master' if performance_timeframe == "All" else f'{performance_timeframe} Master',
                    'position_score': 'Position',
                    'momentum_score': 'Momentum',
                    'volume_score': 'Volume',
                    'acceleration_score': 'Accel',
                    'breakout_score': 'Breakout',
                    'rvol_score': 'RVOL Scr',
                    'trend_quality': 'Trend',
                    'long_term_strength': 'LT Str',
                    'liquidity_score': 'Liquid',
                    'overall_market_strength': 'Mkt Str',
                    'price': 'Price',
                    'vmi': 'VMI',
                    'patterns': 'Patterns'
                }
                
                # Add fundamentals if in Hybrid mode
                if show_fundamentals:
                    display_cols.update({
                        'pe': 'PE',
                        'eps_current': 'EPS',
                        'eps_change_pct': 'EPS Δ%'
                    })
                
            elif view_mode == "Complete":
                # Everything - ultimate professional view
                display_cols = {
                    'rank': 'Rank',
                    'ticker': 'Ticker', 
                    'company_name': 'Company',
                    score_column: 'Master' if performance_timeframe == "All" else f'{performance_timeframe} Master',
                    'position_score': 'Pos',
                    'momentum_score': 'Mom',
                    'volume_score': 'Vol',
                    'acceleration_score': 'Acc',
                    'breakout_score': 'Brk',
                    'rvol_score': 'RVS',
                    'trend_quality': 'Trd',
                    'long_term_strength': 'LTS',
                    'liquidity_score': 'Liq',
                    'overall_market_strength': 'MktS',
                    'market_state': 'State',
                    'price': 'Price',
                    'from_low_pct': 'Low%',
                    'from_high_pct': 'High%',
                    'ret_1d': '1D%',
                    'ret_3d': '3D%',
                    'ret_7d': '7D%',
                    'ret_30d': '30D%',
                    'rvol': 'RVOL',
                    'vmi': 'VMI',
                    'volume_1d': 'Vol(Cr)',
                    'money_flow_mm': 'MF(MM)',
                    'patterns': 'Patterns',
                    'category': 'Category',
                    'sector': 'Sector',
                    'industry': 'Industry'
                }
                
                # Add fundamentals if available - ONLY WHAT EXISTS IN V9.PY
                if show_fundamentals:
                    display_cols.update({
                        'pe': 'PE',
                        'eps_current': 'EPS',
                        'eps_change_pct': 'EPS Δ%'
                    })
                    
            else:  # Custom view
                # Let user select what they want to see
                st.markdown("#### 🛠️ Customize Your View")
                
                available_cols = [
                    'rank', 'ticker', 'company_name', 'master_score', 'position_score', 'momentum_score',
                    'volume_score', 'acceleration_score', 'breakout_score', 'rvol_score', 'trend_quality',
                    'long_term_strength', 'liquidity_score', 'overall_market_strength', 'market_state',
                    'price', 'from_low_pct', 'from_high_pct', 'ret_1d', 'ret_3d', 'ret_7d', 'ret_30d',
                    'rvol', 'vmi', 'volume_1d', 'money_flow_mm', 'patterns',
                    'category', 'sector', 'industry'
                ]
                
                # Add fundamentals if available - ONLY WHAT EXISTS IN V9.PY  
                if show_fundamentals:
                    available_cols.extend(['pe', 'eps_current', 'eps_change_pct'])
                
                # Filter to only available columns
                available_cols = [col for col in available_cols if col in display_df.columns]
                
                # Smart default selection - only columns that exist
                default_cols = ['rank', 'ticker', 'company_name', 'master_score', 'price']
                if 'ret_30d' in available_cols:
                    default_cols.append('ret_30d')
                if 'rvol' in available_cols:
                    default_cols.append('rvol')
                if 'patterns' in available_cols:
                    default_cols.append('patterns')
                
                custom_cols = st.multiselect(
                    "Select columns to display:",
                    options=available_cols,
                    default=default_cols,
                    key="custom_columns_select"
                )
                
                # Create display_cols dict for custom selection
                display_cols = {col: col.replace('_', ' ').title() for col in custom_cols}
        
            # ROBUST COLUMN FILTERING - Only include columns that actually exist
            available_display_cols = [c for c in display_cols.keys() if c in display_df.columns]
            final_display_cols = {k: display_cols[k] for k in available_display_cols}
            
            # Check if fundamental columns were requested but missing
            if show_fundamentals:
                requested_fund_cols = [c for c in ['pe', 'eps_current', 'eps_change_pct'] if c in display_cols.keys()]
                missing_fund_cols = [c for c in requested_fund_cols if c not in display_df.columns]
                
                if missing_fund_cols:
                    st.warning(f"⚠️ **Hybrid Mode**: Some fundamental data missing from dataset: {', '.join(missing_fund_cols)}")
                    st.info("💡 **Tip**: Upload data with PE, EPS columns or switch to Technical mode for full experience")
            
            # SAFETY CHECK: Ensure no duplicate column names
            column_names = list(final_display_cols.values())
            if len(column_names) != len(set(column_names)):
                st.error(f"🚨 **CRITICAL ERROR**: Duplicate column names detected: {column_names}")
                st.error("This will cause the application to crash. Please report this bug.")
                return
            
            # Create formatted dataframe for display - PERFORMANCE OPTIMIZED
            # Only copy columns that will be displayed to reduce memory usage
            display_cols_list = list(final_display_cols.keys())
            display_df_formatted = display_df[display_cols_list].copy()
            
            # PROFESSIONAL FORMATTING RULES
            format_rules = {
                'master_score': lambda x: f"{x:.1f}" if pd.notna(x) else '-',
                'master_score_adjusted': lambda x: f"{x:.1f}" if pd.notna(x) else '-',  # Add adjusted score formatting
                'position_score': lambda x: f"{x:.0f}" if pd.notna(x) else '-',
                'momentum_score': lambda x: f"{x:.0f}" if pd.notna(x) else '-',
                'volume_score': lambda x: f"{x:.0f}" if pd.notna(x) else '-',
                'acceleration_score': lambda x: f"{x:.0f}" if pd.notna(x) else '-',
                'breakout_score': lambda x: f"{x:.0f}" if pd.notna(x) else '-',
                'rvol_score': lambda x: f"{x:.0f}" if pd.notna(x) else '-',
                'trend_quality': lambda x: f"{x:.0f}" if pd.notna(x) else '-',
                'long_term_strength': lambda x: f"{x:.0f}" if pd.notna(x) else '-',
                'liquidity_score': lambda x: f"{x:.0f}" if pd.notna(x) else '-',
                'overall_market_strength': lambda x: f"{x:.0f}" if pd.notna(x) else '-',
                'price': lambda x: f"₹{x:,.0f}" if pd.notna(x) else '-',
                'from_low_pct': lambda x: f"{x:.0f}%" if pd.notna(x) else '-',
                'from_high_pct': lambda x: f"{x:+.1f}%" if pd.notna(x) else '-',
                'ret_1d': lambda x: f"{x:+.1f}%" if pd.notna(x) else '-',
                'ret_3d': lambda x: f"{x:+.1f}%" if pd.notna(x) else '-',
                'ret_7d': lambda x: f"{x:+.1f}%" if pd.notna(x) else '-',
                'ret_30d': lambda x: f"{x:+.1f}%" if pd.notna(x) else '-',
                'rvol': lambda x: f"{x:.1f}x" if pd.notna(x) else '-',
                'vmi': lambda x: f"{x:.2f}" if pd.notna(x) else '-',
                'volume_1d': lambda x: f"{x:.1f}" if pd.notna(x) else '-',
                'money_flow_mm': lambda x: f"₹{x:.0f}M" if pd.notna(x) else '-',
                # Fundamental columns formatting
                'pe': lambda x: f"{x:.1f}x" if pd.notna(x) and x > 0 else 'N/A',
                'eps_current': lambda x: f"₹{x:.2f}" if pd.notna(x) else 'N/A',
                'eps_change_pct': lambda x: f"{x:+.1f}%" if pd.notna(x) else 'N/A',
                'market_cap_cr': lambda x: f"₹{x:.0f}Cr" if pd.notna(x) else '-'
            }
            
            # Apply formatting
            for col, formatter in format_rules.items():
                if col in display_df_formatted.columns:
                    display_df_formatted[col] = display_df[col].apply(formatter)
            
            # Format PE with professional logic
            def format_pe_professional(value):
                try:
                    if pd.isna(value) or value == 'N/A':
                        return '-'
                    
                    val = float(value)
                    
                    if val <= 0:
                        return 'Loss'
                    elif val > 10000:
                        return '>10K'
                    elif val > 1000:
                        return f"{val:.0f}"
                    elif val > 100:
                        return f"{val:.1f}"
                    else:
                        return f"{val:.1f}"
                except (ValueError, TypeError, AttributeError):
                    return '-'
            
            # Format EPS Change with professional logic
            def format_eps_change_professional(value):
                try:
                    if pd.isna(value):
                        return '-'
                    
                    val = float(value)
                    
                    if abs(val) >= 1000:
                        return f"{val/1000:+.1f}K%"
                    elif abs(val) >= 100:
                        return f"{val:+.0f}%"
                    else:
                        return f"{val:+.1f}%"
                except (ValueError, TypeError, AttributeError):
                    return '-'
            
            # Apply professional fundamental formatting - ONLY EXISTING COLUMNS
            if show_fundamentals:
                if 'pe' in display_df_formatted.columns:
                    display_df_formatted['pe'] = display_df['pe'].apply(format_pe_professional)
                
                if 'eps_change_pct' in display_df_formatted.columns:
                    display_df_formatted['eps_change_pct'] = display_df['eps_change_pct'].apply(format_eps_change_professional)
                
                # Format EPS current (earnings per share)
                if 'eps_current' in display_df_formatted.columns:
                    display_df_formatted['eps_current'] = display_df['eps_current'].apply(lambda x: f"₹{x:.1f}" if pd.notna(x) and x > 0 else '-')
            
            # Add trend indicators for better visualization
            if 'trend_quality' in display_df.columns:
                def get_trend_indicator_professional(score):
                    if pd.isna(score):
                        return "➖"
                    elif score >= 85:
                        return "🔥"  # Exceptional
                    elif score >= 70:
                        return "🚀"  # Strong
                    elif score >= 55:
                        return "✅"  # Good
                    elif score >= 40:
                        return "➡️"  # Neutral
                    elif score >= 25:
                        return "⚠️"  # Weak
                    else:
                        return "🔻"  # Poor
                
                display_df_formatted['trend_indicator'] = display_df['trend_quality'].apply(get_trend_indicator_professional)
                if view_mode in ["Essential", "Complete"]:
                    final_display_cols['trend_indicator'] = 'Trend'
            
            # Add momentum strength indicators
            if 'momentum_score' in display_df.columns:
                def get_momentum_indicator(score):
                    if pd.isna(score):
                        return "➖"
                    elif score >= 80:
                        return "🎯"  # Excellent
                    elif score >= 60:
                        return "📈"  # Strong
                    elif score >= 40:
                        return "📊"  # Moderate
                    else:
                        return "📉"  # Weak
                
                display_df_formatted['momentum_indicator'] = display_df['momentum_score'].apply(get_momentum_indicator)
                if view_mode == "Essential":
                    final_display_cols['momentum_indicator'] = 'Mom'
            
            # Select and rename columns for final display
            final_display_df = display_df_formatted[list(final_display_cols.keys())]
            final_display_df.columns = list(final_display_cols.values())
            
            # PROFESSIONAL COLUMN CONFIGURATION FOR STREAMLIT
            column_config = {}
            
            # Define comprehensive column configurations
            config_map = {
                "Rank": st.column_config.NumberColumn("Rank", help="Overall ranking position", format="%d", width="tiny"),
                "Ticker": st.column_config.TextColumn("Ticker", help="Stock symbol", width="small"),
                "Company": st.column_config.TextColumn("Company", help="Company name", width="medium", max_chars=40),
                "Master": st.column_config.TextColumn("Master", help="Master Score (0-100)", width="small"),
                "Score": st.column_config.TextColumn("Score", help="Master Score (0-100)", width="small"),
                "Pos": st.column_config.TextColumn("Pos", help="Position Score", width="tiny"),
                "Position": st.column_config.TextColumn("Position", help="Position Score", width="small"),
                "Mom": st.column_config.TextColumn("Mom", help="Momentum Score", width="tiny"),
                "Momentum": st.column_config.TextColumn("Momentum", help="Momentum Score", width="small"),
                "Vol": st.column_config.TextColumn("Vol", help="Volume Score", width="tiny"),
                "Volume": st.column_config.TextColumn("Volume", help="Volume Score", width="small"),
                "Acc": st.column_config.TextColumn("Acc", help="Acceleration Score", width="tiny"),
                "Accel": st.column_config.TextColumn("Accel", help="Acceleration Score", width="small"),
                "Brk": st.column_config.TextColumn("Brk", help="Breakout Score", width="tiny"),
                "Breakout": st.column_config.TextColumn("Breakout", help="Breakout Score", width="small"),
                "RVS": st.column_config.TextColumn("RVS", help="RVOL Score", width="tiny"),
                "RVOL Scr": st.column_config.TextColumn("RVOL Scr", help="RVOL Score (0-100)", width="small"),
                "RVOL": st.column_config.TextColumn("RVOL", help="Relative Volume vs Avg", width="small"),
                "Trd": st.column_config.TextColumn("Trd", help="Trend Quality Score", width="tiny"),
                "Trend": st.column_config.TextColumn("Trend", help="Trend Quality Indicator", width="small"),
                "LTS": st.column_config.TextColumn("LTS", help="Long Term Strength", width="tiny"),
                "LT Str": st.column_config.TextColumn("LT Str", help="Long Term Strength", width="small"),
                "Liq": st.column_config.TextColumn("Liq", help="Liquidity Score", width="tiny"),
                "Liquid": st.column_config.TextColumn("Liquid", help="Liquidity Score", width="small"),
                "MktS": st.column_config.TextColumn("MktS", help="Overall Market Strength", width="tiny"),
                "Mkt Str": st.column_config.TextColumn("Mkt Str", help="Overall Market Strength", width="small"),
                "State": st.column_config.TextColumn("State", help="Current Market State", width="medium"),
                "Price": st.column_config.TextColumn("Price", help="Current stock price", width="small"),
                "Low%": st.column_config.TextColumn("Low%", help="Distance from 52W low", width="small"),
                "High%": st.column_config.TextColumn("High%", help="Distance from 52W high", width="small"),
                "1D%": st.column_config.TextColumn("1D%", help="1-day return", width="small"),
                "3D%": st.column_config.TextColumn("3D%", help="3-day return", width="small"),
                "7D%": st.column_config.TextColumn("7D%", help="7-day return", width="small"),
                "30D%": st.column_config.TextColumn("30D%", help="30-day return", width="small"),
                "VMI": st.column_config.TextColumn("VMI", help="Volume Momentum Index", width="small"),
                "Vol(Cr)": st.column_config.TextColumn("Vol(Cr)", help="Volume in Crores", width="small"),
                "MF(MM)": st.column_config.TextColumn("MF(MM)", help="Money Flow in MM", width="small"),
                "ATR%": st.column_config.TextColumn("ATR%", help="Average True Range %", width="small"),
                "RSI": st.column_config.TextColumn("RSI", help="Relative Strength Index", width="small"),
                "Patterns": st.column_config.TextColumn("Patterns", help="Technical Patterns", width="large", max_chars=80),
                "Category": st.column_config.TextColumn("Category", help="Market Cap Category", width="medium"),
                "Sector": st.column_config.TextColumn("Sector", help="Sector Classification", width="medium"),
                "Industry": st.column_config.TextColumn("Industry", help="Industry Classification", width="medium", max_chars=40),
                
                # Fundamental columns - ONLY WHAT EXISTS IN V9.PY
                "PE": st.column_config.TextColumn("PE", help="Price to Earnings Ratio", width="small"),
                "EPS": st.column_config.TextColumn("EPS", help="Earnings Per Share (Current)", width="small"),
                "EPS Δ%": st.column_config.TextColumn("EPS Δ%", help="EPS Change %", width="small"),
                
                # Indicator columns
                "Mom": st.column_config.TextColumn("Mom", help="Momentum Indicator", width="tiny"),
            }
            
            # Apply configurations for columns that exist in our dataframe
            for col_name in final_display_df.columns:
                if col_name in config_map:
                    column_config[col_name] = config_map[col_name]
                else:
                    # Default configuration for any missing columns
                    column_config[col_name] = st.column_config.TextColumn(col_name, width="medium")
            
            # ENHANCED MAIN DATAFRAME DISPLAY
            st.markdown("#### 📊 Main Rankings Table")
            
            # Enhanced performance timeframe display with detailed info
            if performance_timeframe != "All":
                timeframe_info_cols = st.columns([3, 1])
                
                with timeframe_info_cols[0]:
                    # Show performance impact details
                    if 'master_score_adjusted' in display_df.columns:
                        adjustment_stats = {
                            'boosted': (display_df['master_score_adjusted'] > display_df['master_score']).sum(),
                            'penalized': (display_df['master_score_adjusted'] < display_df['master_score']).sum(),
                            'unchanged': (display_df['master_score_adjusted'] == display_df['master_score']).sum()
                        }
                        
                        st.info(f"📅 **{performance_timeframe} Performance Focus Active** | "
                               f"🚀 {adjustment_stats['boosted']} boosted • "
                               f"⚠️ {adjustment_stats['penalized']} penalized • "
                               f"➖ {adjustment_stats['unchanged']} unchanged")
                    else:
                        st.info(f"📅 **Performance Focus**: {performance_timeframe} timeframe analysis")
                
                with timeframe_info_cols[1]:
                    if st.button("🔄 Reset to All", key="reset_timeframe", help="Show all timeframes"):
                        st.rerun()
            
            st.dataframe(
                final_display_df,
                width='stretch',
                height=min(800, len(final_display_df) * 35 + 100),
                hide_index=True,
                column_config=column_config
            )
            
            # PROFESSIONAL EXPORT FUNCTIONALITY
            if export_format != "None":
                st.markdown("---")
                export_cols = st.columns([3, 1])
                
                with export_cols[0]:
                    st.markdown(f"#### 💾 Export Data ({export_format})")
                    
                with export_cols[1]:
                    if export_format == "CSV":
                        csv_data = ExportEngine.create_csv_export(display_df)
                        st.download_button(
                            label=f"📥 Download Top {display_count} ({export_format})",
                            data=csv_data,
                            file_name=f"professional_rankings_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}.csv",
                            mime="text/csv"
                        )
                    elif export_format == "Excel":
                        # Create Excel export with multiple sheets
                        excel_buffer = BytesIO()
                        with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
                            display_df.to_excel(writer, sheet_name='Rankings', index=False)
                            
                            # Add summary sheet - USING ACTUAL V9.PY COLUMNS
                            summary_data = {
                                'Metric': ['Total Stocks', 'Avg Master Score', 'Avg 30D Return', 'Top Category', 'Avg PE'],
                                'Value': [
                                    len(display_df),
                                    f"{display_df['master_score'].mean():.1f}",
                                    f"{display_df['ret_30d'].mean():.1f}%" if 'ret_30d' in display_df.columns else 'N/A',
                                    display_df['category'].mode().iloc[0] if 'category' in display_df.columns else 'N/A',
                                    f"{display_df['pe'].median():.1f}" if 'pe' in display_df.columns else 'N/A'
                                ]
                            }
                            pd.DataFrame(summary_data).to_excel(writer, sheet_name='Summary', index=False)
                        
                        excel_buffer.seek(0)
                        st.download_button(
                            label=f"📥 Download Top {display_count} (Excel)",
                            data=excel_buffer.getvalue(),
                            file_name=f"professional_rankings_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}.xlsx",
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                        )
                    elif export_format == "JSON":
                        json_data = display_df.to_json(orient='records', indent=2)
                        st.download_button(
                            label=f"📥 Download Top {display_count} (JSON)",
                            data=json_data,
                            file_name=f"professional_rankings_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}.json",
                            mime="application/json"
                        )
            
            
            # PROFESSIONAL ANALYTICS SECTION - COMPREHENSIVE INSIGHTS
            st.markdown("---")
            st.markdown("### 📊 PROFESSIONAL ANALYTICS & INSIGHTS")
            
            # Create analytics tabs for better organization
            analytics_tabs = st.tabs(["📈 Score Analysis", "💰 Performance Metrics", "🔍 Technical Analysis", "🏭 Sector Analysis", "⚡ Quick Stats"])
            
            with analytics_tabs[0]:  # Score Analysis
                st.markdown("#### 📈 Score Distribution Analysis")
                
                score_cols = st.columns(4)
                
                with score_cols[0]:
                    st.markdown("**🎯 Master Score Distribution**")
                    if 'master_score' in display_df.columns:
                        score_stats = {
                            'Elite (90+)': f"{(display_df['master_score'] >= 90).sum()}",
                            'Excellent (80-89)': f"{((display_df['master_score'] >= 80) & (display_df['master_score'] < 90)).sum()}",
                            'Good (70-79)': f"{((display_df['master_score'] >= 70) & (display_df['master_score'] < 80)).sum()}",
                            'Average (60-69)': f"{((display_df['master_score'] >= 60) & (display_df['master_score'] < 70)).sum()}",
                            'Below Avg (<60)': f"{(display_df['master_score'] < 60).sum()}",
                            'Mean Score': f"{display_df['master_score'].mean():.1f}",
                            'Median Score': f"{display_df['master_score'].median():.1f}"
                        }
                        
                        score_df = pd.DataFrame(list(score_stats.items()), columns=['Range', 'Count'])
                        st.dataframe(score_df, width='stretch', hide_index=True)
                
                with score_cols[1]:
                    st.markdown("**🚀 Momentum Analysis**")
                    if 'momentum_score' in display_df.columns:
                        momentum_stats = {
                            'Explosive (80+)': f"{(display_df['momentum_score'] >= 80).sum()}",
                            'Strong (60-79)': f"{((display_df['momentum_score'] >= 60) & (display_df['momentum_score'] < 80)).sum()}",
                            'Moderate (40-59)': f"{((display_df['momentum_score'] >= 40) & (display_df['momentum_score'] < 60)).sum()}",
                            'Weak (<40)': f"{(display_df['momentum_score'] < 40).sum()}",
                            'Avg Momentum': f"{display_df['momentum_score'].mean():.1f}",
                            'Top 10 Avg': f"{display_df.head(10)['momentum_score'].mean():.1f}"
                        }
                        
                        momentum_df = pd.DataFrame(list(momentum_stats.items()), columns=['Level', 'Value'])
                        st.dataframe(momentum_df, width='stretch', hide_index=True)
                
                with score_cols[2]:
                    st.markdown("**📊 Volume Analysis**") 
                    if 'volume_score' in display_df.columns:
                        volume_stats = {
                            'Exceptional (80+)': f"{(display_df['volume_score'] >= 80).sum()}",
                            'High (60-79)': f"{((display_df['volume_score'] >= 60) & (display_df['volume_score'] < 80)).sum()}",
                            'Normal (40-59)': f"{((display_df['volume_score'] >= 40) & (display_df['volume_score'] < 60)).sum()}",
                            'Low (<40)': f"{(display_df['volume_score'] < 40).sum()}",
                            'Avg Vol Score': f"{display_df['volume_score'].mean():.1f}",
                            'RVOL > 3x': f"{(display_df['rvol'] > 3).sum()}" if 'rvol' in display_df.columns else 'N/A'
                        }
                        
                        volume_df = pd.DataFrame(list(volume_stats.items()), columns=['Level', 'Count'])
                        st.dataframe(volume_df, width='stretch', hide_index=True)
                
                with score_cols[3]:
                    st.markdown("**📍 Position Analysis**")
                    if 'position_score' in display_df.columns:
                        position_stats = {
                            'Near Highs (80+)': f"{(display_df['position_score'] >= 80).sum()}",
                            'Strong (60-79)': f"{((display_df['position_score'] >= 60) & (display_df['position_score'] < 80)).sum()}",
                            'Mid Range (40-59)': f"{((display_df['position_score'] >= 40) & (display_df['position_score'] < 60)).sum()}",
                            'Near Lows (<40)': f"{(display_df['position_score'] < 40).sum()}",
                            'Avg Position': f"{display_df['position_score'].mean():.1f}",
                            'From Low Avg': f"{display_df['from_low_pct'].mean():.0f}%" if 'from_low_pct' in display_df.columns else 'N/A'
                        }
                        
                        position_df = pd.DataFrame(list(position_stats.items()), columns=['Range', 'Value'])
                        st.dataframe(position_df, width='stretch', hide_index=True)
            
            with analytics_tabs[1]:  # Performance Metrics
                st.markdown("#### 💰 Performance & Returns Analysis")
                
                perf_cols = st.columns(3)
                
                with perf_cols[0]:
                    st.markdown("**📈 Returns Distribution**")
                    return_stats = {}
                    
                    if performance_timeframe == "1D" and 'ret_1d' in display_df.columns:
                        ret_col = 'ret_1d'
                    elif performance_timeframe == "3D" and 'ret_3d' in display_df.columns:
                        ret_col = 'ret_3d'
                    elif performance_timeframe == "7D" and 'ret_7d' in display_df.columns:
                        ret_col = 'ret_7d'
                    elif 'ret_30d' in display_df.columns:
                        ret_col = 'ret_30d'
                    else:
                        ret_col = None
                    
                    if ret_col:
                        return_stats = {
                            'Best Performer': f"{display_df[ret_col].max():.1f}%",
                            'Worst Performer': f"{display_df[ret_col].min():.1f}%",
                            'Average Return': f"{display_df[ret_col].mean():.1f}%",
                            'Median Return': f"{display_df[ret_col].median():.1f}%",
                            'Positive Returns': f"{(display_df[ret_col] > 0).sum()}",
                            'Negative Returns': f"{(display_df[ret_col] < 0).sum()}",
                            'Win Rate': f"{(display_df[ret_col] > 0).sum() / len(display_df) * 100:.0f}%",
                            'Strong Gains (>10%)': f"{(display_df[ret_col] > 10).sum()}",
                            'Big Gains (>20%)': f"{(display_df[ret_col] > 20).sum()}"
                        }
                    else:
                        return_stats = {'No Data': 'Returns data not available'}
                    
                    ret_df = pd.DataFrame(list(return_stats.items()), columns=['Metric', 'Value'])
                    st.dataframe(ret_df, width='stretch', hide_index=True)
                
                with perf_cols[1]:
                    st.markdown("**💎 Quality Metrics**")
                    if show_fundamentals:
                        fund_stats = {}
                        
                        # ONLY USE COLUMNS THAT EXIST IN V9.PY
                        if 'pe' in display_df.columns:
                            valid_pe = display_df['pe'].notna() & (display_df['pe'] > 0) & (display_df['pe'] < 1000)
                            if valid_pe.any():
                                fund_stats.update({
                                    'Median PE': f"{display_df.loc[valid_pe, 'pe'].median():.1f}x",
                                    'Value Stocks (PE<15)': f"{(display_df['pe'] < 15).sum()}",
                                    'Growth Stocks (PE>30)': f"{(display_df['pe'] > 30).sum()}"
                                })
                        
                        if 'eps_change_pct' in display_df.columns:
                            valid_eps = display_df['eps_change_pct'].notna()
                            if valid_eps.any():
                                fund_stats.update({
                                    'EPS Growth +ve': f"{(display_df['eps_change_pct'] > 0).sum()}",
                                    'Strong Growth (>25%)': f"{(display_df['eps_change_pct'] > 25).sum()}",
                                    'Avg EPS Growth': f"{display_df['eps_change_pct'].mean():.1f}%"
                                })
                        
                        if 'eps_current' in display_df.columns:
                            valid_eps_current = display_df['eps_current'].notna() & (display_df['eps_current'] > 0)
                            if valid_eps_current.any():
                                fund_stats.update({
                                    'Profitable Stocks': f"{(display_df['eps_current'] > 0).sum()}",
                                    'Avg EPS': f"₹{display_df.loc[valid_eps_current, 'eps_current'].mean():.1f}"
                                })
                        
                        if fund_stats:
                            fund_df = pd.DataFrame(list(fund_stats.items()), columns=['Metric', 'Value'])
                            st.dataframe(fund_df, width='stretch', hide_index=True)
                        else:
                            st.info("No fundamental data available")
                    else:
                        liquidity_stats = {}
                        if 'liquidity_score' in display_df.columns:
                            liquidity_stats = {
                                'High Liquidity (80+)': f"{(display_df['liquidity_score'] >= 80).sum()}",
                                'Good Liquidity (60-79)': f"{((display_df['liquidity_score'] >= 60) & (display_df['liquidity_score'] < 80)).sum()}",
                                'Average Liquidity': f"{display_df['liquidity_score'].mean():.1f}",
                                'Top 10 Avg Liq': f"{display_df.head(10)['liquidity_score'].mean():.1f}"
                            }
                        
                        if 'volume_1d' in display_df.columns:
                            liquidity_stats.update({
                                'Avg Volume (Cr)': f"{display_df['volume_1d'].mean():.1f}",
                                'Max Volume (Cr)': f"{display_df['volume_1d'].max():.1f}"
                            })
                        
                        liq_df = pd.DataFrame(list(liquidity_stats.items()), columns=['Metric', 'Value'])
                        st.dataframe(liq_df, width='stretch', hide_index=True)
                
                with perf_cols[2]:
                    st.markdown("**⚡ Risk & Volatility**")
                    risk_stats = {}
                    
                    if 'from_high_pct' in display_df.columns:
                        risk_stats.update({
                            'Near Highs (>-5%)': f"{(display_df['from_high_pct'] > -5).sum()}",
                            'Deep Correction (<-20%)': f"{(display_df['from_high_pct'] < -20).sum()}"
                        })
                    
                    if risk_stats:
                        risk_df = pd.DataFrame(list(risk_stats.items()), columns=['Metric', 'Value'])
                        st.dataframe(risk_df, width='stretch', hide_index=True)
                    else:
                        st.info("Risk metrics not available")
            
            with analytics_tabs[2]:  # Technical Analysis
                st.markdown("#### 🔍 Advanced Technical Analysis")
                
                tech_cols = st.columns(3)
                
                with tech_cols[0]:
                    st.markdown("**🎯 Pattern Analysis**")
                    if 'patterns' in display_df.columns:
                        # Count pattern occurrences
                        pattern_counts = {}
                        for patterns_str in display_df['patterns'].dropna():
                            if patterns_str and patterns_str.strip():
                                for pattern in patterns_str.split(' | '):
                                    pattern = pattern.strip()
                                    if pattern:
                                        pattern_counts[pattern] = pattern_counts.get(pattern, 0) + 1
                        
                        if pattern_counts:
                            # Get top 8 patterns
                            top_patterns = sorted(pattern_counts.items(), key=lambda x: x[1], reverse=True)[:8]
                            pattern_data = [{'Pattern': p, 'Count': c} for p, c in top_patterns]
                            patterns_df = pd.DataFrame(pattern_data)
                            st.dataframe(patterns_df, width='stretch', hide_index=True)
                        else:
                            st.info("No patterns detected")
                    else:
                        st.info("Pattern data not available")
                
                with tech_cols[1]:
                    st.markdown("**📊 Breakout Analysis**")
                    if 'breakout_score' in display_df.columns:
                        breakout_stats = {
                            'Strong Breakouts (80+)': f"{(display_df['breakout_score'] >= 80).sum()}",
                            'Potential Breakouts (60-79)': f"{((display_df['breakout_score'] >= 60) & (display_df['breakout_score'] < 80)).sum()}",
                            'Building (40-59)': f"{((display_df['breakout_score'] >= 40) & (display_df['breakout_score'] < 60)).sum()}",
                            'No Breakout (<40)': f"{(display_df['breakout_score'] < 40).sum()}",
                            'Avg Breakout Score': f"{display_df['breakout_score'].mean():.1f}",
                            'Top 10 Avg': f"{display_df.head(10)['breakout_score'].mean():.1f}"
                        }
                        
                        breakout_df = pd.DataFrame(list(breakout_stats.items()), columns=['Level', 'Count'])
                        st.dataframe(breakout_df, width='stretch', hide_index=True)
                
                with tech_cols[2]:
                    st.markdown("**⚡ Acceleration Signals**")
                    if 'acceleration_score' in display_df.columns:
                        accel_stats = {
                            'High Acceleration (80+)': f"{(display_df['acceleration_score'] >= 80).sum()}",
                            'Moderate (60-79)': f"{((display_df['acceleration_score'] >= 60) & (display_df['acceleration_score'] < 80)).sum()}",
                            'Building (40-59)': f"{((display_df['acceleration_score'] >= 40) & (display_df['acceleration_score'] < 60)).sum()}",
                            'Low (<40)': f"{(display_df['acceleration_score'] < 40).sum()}",
                            'Avg Acceleration': f"{display_df['acceleration_score'].mean():.1f}",
                            'Momentum Leaders': f"{((display_df['acceleration_score'] >= 80) & (display_df['momentum_score'] >= 80)).sum()}"
                        }
                        
                        accel_df = pd.DataFrame(list(accel_stats.items()), columns=['Level', 'Count'])
                        st.dataframe(accel_df, width='stretch', hide_index=True)
            
            with analytics_tabs[3]:  # Sector Analysis
                st.markdown("#### 🏭 Sector & Category Performance")
                
                sector_cols = st.columns(2)
                
                with sector_cols[0]:
                    st.markdown("**📈 Category Performance**")
                    if 'category' in display_df.columns:
                        cat_performance = display_df.groupby('category').agg({
                            'master_score': ['mean', 'count'],
                            'ret_30d': 'mean' if 'ret_30d' in display_df.columns else lambda x: 0,
                            'rvol': 'mean' if 'rvol' in display_df.columns else lambda x: 0
                        }).round(2)
                        
                        # Flatten column names
                        cat_performance.columns = ['Avg Score', 'Count', 'Avg 30D Ret', 'Avg RVOL']
                        cat_performance = cat_performance.sort_values('Avg Score', ascending=False)
                        
                        # Format the display
                        cat_display = cat_performance.copy()
                        if 'Avg 30D Ret' in cat_display.columns:
                            cat_display['Avg 30D Ret'] = cat_display['Avg 30D Ret'].apply(lambda x: f"{x:.1f}%")
                        if 'Avg RVOL' in cat_display.columns:
                            cat_display['Avg RVOL'] = cat_display['Avg RVOL'].apply(lambda x: f"{x:.1f}x")
                        
                        st.dataframe(cat_display, width='stretch')
                    else:
                        st.info("Category data not available")
                
                with sector_cols[1]:
                    st.markdown("**🏢 Sector Distribution**")
                    if 'sector' in display_df.columns:
                        sector_counts = display_df['sector'].value_counts().head(10)
                        sector_df = pd.DataFrame({
                            'Sector': sector_counts.index,
                            'Count': sector_counts.values
                        })
                        st.dataframe(sector_df, width='stretch', hide_index=True)
                    else:
                        st.info("Sector data not available")
            
            with analytics_tabs[4]:  # Quick Stats
                st.markdown("#### ⚡ Quick Professional Stats")
                
                quick_cols = st.columns(4)
                
                with quick_cols[0]:
                    UIComponents.render_metric_card(
                        "💎 Elite Stocks", 
                        f"{(display_df['master_score'] >= 90).sum()}",
                        f"{(display_df['master_score'] >= 90).sum()/len(display_df)*100:.0f}% of total"
                    )
                
                with quick_cols[1]:
                    if 'patterns' in display_df.columns:
                        with_patterns = (display_df['patterns'] != '').sum()
                        UIComponents.render_metric_card(
                            "🎯 With Patterns", 
                            f"{with_patterns}",
                            f"{with_patterns/len(display_df)*100:.0f}% have signals"
                        )
                
                with quick_cols[2]:
                    if 'rvol' in display_df.columns:
                        high_volume = (display_df['rvol'] > 2).sum()
                        UIComponents.render_metric_card(
                            "🔊 High Volume", 
                            f"{high_volume}",
                            f"RVOL > 2x"
                        )
                
                with quick_cols[3]:
                    if 'ret_30d' in display_df.columns:
                        positive_returns = (display_df['ret_30d'] > 0).sum()
                        UIComponents.render_metric_card(
                            "📈 Positive Returns", 
                            f"{positive_returns}",
                            f"{positive_returns/len(display_df)*100:.0f}% winners"
                        )
        
        else:
            st.warning("No stocks match the selected filters.")
            
            # Show filter summary
            st.markdown("#### Current Filters Applied:")
            active_filter_count = FilterEngine.get_active_count()
            if active_filter_count > 0:
                filter_summary = []
                
                if st.session_state.filter_state.get('categories'):
                    filter_summary.append(f"Categories: {', '.join(st.session_state.filter_state['categories'])}")
                if st.session_state.filter_state.get('sectors'):
                    filter_summary.append(f"Sectors: {', '.join(st.session_state.filter_state['sectors'])}")
                if st.session_state.filter_state.get('industries'):
                    filter_summary.append(f"Industries: {', '.join(st.session_state.filter_state['industries'][:5])}...")
                if st.session_state.filter_state.get('min_score', 0) > 0:
                    filter_summary.append(f"Min Score: {st.session_state.filter_state['min_score']}")
                if st.session_state.filter_state.get('patterns'):
                    filter_summary.append(f"Patterns: {len(st.session_state.filter_state['patterns'])} selected")
                
                for filter_text in filter_summary:
                    st.write(f"• {filter_text}")
                
                if st.button("Clear All Filters", type="primary", key="clear_filters_ranking_btn"):
                    FilterEngine.clear_all_filters()
                    SessionStateManager.clear_filters()
                    st.rerun()
            else:
                st.info("No filters applied. All stocks should be visible unless there's no data loaded.")
        
    # Tab 2: Ultimate Market Radar - ALL TIME BEST IMPLEMENTATION
    with tabs[2]:
        st.markdown("### 🎯 MARKET RADAR - TRADING INTELLIGENCE")
        st.markdown("*Advanced multi-dimensional market analysis for professional traders*")
        
        # ================================================================================================
        # 🔥 PROFESSIONAL CONTROL PANEL
        # ================================================================================================
        
        # Main Control Row
        radar_controls = st.columns([2, 2, 2, 2, 2])
        
        with radar_controls[0]:
            radar_mode = st.selectbox(
                "🎯 Radar Mode",
                options=[
                    "🌊 Wave Hunter", 
                    "⚡ Breakout Scanner", 
                    "🏗️ Pattern Recognition",
                    "💰 Institutional Flow",
                    "🔥 Momentum Surge",
                    "📊 Full Spectrum"
                ],
                index=5,  # Default to Full Spectrum
                key="radar_mode_select",
                help="Choose your market analysis focus"
            )
        
        with radar_controls[1]:
            timeframe_focus = st.selectbox(
                "⏰ Timeframe Focus",
                options=[
                    "🚀 Intraday (1D)",
                    "📈 Short-term (3D)", 
                    "🌊 Medium-term (7D)",
                    "💪 Long-term (30D)",
                    "🔄 Multi-timeframe"
                ],
                index=4,  # Default to Multi-timeframe
                key="timeframe_focus_select",
                help="Primary analysis timeframe"
            )
        
        with radar_controls[2]:
            sensitivity_level = st.select_slider(
                "🎚️ Signal Sensitivity",
                options=["🛡️ Ultra Conservative", "🔒 Conservative", "⚖️ Balanced", "🚀 Aggressive", "🔥 Ultra Aggressive"],
                value="⚖️ Balanced",
                key="sensitivity_level_select",
                help="Signal detection sensitivity level"
            )
        
        with radar_controls[3]:
            risk_filter = st.selectbox(
                "⚖️ Risk Profile",
                options=[
                    "🛡️ Low Risk Only",
                    "⚖️ Balanced Risk", 
                    "🚀 High Risk/Reward",
                    "🔥 Maximum Alpha"
                ],
                index=1,
                key="risk_filter_select",
                help="Filter opportunities by risk profile"
            )
        
        with radar_controls[4]:
            market_regime = st.selectbox(
                "📊 Market Regime",
                options=[
                    "🐂 Bull Market",
                    "🐻 Bear Market", 
                    "🔄 Sideways/Choppy",
                    "📊 Auto-Detect"
                ],
                index=3,
                key="market_regime_select",
                help="Adjust analysis for market conditions"
            )
        
        # Advanced Controls Row
        advanced_controls = st.columns([2, 2, 2, 2, 2])
        
        with advanced_controls[0]:
            enable_ai_signals = st.checkbox(
                "🧠 AI Pattern Recognition",
                value=True,
                key="enable_ai_signals",
                help="Enable AI-powered pattern detection"
            )
        
        with advanced_controls[1]:
            show_institutional = st.checkbox(
                "🏦 Institutional Analysis",
                value=True, 
                key="show_institutional",
                help="Show institutional flow analysis"
            )
        
        with advanced_controls[2]:
            enable_alerts = st.checkbox(
                "🚨 Smart Alerts",
                value=True,
                key="enable_alerts", 
                help="Enable intelligent trading alerts"
            )
        
        with advanced_controls[3]:
            show_correlations = st.checkbox(
                "🔗 Cross-Asset Analysis",
                value=False,
                key="show_correlations",
                help="Show sector/asset correlations"
            )
        
        with advanced_controls[4]:
            export_signals = st.checkbox(
                "📤 Export Signals",
                value=False,
                key="export_signals",
                help="Enable signal export functionality"
            )
        
        # Additional Control Variables
        show_sensitivity_details = st.checkbox(
            "📊 Show Sensitivity Details",
            value=False,
            key="show_sensitivity_details",
            help="Display detailed sensitivity threshold information"
        )
        
        show_market_regime = st.checkbox(
            "📊 Show Market Regime",
            value=True,
            key="show_market_regime",
            help="Display market regime and category rotation analysis"
        )
        
        st.markdown("---")
        
        # ================================================================================================
        # 🧠 AI-POWERED MARKET INTELLIGENCE ENGINE
        # ================================================================================================
        
        # Initialize the filtered dataframe
        radar_df = filtered_df.copy()
        
        # ================================================================================================
        # 🚨 CRITICAL FIX: APPLY RADAR MODE FILTERING (PREVIOUSLY MISSING!)
        # ================================================================================================
        
        # Apply Radar Mode filtering
        original_count = len(radar_df)
        
        if radar_mode == "🌊 Wave Hunter":
            radar_df = radar_df[
                (radar_df.get('momentum_score', 0) >= 50) &
                (radar_df.get('rvol', 0) >= 1.5) &
                (radar_df.get('acceleration_score', 0) >= 40)
            ]
            st.info(f"🌊 Wave Hunter Mode: {len(radar_df)}/{original_count} stocks meet momentum+volume criteria")
            
        elif radar_mode == "⚡ Breakout Scanner":
            radar_df = radar_df[radar_df.get('breakout_score', 0) >= 65]
            st.info(f"⚡ Breakout Scanner Mode: {len(radar_df)}/{original_count} stocks have breakout score ≥65")
            
        elif radar_mode == "🏗️ Pattern Recognition":
            if 'patterns' in radar_df.columns:
                radar_df = radar_df[radar_df['patterns'].str.len() > 0]
                st.info(f"🏗️ Pattern Recognition Mode: {len(radar_df)}/{original_count} stocks have detected patterns")
            else:
                st.warning("🏗️ Pattern Recognition Mode: 'patterns' column not available - using all data")
                
        elif radar_mode == "💰 Institutional Flow":
            flow_filter = (
                (radar_df.get('money_flow_mm', 0).abs() >= 10) |
                (radar_df.get('rvol', 0) >= 3.0)
            )
            radar_df = radar_df[flow_filter]
            st.info(f"💰 Institutional Flow Mode: {len(radar_df)}/{original_count} stocks show institutional activity")
            
        elif radar_mode == "🔥 Momentum Surge":
            momentum_filter = (
                (radar_df.get('momentum_score', 0) >= 60) &
                (radar_df.get('acceleration_score', 0) >= 50)
            )
            radar_df = radar_df[momentum_filter]
            st.info(f"🔥 Momentum Surge Mode: {len(radar_df)}/{original_count} stocks in momentum surge")
            
        # 📊 Full Spectrum uses all data (no additional filtering)
        elif radar_mode == "📊 Full Spectrum":
            pass  # No filtering applied
        
        # ================================================================================================
        # 🚨 CRITICAL FIX: APPLY RISK PROFILE FILTERING (PREVIOUSLY MISSING!)
        # ================================================================================================
        
        pre_risk_count = len(radar_df)
        
        if risk_filter == "🛡️ Low Risk Only":
            risk_filter_condition = (
                (radar_df.get('pe', 999) < 25) &
                (radar_df.get('volatility_score', 100) < 60) &
                (radar_df.get('master_score', 0) >= 50)
            )
            radar_df = radar_df[risk_filter_condition]
            st.info(f"🛡️ Low Risk Filter: {len(radar_df)}/{pre_risk_count} stocks meet low-risk criteria")
            
        elif risk_filter == "🚀 High Risk/Reward":
            risk_filter_condition = (
                (radar_df.get('momentum_score', 0) >= 70) |
                (radar_df.get('from_low_pct', 0) >= 100) |
                (radar_df.get('volatility_score', 0) >= 70)
            )
            radar_df = radar_df[risk_filter_condition]
            st.info(f"🚀 High Risk/Reward Filter: {len(radar_df)}/{pre_risk_count} high-potential stocks")
            
        elif risk_filter == "🔥 Maximum Alpha":
            risk_filter_condition = (
                (radar_df.get('master_score', 0) >= 80) &
                (radar_df.get('rvol', 0) >= 2.0) &
                (radar_df.get('momentum_score', 0) >= 65)
            )
            radar_df = radar_df[risk_filter_condition]
            st.info(f"🔥 Maximum Alpha Filter: {len(radar_df)}/{pre_risk_count} elite alpha-generating stocks")
            
        # ⚖️ Balanced Risk uses current data (no additional filtering)
        elif risk_filter == "⚖️ Balanced Risk":
            pass  # No filtering applied
        
        # ================================================================================================
        # 🚨 CRITICAL FIX: APPLY MARKET REGIME FILTERING (PREVIOUSLY MISSING!)
        # ================================================================================================
        
        pre_regime_count = len(radar_df)
        
        if market_regime == "🐂 Bull Market":
            regime_filter = (
                (radar_df.get('momentum_score', 0) >= 60) &
                (radar_df.get('master_score', 0) >= 50)
            )
            radar_df = radar_df[regime_filter]
            st.info(f"🐂 Bull Market Filter: {len(radar_df)}/{pre_regime_count} stocks aligned with bull market")
            
        elif market_regime == "🐻 Bear Market":
            regime_filter = (
                (radar_df.get('from_high_pct', 0) < -20) &
                (radar_df.get('master_score', 0) >= 50)
            )
            radar_df = radar_df[regime_filter]
            st.info(f"🐻 Bear Market Filter: {len(radar_df)}/{pre_regime_count} defensive/value stocks")
            
        elif market_regime == "🔄 Sideways/Choppy":
            regime_filter = (
                (radar_df.get('from_high_pct', 0) > -20) &
                (radar_df.get('from_high_pct', 0) < 5) &
                (radar_df.get('master_score', 0) >= 45)
            )
            radar_df = radar_df[regime_filter]
            st.info(f"🔄 Sideways Market Filter: {len(radar_df)}/{pre_regime_count} range-bound opportunities")
            
        # 📊 Auto-Detect: Detect market regime and apply appropriate filter
        elif market_regime == "📊 Auto-Detect":
            # Detect the current market regime using the full filtered dataset
            detected_regime, regime_metrics = MarketIntelligence.detect_market_regime(filtered_df)
            
            # Map detected regime to appropriate filter
            if detected_regime == "🔥 RISK-ON BULL":
                # Apply Bull Market filter
                regime_filter = (
                    (radar_df.get('momentum_score', 0) >= 60) &
                    (radar_df.get('master_score', 0) >= 50)
                )
                radar_df = radar_df[regime_filter]
                st.success(f"📊 Auto-Detected: {detected_regime} → Applying Bull Market filter: {len(radar_df)}/{pre_regime_count} momentum stocks")
                
            elif detected_regime == "🛡️ RISK-OFF DEFENSIVE":
                # Apply Bear Market filter
                regime_filter = (
                    (radar_df.get('from_high_pct', 0) < -20) &
                    (radar_df.get('master_score', 0) >= 50)
                )
                radar_df = radar_df[regime_filter]
                st.warning(f"📊 Auto-Detected: {detected_regime} → Applying Bear Market filter: {len(radar_df)}/{pre_regime_count} defensive/value stocks")
                
            else:
                # For ⚡ VOLATILE OPPORTUNITY, 😴 RANGE-BOUND, 🔄 MIXED SIGNALS → Apply Sideways filter
                regime_filter = (
                    (radar_df.get('from_high_pct', 0) > -20) &
                    (radar_df.get('from_high_pct', 0) < 5) &
                    (radar_df.get('master_score', 0) >= 45)
                )
                radar_df = radar_df[regime_filter]
                st.info(f"📊 Auto-Detected: {detected_regime} → Applying Sideways filter: {len(radar_df)}/{pre_regime_count} range-bound opportunities")
        
        # ================================================================================================
        # 🚨 CRITICAL FIX: CREATE SENSITIVITY THRESHOLD FUNCTION (PREVIOUSLY MISSING!)
        # ================================================================================================
        
        def get_sensitivity_thresholds(sensitivity_level):
            """Get thresholds based on sensitivity level for consistent application across all tabs"""
            if "Conservative" in sensitivity_level:
                return {
                    'momentum': 70, 'acceleration': 80, 'rvol': 2.5,
                    'breakout': 80, 'pattern': 75, 'trend': 80, 'confluence': 70,
                    'institutional': 50, 'probability': 75
                }
            elif "Balanced" in sensitivity_level:
                return {
                    'momentum': 60, 'acceleration': 70, 'rvol': 2.0, 
                    'breakout': 70, 'pattern': 65, 'trend': 70, 'confluence': 60,
                    'institutional': 25, 'probability': 65
                }
            else:  # Aggressive
                return {
                    'momentum': 50, 'acceleration': 60, 'rvol': 1.5,
                    'breakout': 60, 'pattern': 55, 'trend': 60, 'confluence': 50,
                    'institutional': 10, 'probability': 55
                }
        
        # Get sensitivity thresholds for use throughout all analysis
        thresholds = get_sensitivity_thresholds(sensitivity_level)
        
        if not radar_df.empty:
            
            # ============================================================================================
            # 📊 REAL-TIME MARKET OVERVIEW DASHBOARD
            # ============================================================================================
            
            st.markdown("### 📊 REAL-TIME MARKET INTELLIGENCE")
            
            # Market Overview Metrics
            overview_cols = st.columns([2, 2, 2, 2, 2])
            
            try:
                with overview_cols[0]:
                    total_stocks = len(radar_df)
                    strong_stocks = len(radar_df[radar_df['master_score'] >= 70]) if 'master_score' in radar_df.columns else 0
                    strength_pct = (strong_stocks / total_stocks * 100) if total_stocks > 0 else 0
                    
                    UIComponents.render_metric_card(
                        "Market Strength",
                        f"{strength_pct:.1f}%",
                        f"{strong_stocks}/{total_stocks} stocks"
                    )
                
                with overview_cols[1]:
                    if 'rvol' in radar_df.columns:
                        high_volume = len(radar_df[radar_df['rvol'] >= 2.0])
                        volume_pct = (high_volume / total_stocks * 100) if total_stocks > 0 else 0
                        volume_status = "🔥 Active" if volume_pct > 30 else "📊 Normal" if volume_pct > 15 else "💤 Quiet"
                        
                        UIComponents.render_metric_card(
                            "Volume Activity",
                            f"{volume_pct:.1f}%",
                            f"{volume_status}"
                        )
                    else:
                        UIComponents.render_metric_card("Volume Activity", "N/A", "Data not available")
                
                with overview_cols[2]:
                    if 'momentum_score' in radar_df.columns:
                        momentum_avg = radar_df['momentum_score'].mean()
                        momentum_trend = "🚀 Bullish" if momentum_avg > 60 else "📊 Neutral" if momentum_avg > 40 else "🐻 Bearish"
                        
                        UIComponents.render_metric_card(
                            "Momentum Regime",
                            f"{momentum_avg:.0f}",
                            momentum_trend
                        )
                    else:
                        UIComponents.render_metric_card("Momentum Regime", "N/A", "Data not available")
                
                with overview_cols[3]:
                    if 'patterns' in radar_df.columns:
                        pattern_stocks = len(radar_df[radar_df['patterns'].str.len() > 0])
                        pattern_pct = (pattern_stocks / total_stocks * 100) if total_stocks > 0 else 0
                        pattern_activity = "🎯 High" if pattern_pct > 20 else "📈 Moderate" if pattern_pct > 10 else "📊 Low"
                        
                        UIComponents.render_metric_card(
                            "Pattern Activity",
                            f"{pattern_pct:.1f}%",
                            f"{pattern_activity}"
                        )
                    else:
                        UIComponents.render_metric_card("Pattern Activity", "N/A", "Data not available")
                
                with overview_cols[4]:
                    if 'breakout_score' in radar_df.columns:
                        breakout_candidates = len(radar_df[radar_df['breakout_score'] >= 75])
                        breakout_pct = (breakout_candidates / total_stocks * 100) if total_stocks > 0 else 0
                        breakout_status = "🚀 Explosive" if breakout_pct > 15 else "📈 Active" if breakout_pct > 8 else "📊 Quiet"
                        
                        UIComponents.render_metric_card(
                            "Breakout Potential",
                            f"{breakout_pct:.1f}%",
                            breakout_status
                        )
                    else:
                        UIComponents.render_metric_card("Breakout Potential", "N/A", "Data not available")
            
            except Exception as e:
                logger.error(f"Error in market overview: {str(e)}")
                st.error("Error calculating market overview metrics")
            
            st.markdown("---")
        
        if show_sensitivity_details:
            with st.expander("📊 Current Sensitivity Thresholds", expanded=True):
                if "Conservative" in sensitivity_level:
                    st.markdown("""
                    **Conservative Settings** 🛡️
                    - **Momentum Shifts:** Score ≥ 60, Acceleration ≥ 70
                    - **Emerging Patterns:** Within 5% of qualifying threshold
                    - **Volume Surges:** RVOL ≥ 3.0x (extreme volumes only)
                    - **Acceleration Alerts:** Score ≥ 85 (strongest signals)
                    - **Pattern Distance:** 5% from qualification
                    """)
                elif "Balanced" in sensitivity_level:
                    st.markdown("""
                    **Balanced Settings** ⚖️
                    - **Momentum Shifts:** Score ≥ 50, Acceleration ≥ 60
                    - **Emerging Patterns:** Within 10% of qualifying threshold
                    - **Volume Surges:** RVOL ≥ 2.0x (standard threshold)
                    - **Acceleration Alerts:** Score ≥ 70 (good acceleration)
                    - **Pattern Distance:** 10% from qualification
                    """)
                else:  # Aggressive
                    st.markdown("""
                    **Aggressive Settings** 🚀
                    - **Momentum Shifts:** Score ≥ 40, Acceleration ≥ 50
                    - **Emerging Patterns:** Within 15% of qualifying threshold
                    - **Volume Surges:** RVOL ≥ 1.5x (building volume)
                    - **Acceleration Alerts:** Score ≥ 60 (early signals)
                    - **Pattern Distance:** 15% from qualification
                    """)
                
                st.info("💡 **Tip**: Start with Balanced, then adjust based on market conditions and your risk tolerance.")
        
        # Convert timeframe_focus to wave_timeframe for backward compatibility
        wave_timeframe_map = {
            "🚀 Intraday (1D)": "Intraday Surge",
            "📈 Short-term (3D)": "3-Day Buildup", 
            "🌊 Medium-term (7D)": "Weekly Breakout",
            "💪 Long-term (30D)": "Monthly Trend",
            "🔄 Multi-timeframe": "All Waves"
        }
        wave_timeframe = wave_timeframe_map.get(timeframe_focus, "All Waves")
        
        # Initialize wave_filtered_df
        wave_filtered_df = radar_df.copy()
        
        if wave_timeframe != "All Waves":
            try:
                if wave_timeframe == "Intraday Surge":
                    required_cols = ['rvol', 'ret_1d', 'price', 'prev_close']
                    if all(col in wave_filtered_df.columns for col in required_cols):
                        wave_filtered_df = wave_filtered_df[
                            (wave_filtered_df['rvol'] >= 2.5) &
                            (wave_filtered_df['ret_1d'] > 2) &
                            (wave_filtered_df['price'] > wave_filtered_df['prev_close'] * 1.02)
                        ]
                    
                elif wave_timeframe == "3-Day Buildup":
                    required_cols = ['ret_3d', 'vol_ratio_7d_90d', 'price', 'sma_20d']
                    if all(col in wave_filtered_df.columns for col in required_cols):
                        wave_filtered_df = wave_filtered_df[
                            (wave_filtered_df['ret_3d'] > 5) &
                            (wave_filtered_df['vol_ratio_7d_90d'] > 1.5) &
                            (wave_filtered_df['price'] > wave_filtered_df['sma_20d'])
                        ]
                
                elif wave_timeframe == "Weekly Breakout":
                    required_cols = ['ret_7d', 'vol_ratio_7d_90d', 'from_high_pct']
                    if all(col in wave_filtered_df.columns for col in required_cols):
                        wave_filtered_df = wave_filtered_df[
                            (wave_filtered_df['ret_7d'] > 8) &
                            (wave_filtered_df['vol_ratio_7d_90d'] > 2.0) &
                            (wave_filtered_df['from_high_pct'] > -10)
                        ]
                
                elif wave_timeframe == "Monthly Trend":
                    required_cols = ['ret_30d', 'vol_ratio_30d_180d', 'from_low_pct']
                    if all(col in wave_filtered_df.columns for col in required_cols):
                        wave_filtered_df = wave_filtered_df[
                            (wave_filtered_df['ret_30d'] > 15) &
                            (wave_filtered_df['vol_ratio_30d_180d'] > 1.2) &
                            (wave_filtered_df['from_low_pct'] > 30)
                        ]
            except Exception as e:
                logger.warning(f"Error applying {wave_timeframe} filter: {str(e)}")
                st.warning(f"Some data not available for {wave_timeframe} filter")
        
        if not wave_filtered_df.empty:
            st.markdown("#### 🚀 Momentum Shifts - Stocks Entering Strength")
            
            if "Conservative" in sensitivity_level:
                momentum_threshold = 60
                acceleration_threshold = 70
                min_rvol = 3.0
            elif "Balanced" in sensitivity_level:
                momentum_threshold = 50
                acceleration_threshold = 60
                min_rvol = 2.0
            else:
                momentum_threshold = 40
                acceleration_threshold = 50
                min_rvol = 1.5
            
            momentum_shifts = wave_filtered_df[
                (wave_filtered_df['momentum_score'] >= momentum_threshold) & 
                (wave_filtered_df['acceleration_score'] >= acceleration_threshold)
            ].copy()
            
            if len(momentum_shifts) > 0:
                momentum_shifts['signal_count'] = 0
                momentum_shifts.loc[momentum_shifts['momentum_score'] >= momentum_threshold, 'signal_count'] += 1
                momentum_shifts.loc[momentum_shifts['acceleration_score'] >= acceleration_threshold, 'signal_count'] += 1
                momentum_shifts.loc[momentum_shifts['rvol'] >= min_rvol, 'signal_count'] += 1
                if 'breakout_score' in momentum_shifts.columns:
                    momentum_shifts.loc[momentum_shifts['breakout_score'] >= 75, 'signal_count'] += 1
                if 'vol_ratio_7d_90d' in momentum_shifts.columns:
                    momentum_shifts.loc[momentum_shifts['vol_ratio_7d_90d'] >= 1.5, 'signal_count'] += 1
                
                momentum_shifts['shift_strength'] = (
                    momentum_shifts['momentum_score'] * 0.4 +
                    momentum_shifts['acceleration_score'] * 0.4 +
                    momentum_shifts['rvol_score'] * 0.2
                )
                
                top_shifts = momentum_shifts.sort_values(['signal_count', 'shift_strength'], ascending=[False, False]).head(20)
                
                display_columns = ['ticker', 'company_name', 'master_score', 'momentum_score', 
                                 'acceleration_score', 'rvol', 'signal_count', 'market_state']
                
                if 'ret_7d' in top_shifts.columns:
                    display_columns.insert(-2, 'ret_7d')
                
                display_columns.append('category')
                
                shift_display = top_shifts[[col for col in display_columns if col in top_shifts.columns]].copy()
                
                shift_display['Signals'] = shift_display['signal_count'].apply(
                    lambda x: f"{'🔥' * min(x, 3)} {x}/5"
                )
                
                if 'ret_7d' in shift_display.columns:
                    shift_display['7D Return'] = shift_display['ret_7d'].apply(lambda x: f"{x:.1f}%" if pd.notna(x) else '-')
                
                if 'rvol' in shift_display.columns:
                    shift_display['RVOL'] = shift_display['rvol'].apply(lambda x: f"{x:.1f}x" if pd.notna(x) else '-')
                    shift_display = shift_display.drop('rvol', axis=1)
                
                rename_dict = {
                    'ticker': 'Ticker',
                    'company_name': 'Company',
                    'master_score': 'Score',
                    'momentum_score': 'Momentum',
                    'acceleration_score': 'Acceleration',
                    'market_state': 'market_state',
                    'category': 'Category'
                }
                
                shift_display = shift_display.rename(columns=rename_dict)
                
                if 'signal_count' in shift_display.columns:
                    shift_display = shift_display.drop('signal_count', axis=1)
                
                # OPTIMIZED DATAFRAME WITH COLUMN_CONFIG
                st.dataframe(
                    shift_display, 
                    width="stretch", 
                    hide_index=True,
                    column_config={
                        'Ticker': st.column_config.TextColumn(
                            'Ticker',
                            help="Stock symbol",
                            width="small"
                        ),
                        'Company': st.column_config.TextColumn(
                            'Company',
                            help="Company name",
                            width="medium"
                        ),
                        'Score': st.column_config.ProgressColumn(
                            'Score',
                            help="Master Score",
                            format="%.1f",
                            min_value=0,
                            max_value=100,
                            width="small"
                        ),
                        'Momentum': st.column_config.ProgressColumn(
                            'Momentum',
                            help="Momentum Score",
                            format="%.0f",
                            min_value=0,
                            max_value=100,
                            width="small"
                        ),
                        'Acceleration': st.column_config.ProgressColumn(
                            'Acceleration',
                            help="Acceleration Score",
                            format="%.0f",
                            min_value=0,
                            max_value=100,
                            width="small"
                        ),
                        'RVOL': st.column_config.TextColumn(
                            'RVOL',
                            help="Relative Volume",
                            width="small"
                        ),
                        'Signals': st.column_config.TextColumn(
                            'Signals',
                            help="Signal strength indicator",
                            width="small"
                        ),
                        '7D Return': st.column_config.TextColumn(
                            '7D Return',
                            help="7-day return percentage",
                            width="small"
                        ),
                        'Market State': st.column_config.TextColumn(
                            'Market State',
                            help="Current market state",
                            width="medium"
                        ),
                        'Category': st.column_config.TextColumn(
                            'Category',
                            help="Market cap category",
                            width="medium"
                        )
                    }
                )
                
                multi_signal = len(top_shifts[top_shifts['signal_count'] >= 3])
                if multi_signal > 0:
                    st.success(f"🏆 Found {multi_signal} stocks with 3+ signals (strongest momentum)")
                
                super_signals = top_shifts[top_shifts['signal_count'] >= 4]
                if len(super_signals) > 0:
                    st.warning(f"🔥🔥 {len(super_signals)} stocks showing EXTREME momentum (4+ signals)!")
            else:
                st.info(f"No momentum shifts detected in {wave_timeframe} timeframe. Try 'Aggressive' sensitivity.")
            
            st.markdown("#### 🚀 Acceleration Profiles - Momentum Building Over Time")
            
            if "Conservative" in sensitivity_level:
                accel_threshold = 85
            elif "Balanced" in sensitivity_level:
                accel_threshold = 70
            else:
                accel_threshold = 60
            
            accelerating_stocks = wave_filtered_df[
                wave_filtered_df['acceleration_score'] >= accel_threshold
            ].nlargest(10, 'acceleration_score')
            
            if len(accelerating_stocks) > 0:
                fig_accel = Visualizer.create_acceleration_profiles(accelerating_stocks, n=10)
                st.plotly_chart(fig_accel, width="stretch", theme="streamlit")
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    perfect_accel = len(accelerating_stocks[accelerating_stocks['acceleration_score'] >= 90])
                    st.metric("Perfect Acceleration (90+)", perfect_accel)
                with col2:
                    strong_accel = len(accelerating_stocks[accelerating_stocks['acceleration_score'] >= 80])
                    st.metric("Strong Acceleration (80+)", strong_accel)
                with col3:
                    avg_accel = accelerating_stocks['acceleration_score'].mean()
                    st.metric("Avg Acceleration Score", f"{avg_accel:.1f}")
            else:
                st.info(f"No stocks meet the acceleration threshold ({accel_threshold}+) for {sensitivity_level} sensitivity.")
            
            if show_market_regime:
                st.markdown("#### 💰 Category Rotation - Smart Money Flow")
                
                col1, col2 = st.columns([3, 2])
                
                with col1:
                    try:
                        # Use professional category rotation detection
                        category_rotation = MarketIntelligence.detect_category_rotation(wave_filtered_df)
                        
                        if not category_rotation.empty:
                            # Prepare data for visualization
                            category_flow = category_rotation.copy()
                            
                            # Rename columns for display compatibility
                            if 'flow_score' in category_flow.columns:
                                category_flow = category_flow.rename(columns={'flow_score': 'Flow Score'})
                            if 'avg_score' in category_flow.columns:
                                category_flow = category_flow.rename(columns={'avg_score': 'Avg Score'})
                            if 'analyzed_stocks' in category_flow.columns:
                                category_flow = category_flow.rename(columns={'analyzed_stocks': 'Count'})
                            if 'avg_momentum' in category_flow.columns:
                                category_flow = category_flow.rename(columns={'avg_momentum': 'Avg Momentum'})
                            if 'avg_volume' in category_flow.columns:
                                category_flow = category_flow.rename(columns={'avg_volume': 'Avg Volume'})
                            
                            # Determine flow direction based on market dynamics (NOT just category names)
                            top_category = category_flow.index[0] if len(category_flow) > 0 else ""
                            top_score = category_flow['Flow Score'].iloc[0] if len(category_flow) > 0 else 0
                            avg_score = category_flow['Flow Score'].mean()
                            
                            # Get small cap and large cap performance
                            small_caps_avg = category_flow[category_flow.index.str.contains('Small|Micro', na=False)]['Flow Score'].mean() if any(category_flow.index.str.contains('Small|Micro', na=False)) else 0
                            large_caps_avg = category_flow[category_flow.index.str.contains('Large|Mega', na=False)]['Flow Score'].mean() if any(category_flow.index.str.contains('Large|Mega', na=False)) else 0
                            
                            # Professional Market Regime Detection
                            if avg_score > 60:  # Strong overall market
                                if small_caps_avg > large_caps_avg + 5:
                                    flow_direction = "🔥 RISK-ON (Small Cap Rally)"
                                elif large_caps_avg > 65:
                                    flow_direction = "🔥 RISK-ON (Bull Market)"
                                else:
                                    flow_direction = "📈 RISK-ON"
                            elif avg_score < 40:  # Weak overall market
                                if large_caps_avg > small_caps_avg + 10:
                                    flow_direction = "❄️ RISK-OFF (Flight to Quality)"
                                else:
                                    flow_direction = "❄️ RISK-OFF"
                            else:  # Neutral market (40-60)
                                if abs(small_caps_avg - large_caps_avg) < 5:
                                    flow_direction = "➡️ NEUTRAL (Balanced)"
                                elif small_caps_avg > large_caps_avg:
                                    flow_direction = "🌱 RISK-ON (Early Stage)"
                                else:
                                    flow_direction = "🛡️ DEFENSIVE (Large Cap Preference)"
                            
                            # Create professional visualization with color zones
                            fig_flow = go.Figure()
                            
                            # Create color mapping based on flow score (same as sector rotation)
                            colors = []
                            for score in category_flow['Flow Score']:
                                if score > 75:
                                    colors.append('#00ff00')  # Bright green for hot categories
                                elif score > 60:
                                    colors.append('#32cd32')  # Green for strong categories
                                elif score > 45:
                                    colors.append('#ffd700')  # Gold for neutral
                                elif score > 30:
                                    colors.append('#ff8c00')  # Orange for weak
                                else:
                                    colors.append('#ff4500')  # Red for very weak
                            
                            fig_flow.add_trace(go.Bar(
                                x=category_flow.index,
                                y=category_flow['Flow Score'],
                                text=[f"{val:.1f}" for val in category_flow['Flow Score']],
                                textposition='outside',
                                marker_color=colors,
                                marker_line=dict(color='rgba(0,0,0,0.3)', width=1),
                                hovertemplate=(
                                    '<b>%{x}</b><br>'
                                    'Flow Score: %{y:.1f}<br>'
                                    'LDI Score: %{customdata[0]:.1f}%<br>'
                                    'Market Leaders: %{customdata[1]} of %{customdata[2]}<br>'
                                    'Leadership Quality: %{customdata[3]}<br>'
                                    'Avg Score: %{customdata[4]:.1f}<extra></extra>'
                                ),
                                customdata=[[category_rotation.loc[cat, 'ldi_score'] if 'ldi_score' in category_rotation.columns else 0,
                                           category_flow.loc[cat, 'Count'],
                                           category_rotation.loc[cat, 'total_stocks'] if 'total_stocks' in category_rotation.columns else category_flow.loc[cat, 'Count'],
                                           category_rotation.loc[cat, 'ldi_quality'] if 'ldi_quality' in category_rotation.columns else 'N/A',
                                           category_flow.loc[cat, 'Avg Score'] if 'Avg Score' in category_flow.columns else 0] 
                                          for cat in category_flow.index]
                            ))
                            
                            # Enhanced layout with professional styling
                            fig_flow.update_layout(
                                title=f"🎯 INSTITUTIONAL CATEGORY ROTATION MAP - {flow_direction}",
                                xaxis_title="Market Cap Category",
                                yaxis_title="Enhanced Flow Score",
                                height=450,
                                template='plotly_white',
                                showlegend=False,
                                plot_bgcolor='rgba(0,0,0,0)',
                                paper_bgcolor='rgba(0,0,0,0)',
                                font=dict(size=12, color='#2c3e50'),
                                title_font=dict(size=16, color='#2c3e50')
                            )
                            
                            # Add horizontal reference lines for zones
                            fig_flow.add_hline(y=75, line_dash="dash", line_color="green", annotation_text="Hot Zone")
                            fig_flow.add_hline(y=50, line_dash="dash", line_color="orange", annotation_text="Neutral Zone")
                            fig_flow.add_hline(y=25, line_dash="dash", line_color="red", annotation_text="Cold Zone")
                            
                            st.plotly_chart(fig_flow, width="stretch", theme="streamlit")
                            
                            # Add zone-based insights
                            hot_categories = category_flow[category_flow['Flow Score'] > 75]
                            if len(hot_categories) > 0:
                                st.success(f"🔥 **HOT CATEGORIES**: {', '.join(hot_categories.index[:3])}")
                            
                            cold_categories = category_flow[category_flow['Flow Score'] < 35]
                            if len(cold_categories) > 0:
                                st.warning(f"❄️ **AVOID CATEGORIES**: {', '.join(cold_categories.index[-2:])}")
                            
                            neutral_categories = category_flow[(category_flow['Flow Score'] >= 45) & (category_flow['Flow Score'] <= 60)]
                            if len(neutral_categories) > 0:
                                st.info(f"⚖️ **NEUTRAL CATEGORIES**: {', '.join(neutral_categories.index[:2])}")
                        else:
                            st.info("Insufficient data for professional category rotation analysis.")
                            
                    except Exception as e:
                        logger.error(f"Error in professional category rotation analysis: {str(e)}")
                        st.error("Unable to analyze category rotation")
                
                with col2:
                    if 'category_flow' in locals() and not category_flow.empty:
                        st.markdown(f"**🎯 Market Regime: {flow_direction}**")
                        
                        st.markdown("**💎 Strongest Categories:**")
                        for i, (cat, row) in enumerate(category_flow.head(3).iterrows()):
                            emoji = "🥇" if i == 0 else "🥈" if i == 1 else "🥉"
                            quality = category_rotation.loc[cat, 'ldi_quality'] if 'ldi_quality' in category_rotation.columns else ''
                            quality_emoji = {'Elite': '🏆', 'Strong': '💪', 'Moderate': '📈', 'Weak': '😐'}.get(quality, '')
                            st.write(f"{emoji} **{cat}**: Score {row['Flow Score']:.1f} {quality_emoji}{quality}")
                        
                        st.markdown("**🔄 Category Shifts:**")
                        small_caps_score = category_flow[category_flow.index.str.contains('Small|Micro', na=False)]['Flow Score'].mean() if any(category_flow.index.str.contains('Small|Micro', na=False)) else 0
                        large_caps_score = category_flow[category_flow.index.str.contains('Large|Mega', na=False)]['Flow Score'].mean() if any(category_flow.index.str.contains('Large|Mega', na=False)) else 0
                        
                        # More sophisticated shift analysis
                        score_diff = small_caps_score - large_caps_score
                        
                        if score_diff > 15:
                            st.success("🚀 Small Caps Surging - Strong Risk-On Signal!")
                        elif score_diff > 5:
                            st.success("🚀📈 Small Caps Leading - Early Bull Signal!")
                        elif score_diff < -15:
                            st.error("🛡️ Flight to Large Caps - Strong Risk-Off Signal!")
                        elif score_diff < -5:
                            st.warning("📉 Large Caps Leading - Defensive Mode")
                        else:
                            st.info("⚖️ Balanced Market - No Clear Size Preference")
                        
                        # Additional market insights
                        if avg_score > 70:
                            st.success(f"💪 **Strong Market** (Avg Score: {avg_score:.1f})")
                        elif avg_score < 35:
                            st.error(f"😰 **Weak Market** (Avg Score: {avg_score:.1f})")
                        else:
                            st.info(f"📊 **Mixed Market** (Avg Score: {avg_score:.1f})")
                        
                        # Show LDI quality breakdown
                        if 'ldi_quality' in category_rotation.columns:
                            st.markdown("**🎯 Leadership Quality:**")
                            quality_counts = category_rotation['ldi_quality'].value_counts()
                            for quality, count in quality_counts.items():
                                emoji = {'Elite': '🏆', 'Strong': '💪', 'Moderate': '📈', 'Weak': '😐'}.get(quality, '')
                                st.write(f"{emoji} {quality}: {count} categories")
                    else:
                        st.info("Category data not available")
            
            st.markdown("#### 🎯 Emerging Patterns - About to Qualify")
            
            pattern_distance_map = {"Conservative": 5, "Balanced": 10, "Aggressive": 15}
            pattern_distance = 10  # Default
            for key in pattern_distance_map:
                if key in sensitivity_level:
                    pattern_distance = pattern_distance_map[key]
                    break
            
            emergence_data = []
            
            if 'category_percentile' in wave_filtered_df.columns:
                close_to_leader = wave_filtered_df[
                    (wave_filtered_df['category_percentile'] >= (90 - pattern_distance)) & 
                    (wave_filtered_df['category_percentile'] < 90)
                ]
                for _, stock in close_to_leader.iterrows():
                    emergence_data.append({
                        'Ticker': stock['ticker'],
                        'Company': stock['company_name'],
                        'Pattern': '🐱 CAT LEADER',
                        'Distance': f"{90 - stock['category_percentile']:.1f}% away",
                        'Current': f"{stock['category_percentile']:.1f}%ile",
                        'Score': stock['master_score']
                    })
            
            if 'breakout_score' in wave_filtered_df.columns:
                close_to_breakout = wave_filtered_df[
                    (wave_filtered_df['breakout_score'] >= (80 - pattern_distance)) & 
                    (wave_filtered_df['breakout_score'] < 80)
                ]
                for _, stock in close_to_breakout.iterrows():
                    emergence_data.append({
                        'Ticker': stock['ticker'],
                        'Company': stock['company_name'],
                        'Pattern': '🎯 HIGH BREAKOUT SCORE',
                        'Distance': f"{80 - stock['breakout_score']:.1f} pts away",
                        'Current': f"{stock['breakout_score']:.1f} score",
                        'Score': stock['master_score']
                    })
            
            if emergence_data:
                emergence_df = pd.DataFrame(emergence_data).sort_values('Score', ascending=False).head(15)
                
                col1, col2 = st.columns([3, 1])
                with col1:
                    # OPTIMIZED DATAFRAME WITH COLUMN_CONFIG
                    st.dataframe(
                        emergence_df, 
                        width="stretch", 
                        hide_index=True,
                        column_config={
                            'Ticker': st.column_config.TextColumn(
                                'Ticker',
                                help="Stock symbol",
                                width="small"
                            ),
                            'Company': st.column_config.TextColumn(
                                'Company',
                                help="Company name",
                                width="medium"
                            ),
                            'Pattern': st.column_config.TextColumn(
                                'Pattern',
                                help="Pattern about to emerge",
                                width="medium"
                            ),
                            'Distance': st.column_config.TextColumn(
                                'Distance',
                                help="Distance from pattern qualification",
                                width="small"
                            ),
                            'Current': st.column_config.TextColumn(
                                'Current',
                                help="Current value",
                                width="small"
                            ),
                            'Score': st.column_config.ProgressColumn(
                                'Score',
                                help="Master Score",
                                format="%.1f",
                                min_value=0,
                                max_value=100,
                                width="small"
                            )
                        }
                    )
                with col2:
                    UIComponents.render_metric_card("Emerging Patterns", len(emergence_df))
            else:
                st.info(f"No patterns emerging within {pattern_distance}% threshold.")
            
            st.markdown("#### 🌊 Volume Surges - Unusual Activity NOW")
            
            rvol_threshold_map = {"Conservative": 3.0, "Balanced": 2.0, "Aggressive": 1.5}
            rvol_threshold = 2.0  # Default
            for key in rvol_threshold_map:
                if key in sensitivity_level:
                    rvol_threshold = rvol_threshold_map[key]
                    break
            
            volume_surges = wave_filtered_df[wave_filtered_df['rvol'] >= rvol_threshold].copy()
            
            if len(volume_surges) > 0:
                volume_surges['surge_score'] = (
                    volume_surges['rvol_score'] * 0.5 +
                    volume_surges['volume_score'] * 0.3 +
                    volume_surges['momentum_score'] * 0.2
                )
                
                top_surges = volume_surges.nlargest(15, 'surge_score')
                
                col1, col2 = st.columns([2, 1])
                
                with col1:
                    display_cols = ['ticker', 'company_name', 'rvol', 'price', 'money_flow_mm', 'market_state', 'category']
                    
                    if 'ret_1d' in top_surges.columns:
                        display_cols.insert(3, 'ret_1d')
                    
                    surge_display = top_surges[[col for col in display_cols if col in top_surges.columns]].copy()
                    
                    surge_display['Type'] = surge_display['rvol'].apply(
                        lambda x: "🔥🔥🔥" if x > 5 else "🔥🔥" if x > 3 else "🔥"
                    )
                    
                    if 'ret_1d' in surge_display.columns:
                        surge_display['ret_1d'] = surge_display['ret_1d'].apply(lambda x: f"{x:+.1f}%" if pd.notna(x) else '-')
                    
                    if 'money_flow_mm' in surge_display.columns:
                        surge_display['money_flow_mm'] = surge_display['money_flow_mm'].apply(lambda x: f"₹{x:.1f}M" if pd.notna(x) else '-')
                    
                    surge_display['price'] = surge_display['price'].apply(lambda x: f"₹{x:,.0f}" if pd.notna(x) else '-')
                    surge_display['rvol'] = surge_display['rvol'].apply(lambda x: f"{x:.1f}x" if pd.notna(x) else '-')
                    
                    rename_dict = {
                        'ticker': 'Ticker',
                        'company_name': 'Company',
                        'rvol': 'RVOL',
                        'price': 'Price',
                        'money_flow_mm': 'Money Flow',
                        'market_state': 'Market State',
                        'category': 'Category',
                        'ret_1d': '1D Ret'
                    }
                    surge_display = surge_display.rename(columns=rename_dict)
                    
                    # OPTIMIZED DATAFRAME WITH COLUMN_CONFIG
                    st.dataframe(
                        surge_display, 
                        width="stretch", 
                        hide_index=True,
                        column_config={
                            'Type': st.column_config.TextColumn(
                                'Type',
                                help="Volume surge intensity",
                                width="small"
                            ),
                            'Ticker': st.column_config.TextColumn(
                                'Ticker',
                                help="Stock symbol",
                                width="small"
                            ),
                            'Company': st.column_config.TextColumn(
                                'Company',
                                help="Company name",
                                width="medium"
                            ),
                            'RVOL': st.column_config.TextColumn(
                                'RVOL',
                                help="Relative Volume",
                                width="small"
                            ),
                            'Price': st.column_config.TextColumn(
                                'Price',
                                help="Current price",
                                width="small"
                            ),
                            '1D Ret': st.column_config.TextColumn(
                                '1D Ret',
                                help="1-day return",
                                width="small"
                            ),
                            'Money Flow': st.column_config.TextColumn(
                                'Money Flow',
                                help="Money flow in millions",
                                width="small"
                            ),
                            'Market State': st.column_config.TextColumn(
                                'Market State',
                                help="Current market state",
                                width="medium"
                            ),
                            'Category': st.column_config.TextColumn(
                                'Category',
                                help="Market cap category",
                                width="medium"
                            )
                        }
                    )
                
                with col2:
                    UIComponents.render_metric_card("Active Surges", len(volume_surges))
                    UIComponents.render_metric_card("Extreme (>5x)", len(volume_surges[volume_surges['rvol'] > 5]))
                    UIComponents.render_metric_card("High (>3x)", len(volume_surges[volume_surges['rvol'] > 3]))
                    
                    if 'category' in volume_surges.columns:
                        st.markdown("**📊 Surge by Category:**")
                        surge_categories = volume_surges['category'].value_counts()
                        if len(surge_categories) > 0:
                            for cat, count in surge_categories.head(3).items():
                                st.caption(f"• {cat}: {count} stocks")
            else:
                st.info(f"No volume surges detected with {sensitivity_level} sensitivity (requires RVOL ≥ {min_rvol}x).")
        
        # ================================================================================================
        # 🎯 ADVANCED ANALYSIS TABS - 6 SPECIALIZED INTELLIGENCE MODULES
        # ================================================================================================
        
        st.markdown("---")
        st.markdown("### 🎯 SPECIALIZED MARKET INTELLIGENCE MODULES")
        
        radar_analysis_tabs = st.tabs([
            "🚀 Momentum Surge", 
            "🎯 Pattern Recognition", 
            "🏦 Institutional Flow",
            "⚡ Breakout Scanner", 
            "🎲 High Probability", 
            "🔄 Multi-Timeframe"
        ])
        
        # Tab 1: Momentum Surge Analysis
        with radar_analysis_tabs[0]:
            st.markdown("#### 🚀 Momentum Surge Detection")
            
            try:
                momentum_criteria = {
                    "Conservative": {"score": 70, "accel": 80, "rvol": 2.5},
                    "Balanced": {"score": 60, "accel": 70, "rvol": 2.0},
                    "Aggressive": {"score": 50, "accel": 60, "rvol": 1.5}
                }
                
                current_criteria = momentum_criteria.get("Balanced")
                for key in momentum_criteria:
                    if key in sensitivity_level:
                        current_criteria = momentum_criteria[key]
                        break
                
                momentum_stocks = radar_df[
                    (radar_df.get('momentum_score', 0) >= current_criteria["score"]) &
                    (radar_df.get('acceleration_score', 0) >= current_criteria["accel"]) &
                    (radar_df.get('rvol', 0) >= current_criteria["rvol"])
                ].copy()
                
                if not momentum_stocks.empty:
                    # 🚨 CRITICAL FIX: Proper momentum strength calculation with normalized values
                    momentum_stocks['momentum_strength'] = (
                        momentum_stocks.get('momentum_score', 0) * 0.5 +
                        momentum_stocks.get('acceleration_score', 0) * 0.3 +
                        np.clip(momentum_stocks.get('rvol', 0) * 4, 0, 20)  # Cap RVOL contribution at 20 points
                    )
                    
                    top_momentum = momentum_stocks.nlargest(15, 'momentum_strength')
                    
                    # Display metrics
                    momentum_cols = st.columns(4)
                    with momentum_cols[0]:
                        UIComponents.render_metric_card("Total Momentum", len(momentum_stocks))
                    with momentum_cols[1]:
                        avg_score = top_momentum['momentum_score'].mean() if 'momentum_score' in top_momentum.columns and len(top_momentum) > 0 else 0
                        UIComponents.render_metric_card("Avg Score", f"{avg_score:.1f}")
                    with momentum_cols[2]:
                        avg_rvol = top_momentum['rvol'].mean() if 'rvol' in top_momentum.columns and len(top_momentum) > 0 else 0
                        UIComponents.render_metric_card("Avg RVOL", f"{avg_rvol:.1f}x")
                    with momentum_cols[3]:
                        strength_rating = "🔥🔥🔥" if avg_score > 70 else "🔥🔥" if avg_score > 60 else "🔥"
                        UIComponents.render_metric_card("Strength", strength_rating)
                    
                    # Display dataframe
                    display_cols = ['ticker', 'company_name', 'momentum_score', 'acceleration_score', 'rvol', 'momentum_strength']
                    display_momentum = top_momentum[[col for col in display_cols if col in top_momentum.columns]]
                    
                    st.dataframe(
                        display_momentum,
                        width='stretch',
                        hide_index=True,
                        column_config={
                            'ticker': st.column_config.TextColumn("Ticker", width="small"),
                            'company_name': st.column_config.TextColumn("Company", width="medium"),
                            'momentum_score': st.column_config.ProgressColumn("Momentum", min_value=0, max_value=100, format="%.1f"),
                            'acceleration_score': st.column_config.ProgressColumn("Acceleration", min_value=0, max_value=100, format="%.1f"),
                            'rvol': st.column_config.NumberColumn("RVOL", format="%.1fx"),
                            'momentum_strength': st.column_config.ProgressColumn("Strength", min_value=0, max_value=120, format="%.1f")
                        }
                    )
                else:
                    st.info("No momentum surge stocks found with current criteria")
                    
            except Exception as e:
                logger.error(f"Momentum surge analysis error: {str(e)}")
                st.error("Error in momentum surge analysis")
        
        # Tab 2: Pattern Recognition
        with radar_analysis_tabs[1]:
            st.markdown("#### 🎯 Advanced Pattern Recognition")
            
            try:
                pattern_analysis = radar_df.copy()
                
                # 🚨 CRITICAL FIX: Use sensitivity-based pattern thresholds
                pattern_thresholds = {
                    'breakout': thresholds['pattern'],
                    'reversal': thresholds['pattern'],
                    'trend': thresholds['trend']
                }
                
                st.info(f"🎯 Using {sensitivity_level} pattern thresholds: Breakout/Reversal ≥{pattern_thresholds['breakout']}%, Trend ≥{pattern_thresholds['trend']}%")
                
                # Pattern scoring system
                pattern_signals = []
                
                # Breakout patterns
                if 'breakout_score' in pattern_analysis.columns:
                    breakout_stocks = pattern_analysis[pattern_analysis['breakout_score'] >= pattern_thresholds['breakout']]
                    for _, stock in breakout_stocks.iterrows():
                        reliability_threshold = pattern_thresholds['breakout'] + 15
                        pattern_signals.append({
                            'Ticker': stock['ticker'],
                            'Company': stock.get('company_name', 'Unknown'),
                            'Pattern': '📈 Breakout',
                            'Score': stock['breakout_score'],
                            'Reliability': 'High' if stock['breakout_score'] > reliability_threshold else 'Medium'
                        })
                
                # Reversal patterns
                if 'reversal_score' in pattern_analysis.columns:
                    reversal_stocks = pattern_analysis[pattern_analysis['reversal_score'] >= pattern_thresholds['reversal']]
                    for _, stock in reversal_stocks.iterrows():
                        reliability_threshold = pattern_thresholds['reversal'] + 15
                        pattern_signals.append({
                            'Ticker': stock['ticker'],
                            'Company': stock.get('company_name', 'Unknown'),
                            'Pattern': '🔄 Reversal',
                            'Score': stock['reversal_score'],
                            'Reliability': 'High' if stock['reversal_score'] > reliability_threshold else 'Medium'
                        })
                
                # Continuation patterns
                if 'trend_score' in pattern_analysis.columns:
                    trend_stocks = pattern_analysis[pattern_analysis['trend_score'] >= pattern_thresholds['trend']]
                    for _, stock in trend_stocks.iterrows():
                        reliability_threshold = pattern_thresholds['trend'] + 10
                        pattern_signals.append({
                            'Ticker': stock['ticker'],
                            'Company': stock.get('company_name', 'Unknown'),
                            'Pattern': '➡️ Continuation',
                            'Score': stock['trend_score'],
                            'Reliability': 'High' if stock['trend_score'] > reliability_threshold else 'Medium'
                        })
                
                if pattern_signals:
                    pattern_df = pd.DataFrame(pattern_signals)
                    
                    # Pattern statistics
                    pattern_cols = st.columns(4)
                    with pattern_cols[0]:
                        UIComponents.render_metric_card("Total Patterns", len(pattern_df))
                    with pattern_cols[1]:
                        high_rel = len(pattern_df[pattern_df['Reliability'] == 'High'])
                        UIComponents.render_metric_card("High Reliability", high_rel)
                    with pattern_cols[2]:
                        avg_score = pattern_df['Score'].mean()
                        UIComponents.render_metric_card("Avg Score", f"{avg_score:.1f}")
                    with pattern_cols[3]:
                        top_pattern = pattern_df['Pattern'].mode().iloc[0] if not pattern_df.empty else "None"
                        UIComponents.render_metric_card("Top Pattern", top_pattern)
                    
                    # Display patterns
                    st.dataframe(
                        pattern_df.sort_values('Score', ascending=False),
                        width='stretch',
                        hide_index=True,
                        column_config={
                            'Ticker': st.column_config.TextColumn("Ticker", width="small"),
                            'Company': st.column_config.TextColumn("Company", width="medium"),
                            'Pattern': st.column_config.TextColumn("Pattern", width="medium"),
                            'Score': st.column_config.ProgressColumn("Score", min_value=0, max_value=100, format="%.1f"),
                            'Reliability': st.column_config.TextColumn("Reliability", width="small")
                        }
                    )
                else:
                    st.info("No significant patterns detected")
                    
            except Exception as e:
                logger.error(f"Pattern recognition error: {str(e)}")
                st.error("Error in pattern recognition analysis")
        
        # Tab 3: Institutional Flow Analysis
        with radar_analysis_tabs[2]:
            st.markdown("#### 🏦 Institutional Flow Intelligence")
            
            try:
                institutional_analysis = radar_df.copy()
                
                # 🚨 CRITICAL FIX: Use sensitivity-based flow thresholds
                flow_threshold = thresholds['institutional']
                rvol_threshold = 3.0 if "Conservative" in sensitivity_level else 2.5 if "Balanced" in sensitivity_level else 2.0
                
                st.info(f"🎯 Using {sensitivity_level} flow sensitivity: Volume ≥{rvol_threshold}x, Institutional threshold: {flow_threshold}%")
                
                # Money flow analysis
                flow_signals = []
                
                if 'money_flow_mm' in institutional_analysis.columns:
                    # Use sensitivity-based percentile threshold
                    percentile_threshold = 0.9 if "Conservative" in sensitivity_level else 0.8 if "Balanced" in sensitivity_level else 0.7
                    flow_cutoff = institutional_analysis['money_flow_mm'].quantile(percentile_threshold)
                    
                    large_flows = institutional_analysis[
                        abs(institutional_analysis['money_flow_mm']) >= abs(flow_cutoff)
                    ].copy()
                    
                    for _, stock in large_flows.iterrows():
                        flow_value = stock.get('money_flow_mm', 0)
                        flow_type = "🟢 Inflow" if flow_value > 0 else "🔴 Outflow"
                        extreme_threshold = abs(flow_cutoff) * 1.5
                        
                        flow_signals.append({
                            'Ticker': stock['ticker'],
                            'Company': stock.get('company_name', 'Unknown'),
                            'Flow Type': flow_type,
                            'Amount': f"₹{abs(flow_value):.1f}M",
                            'Intensity': 'Extreme' if abs(flow_value) > extreme_threshold else 'High'
                        })
                
                # Volume-based institutional detection with sensitivity threshold
                if 'rvol' in institutional_analysis.columns:
                    inst_volume = institutional_analysis[institutional_analysis['rvol'] >= rvol_threshold].copy()
                    
                    for _, stock in inst_volume.iterrows():
                        if stock['ticker'] not in [s['Ticker'] for s in flow_signals]:
                            extreme_rvol_threshold = rvol_threshold * 2
                            flow_signals.append({
                                'Ticker': stock['ticker'],
                                'Company': stock.get('company_name', 'Unknown'),
                                'Flow Type': '📊 Volume Inst.',
                                'Amount': f"{stock['rvol']:.1f}x RVOL",
                                'Intensity': 'Extreme' if stock['rvol'] > extreme_rvol_threshold else 'High'
                            })
                
                if flow_signals:
                    flow_df = pd.DataFrame(flow_signals)
                    
                    # Flow statistics
                    flow_cols = st.columns(4)
                    with flow_cols[0]:
                        UIComponents.render_metric_card("Total Signals", len(flow_df))
                    with flow_cols[1]:
                        inflows = len(flow_df[flow_df['Flow Type'].str.contains('Inflow')])
                        UIComponents.render_metric_card("Inflows", inflows)
                    with flow_cols[2]:
                        outflows = len(flow_df[flow_df['Flow Type'].str.contains('Outflow')])
                        UIComponents.render_metric_card("Outflows", outflows)
                    with flow_cols[3]:
                        extreme_flows = len(flow_df[flow_df['Intensity'] == 'Extreme'])
                        UIComponents.render_metric_card("Extreme", extreme_flows)
                    
                    # Flow direction indicator
                    if inflows > outflows + 2:
                        st.success("🟢 **Net Institutional Buying** - Bullish sentiment")
                    elif outflows > inflows + 2:
                        st.error("🔴 **Net Institutional Selling** - Bearish sentiment")
                    else:
                        st.info("⚖️ **Balanced Flow** - Mixed sentiment")
                    
                    # Display flows
                    st.dataframe(
                        flow_df,
                        width='stretch',
                        hide_index=True,
                        column_config={
                            'Ticker': st.column_config.TextColumn("Ticker", width="small"),
                            'Company': st.column_config.TextColumn("Company", width="medium"),
                            'Flow Type': st.column_config.TextColumn("Flow Type", width="medium"),
                            'Amount': st.column_config.TextColumn("Amount", width="small"),
                            'Intensity': st.column_config.TextColumn("Intensity", width="small")
                        }
                    )
                else:
                    st.info("No significant institutional flow detected")
                    
            except Exception as e:
                logger.error(f"Institutional flow analysis error: {str(e)}")
                st.error("Error in institutional flow analysis")
        
        # Tab 4: Breakout Scanner
        with radar_analysis_tabs[3]:
            st.markdown("#### ⚡ Advanced Breakout Scanner")
            
            try:
                breakout_analysis = radar_df.copy()
                
                # 🚨 CRITICAL FIX: Use sensitivity-based breakout thresholds
                breakout_threshold = thresholds['breakout']
                price_proximity = 0.98 if "Conservative" in sensitivity_level else 0.96 if "Balanced" in sensitivity_level else 0.94
                volume_threshold = 2.5 if "Conservative" in sensitivity_level else 2.0 if "Balanced" in sensitivity_level else 1.5
                
                st.info(f"🎯 Using {sensitivity_level} breakout criteria: Price proximity ≥{price_proximity*100:.0f}%, Volume ≥{volume_threshold}x")
                
                # Multi-timeframe breakout detection
                breakout_candidates = []
                
                # Price-based breakouts with sensitivity adjustment
                if all(col in breakout_analysis.columns for col in ['price', 'high_20d', 'high_50d']):
                    price_breakouts = breakout_analysis[
                        (breakout_analysis['price'] >= breakout_analysis['high_20d'] * price_proximity) |
                        (breakout_analysis['price'] >= breakout_analysis['high_50d'] * (price_proximity - 0.02))
                    ].copy()
                    
                    for _, stock in price_breakouts.iterrows():
                        # Calculate precise distance metrics
                        high_20_distance = (stock['price'] / stock['high_20d'] - 1) * 100 if 'high_20d' in stock and pd.notna(stock['high_20d']) else -999
                        high_50_distance = (stock['price'] / stock['high_50d'] - 1) * 100 if 'high_50d' in stock and pd.notna(stock['high_50d']) else -999
                        
                        # Determine breakout type
                        if high_20_distance >= (price_proximity - 1) * 100:
                            breakout_type = "🚀 20D High"
                            distance = high_20_distance
                        elif high_50_distance >= (price_proximity - 1.02) * 100:
                            breakout_type = "📈 50D High"
                            distance = high_50_distance
                        else:
                            continue
                        
                        # Apply breakout threshold if available
                        if 'breakout_score' in stock and pd.notna(stock['breakout_score']):
                            if stock['breakout_score'] < breakout_threshold:
                                continue
                        
                        strength = 'Strong' if distance > 2 else 'Emerging'
                        
                        breakout_candidates.append({
                            'Ticker': stock['ticker'],
                            'Company': stock.get('company_name', 'Unknown'),
                            'Breakout Type': breakout_type,
                            'Distance': f"{distance:+.1f}%",
                            'Volume': f"{stock.get('rvol', 0):.1f}x" if 'rvol' in stock else "N/A",
                            'Strength': strength
                        })
                
                # Volume breakouts with sensitivity threshold
                if 'rvol' in breakout_analysis.columns:
                    volume_breakouts = breakout_analysis[breakout_analysis['rvol'] >= volume_threshold].copy()
                    
                    for _, stock in volume_breakouts.iterrows():
                        if stock['ticker'] not in [c['Ticker'] for c in breakout_candidates]:
                            vol_extreme_threshold = volume_threshold * 2
                            vol_strength = "🔥 Extreme" if stock['rvol'] > vol_extreme_threshold else "📊 High"
                            strength = 'Strong' if stock['rvol'] > vol_extreme_threshold else 'Medium'
                            
                            breakout_candidates.append({
                                'Ticker': stock['ticker'],
                                'Company': stock.get('company_name', 'Unknown'),
                                'Breakout Type': f'{vol_strength} Volume',
                                'Distance': f"{stock['rvol']:.1f}x",
                                'Volume': f"{stock['rvol']:.1f}x",
                                'Strength': strength
                            })
                
                if breakout_candidates:
                    breakout_df = pd.DataFrame(breakout_candidates)
                    
                    # Breakout statistics
                    breakout_cols = st.columns(4)
                    with breakout_cols[0]:
                        UIComponents.render_metric_card("Total Breakouts", len(breakout_df))
                    with breakout_cols[1]:
                        strong_breakouts = len(breakout_df[breakout_df['Strength'] == 'Strong'])
                        UIComponents.render_metric_card("Strong", strong_breakouts)
                    with breakout_cols[2]:
                        price_breakouts_count = len(breakout_df[breakout_df['Breakout Type'].str.contains('High')])
                        UIComponents.render_metric_card("Price Breakouts", price_breakouts_count)
                    with breakout_cols[3]:
                        volume_breakouts_count = len(breakout_df[breakout_df['Breakout Type'].str.contains('Volume')])
                        UIComponents.render_metric_card("Volume Breakouts", volume_breakouts_count)
                    
                    # Display breakouts
                    st.dataframe(
                        breakout_df,
                        width='stretch',
                        hide_index=True,
                        column_config={
                            'Ticker': st.column_config.TextColumn("Ticker", width="small"),
                            'Company': st.column_config.TextColumn("Company", width="medium"),
                            'Breakout Type': st.column_config.TextColumn("Type", width="medium"),
                            'Distance': st.column_config.TextColumn("Distance", width="small"),
                            'Volume': st.column_config.TextColumn("Volume", width="small"),
                            'Strength': st.column_config.TextColumn("Strength", width="small")
                        }
                    )
                else:
                    st.info("No breakout candidates detected")
                    
            except Exception as e:
                logger.error(f"Breakout scanner error: {str(e)}")
                st.error("Error in breakout scanner")
        
        # Tab 5: High Probability Signals
        with radar_analysis_tabs[4]:
            st.markdown("#### 🎲 High Probability Signal Generator")
            
            try:
                # Multi-factor scoring for high probability signals
                high_prob_analysis = radar_df.copy()
                
                # 🚨 CRITICAL FIX: Use sensitivity-based probability thresholds
                prob_threshold = thresholds['probability']
                st.info(f"🎯 Using {sensitivity_level} probability threshold: ≥{prob_threshold}%")
                
                # Calculate composite probability score
                prob_factors = []
                
                # Technical score (if available)
                if 'master_score' in high_prob_analysis.columns:
                    prob_factors.append(('Technical', 'master_score', 0.3))
                
                # Momentum factor
                if 'momentum_score' in high_prob_analysis.columns:
                    prob_factors.append(('Momentum', 'momentum_score', 0.25))
                
                # Volume factor with sensitivity adjustment - 🚨 FIXED: Proper volume scoring
                if 'rvol' in high_prob_analysis.columns:
                    volume_multiplier = 8 if "Conservative" in sensitivity_level else 10 if "Balanced" in sensitivity_level else 12
                    high_prob_analysis['volume_score'] = np.clip(high_prob_analysis['rvol'] * volume_multiplier, 0, 100)
                    prob_factors.append(('Volume', 'volume_score', 0.25))
                
                # Trend factor
                if 'trend_score' in high_prob_analysis.columns:
                    prob_factors.append(('Trend', 'trend_score', 0.2))
                
                # Calculate probability score
                if prob_factors:
                    high_prob_analysis['probability_score'] = 0
                    for name, col, weight in prob_factors:
                        if col in high_prob_analysis.columns:
                            high_prob_analysis['probability_score'] += high_prob_analysis[col] * weight
                    
                    # Filter high probability stocks using sensitivity threshold
                    high_prob_stocks = high_prob_analysis[
                        high_prob_analysis['probability_score'] >= prob_threshold
                    ].copy()
                    
                    if not high_prob_stocks.empty:
                        # Add confidence intervals with sensitivity-aware thresholds
                        very_high_threshold = prob_threshold + 15
                        high_threshold = prob_threshold + 10
                        good_threshold = prob_threshold + 5
                        
                        high_prob_stocks['confidence'] = high_prob_stocks['probability_score'].apply(
                            lambda x: 'Very High' if x > very_high_threshold 
                            else 'High' if x > high_threshold 
                            else 'Good' if x > good_threshold 
                            else 'Moderate'
                        )
                        
                        top_prob = high_prob_stocks.nlargest(20, 'probability_score')
                        
                        # Probability statistics
                        prob_cols = st.columns(4)
                        with prob_cols[0]:
                            UIComponents.render_metric_card("High Prob Signals", len(high_prob_stocks))
                        with prob_cols[1]:
                            very_high = len(high_prob_stocks[high_prob_stocks['confidence'] == 'Very High'])
                            UIComponents.render_metric_card("Very High", very_high)
                        with prob_cols[2]:
                            avg_prob = top_prob['probability_score'].mean()
                            UIComponents.render_metric_card("Avg Probability", f"{avg_prob:.1f}%")
                        with prob_cols[3]:
                            success_rate = min(avg_prob * 1.1, 95)  # Estimated success rate
                            UIComponents.render_metric_card("Est. Success", f"{success_rate:.0f}%")
                        
                        # Display high probability signals
                        display_cols = ['ticker', 'company_name', 'probability_score', 'confidence']
                        if 'master_score' in top_prob.columns:
                            display_cols.append('master_score')
                        if 'rvol' in top_prob.columns:
                            display_cols.append('rvol')
                        
                        prob_display = top_prob[[col for col in display_cols if col in top_prob.columns]]
                        
                        st.dataframe(
                            prob_display,
                            width='stretch',
                            hide_index=True,
                            column_config={
                                'ticker': st.column_config.TextColumn("Ticker", width="small"),
                                'company_name': st.column_config.TextColumn("Company", width="medium"),
                                'probability_score': st.column_config.ProgressColumn("Probability", min_value=0, max_value=100, format="%.1f"),
                                'confidence': st.column_config.TextColumn("Confidence", width="small"),
                                'master_score': st.column_config.ProgressColumn("Technical", min_value=0, max_value=100, format="%.1f"),
                                'rvol': st.column_config.NumberColumn("RVOL", format="%.1fx")
                            }
                        )
                        
                        # Risk warning
                        st.warning("⚠️ **Risk Warning**: High probability signals are statistical indicators. Always use proper risk management and position sizing.")
                        
                    else:
                        st.info(f"No high probability signals found with {sensitivity_level} criteria (≥{prob_threshold}%)")
                else:
                    st.info("Insufficient data for probability analysis")
                    
            except Exception as e:
                logger.error(f"High probability analysis error: {str(e)}")
                st.error("Error in high probability analysis")
        
        # Tab 6: Multi-Timeframe Analysis
        with radar_analysis_tabs[5]:
            st.markdown("#### 🔄 Multi-Timeframe Confluence Analysis")
            
            try:
                # 🚨 CRITICAL FIX: Use sensitivity-based confluence threshold
                confluence_threshold = thresholds['confluence']
                st.info(f"🎯 Using {sensitivity_level} confluence threshold: {confluence_threshold}%")
                
                # Enhanced confluence calculation with better data handling
                confluence_signals = []
                
                for _, stock in radar_df.iterrows():
                    short_score = 0
                    medium_score = 0
                    total_factors = 0
                    
                    # Short-term factors (with fallbacks)
                    if 'ret_1d' in stock and pd.notna(stock.get('ret_1d')):
                        short_score += 30 if stock['ret_1d'] > 3 else 20 if stock['ret_1d'] > 1 else 10 if stock['ret_1d'] > 0 else 0
                        total_factors += 1
                        
                    if 'ret_3d' in stock and pd.notna(stock.get('ret_3d')):
                        short_score += 35 if stock['ret_3d'] > 7 else 25 if stock['ret_3d'] > 3 else 15 if stock['ret_3d'] > 0 else 0
                        total_factors += 1
                        
                    if 'momentum_score' in stock and pd.notna(stock.get('momentum_score')):
                        short_score += stock['momentum_score'] * 0.6
                        total_factors += 1
                        
                    # Medium-term factors (with fallbacks)
                    if 'ret_7d' in stock and pd.notna(stock.get('ret_7d')):
                        medium_score += 40 if stock['ret_7d'] > 15 else 30 if stock['ret_7d'] > 7 else 20 if stock['ret_7d'] > 0 else 0
                        total_factors += 1
                        
                    if 'ret_30d' in stock and pd.notna(stock.get('ret_30d')):
                        medium_score += 45 if stock['ret_30d'] > 30 else 35 if stock['ret_30d'] > 15 else 25 if stock['ret_30d'] > 0 else 0
                        total_factors += 1
                        
                    if 'trend_score' in stock and pd.notna(stock.get('trend_score')):
                        medium_score += stock['trend_score'] * 0.5
                        total_factors += 1
                    
                    # Additional scoring factors for better results
                    if 'acceleration_score' in stock and pd.notna(stock.get('acceleration_score')):
                        short_score += stock['acceleration_score'] * 0.3
                        total_factors += 1
                        
                    if 'rvol' in stock and pd.notna(stock.get('rvol')):
                        volume_boost = min(stock['rvol'] * 8, 30)  # 🚨 FIXED: Reduced multiplier and cap
                        short_score += volume_boost
                        total_factors += 1
                    
                    # Only include stocks with minimum data availability
                    if total_factors >= 3:
                        # Calculate normalized scores
                        short_term_final = short_score / max(1, (total_factors * 0.6))
                        medium_term_final = medium_score / max(1, (total_factors * 0.4))
                        
                        # Calculate confluence with timeframe weighting
                        confluence_score = (short_term_final * 0.6 + medium_term_final * 0.4)
                        
                        # Alignment calculation
                        alignment_diff = abs(short_term_final - medium_term_final)
                        alignment = 'Strong' if alignment_diff < 25 else 'Medium' if alignment_diff < 40 else 'Weak'
                        
                        # Apply sensitivity-based threshold
                        if confluence_score >= confluence_threshold:
                            confluence_signals.append({
                                'ticker': stock['ticker'],
                                'company_name': stock.get('company_name', 'Unknown'),
                                'short_term_score': round(short_term_final, 1),
                                'medium_term_score': round(medium_term_final, 1),
                                'confluence_score': round(confluence_score, 1),
                                'alignment': alignment,
                                'data_factors': total_factors
                            })
                
                if confluence_signals:
                    confluence_df = pd.DataFrame(confluence_signals)
                    confluence_df = confluence_df.sort_values('confluence_score', ascending=False)
                    
                    # Enhanced statistics
                    mtf_cols = st.columns(4)
                    with mtf_cols[0]:
                        UIComponents.render_metric_card("Confluence Signals", len(confluence_df))
                    with mtf_cols[1]:
                        strong_alignment = len(confluence_df[confluence_df['alignment'] == 'Strong'])
                        UIComponents.render_metric_card("Strong Alignment", strong_alignment)
                    with mtf_cols[2]:
                        avg_confluence = confluence_df['confluence_score'].mean()
                        UIComponents.render_metric_card("Avg Confluence", f"{avg_confluence:.1f}%")
                    with mtf_cols[3]:
                        max_confluence = confluence_df['confluence_score'].max()
                        UIComponents.render_metric_card("Max Confluence", f"{max_confluence:.1f}%")
                    
                    # Data quality indicator
                    avg_factors = confluence_df['data_factors'].mean()
                    if avg_factors >= 6:
                        data_quality = "🟢 Excellent"
                    elif avg_factors >= 4:
                        data_quality = "🟡 Good"
                    else:
                        data_quality = "🟠 Limited"
                    
                    st.info(f"📊 Data Quality: {data_quality} (Avg {avg_factors:.1f} factors per stock)")
                    
                    # Enhanced dataframe display
                    display_df = confluence_df.drop('data_factors', axis=1)  # Hide technical column
                    
                    st.dataframe(
                        display_df,
                        width='stretch',
                        hide_index=True,
                        column_config={
                            'ticker': st.column_config.TextColumn("Ticker", width="small"),
                            'company_name': st.column_config.TextColumn("Company", width="medium"),
                            'short_term_score': st.column_config.ProgressColumn("Short-term", min_value=0, max_value=100, format="%.1f"),
                            'medium_term_score': st.column_config.ProgressColumn("Medium-term", min_value=0, max_value=100, format="%.1f"),
                            'confluence_score': st.column_config.ProgressColumn("Confluence", min_value=0, max_value=100, format="%.1f"),
                            'alignment': st.column_config.TextColumn("Alignment", width="small")
                        }
                    )
                    
                    # Enhanced analysis feedback
                    strong_signals = len(confluence_df[confluence_df['alignment'] == 'Strong'])
                    total_signals = len(confluence_df)
                    
                    if strong_signals > total_signals * 0.7:
                        st.success("🎯 **Excellent Multi-Timeframe Alignment** - Very high conviction signals!")
                    elif strong_signals > total_signals * 0.5:
                        st.success("✅ **Good Multi-Timeframe Alignment** - High conviction signals")
                    elif strong_signals > total_signals * 0.3:
                        st.info("⚖️ **Mixed Timeframe Signals** - Moderate conviction, selective approach recommended")
                    else:
                        st.warning("⚠️ **Weak Timeframe Alignment** - Lower conviction, careful position sizing advised")
                    
                    # Sensitivity adjustment recommendations
                    if len(confluence_df) < 5:
                        st.info("💡 **Tip**: Try 'Aggressive' sensitivity for more signals, or 'Conservative' for higher quality")
                
                else:
                    # Enhanced "no signals" feedback with actionable advice
                    total_analyzed = len(radar_df)
                    st.warning(f"🔍 **No confluence signals detected** (Analyzed {total_analyzed} stocks with {confluence_threshold}% threshold)")
                    
                    # Provide actionable suggestions
                    st.markdown("**🛠 Troubleshooting Suggestions:**")
                    if "Conservative" in sensitivity_level:
                        st.info("• Try **'Balanced'** or **'Aggressive'** sensitivity for more signals")
                    if radar_mode != "📊 Full Spectrum":
                        st.info(f"• Current mode: **{radar_mode}** - Try **'📊 Full Spectrum'** for broader analysis")
                    if total_analyzed < 50:
                        st.info("• Current filters may be too restrictive - try adjusting Risk Profile or Market Regime")
                    
                    st.info("• Confluence analysis requires multiple timeframe data - some datasets may have limited historical returns")
                    
            except Exception as e:
                logger.error(f"Multi-timeframe analysis error: {str(e)}")
                st.error(f"Error in multi-timeframe analysis: {str(e)}")
                st.info("💡 This might be due to missing return columns (ret_1d, ret_3d, ret_7d, ret_30d) in your dataset")
        
        if filtered_df.empty:
            st.warning("No data available for Ultimate Market Radar analysis")

    # Tab 3: 🏆 ALL TIME BEST ANALYSIS TAB - INSTITUTIONAL GRADE INTELLIGENCE
    with tabs[3]:
        
        if not filtered_df.empty:
            # ================================================================================================
            # 📊 MARKET OVERVIEW - BROAD MARKET INSIGHTS
            # ================================================================================================
            
            st.markdown("### 🌍 **MARKET OVERVIEW**")
            st.markdown("*Get broad market insights with gainers, losers, and sector performance*")
            
            # Render Market Overview using SimpleMarketUI
            try:
                SimpleMarketUI.render_market_analysis(filtered_df)
            except Exception as e:
                logger.error(f"Market Overview rendering failed: {e}", exc_info=True)
                st.error("⚠️ Unable to render Market Overview. Please try again.")
            
            st.markdown("---")
            
            # ================================================================================================
            # 🎯 EXECUTIVE DASHBOARD - TOP-LEVEL MARKET INTELLIGENCE
            # ================================================================================================
            
            st.markdown("### 📊 **EXECUTIVE MARKET DASHBOARD**")
            
            # Calculate comprehensive market intelligence metrics
            total_analyzed = len(filtered_df)
            elite_threshold = 80
            strong_threshold = 70
            
            elite_stocks = len(filtered_df[filtered_df['master_score'] >= elite_threshold]) if 'master_score' in filtered_df.columns else 0
            strong_stocks = len(filtered_df[filtered_df['master_score'] >= strong_threshold]) if 'master_score' in filtered_df.columns else 0
            
            # PROFESSIONAL UNIQUE METRICS (NOT IN SUMMARY TAB)
            
            # Executive metrics row
            exec_cols = st.columns(5)
            
            with exec_cols[0]:
                # 🔄 SECTOR LEADERSHIP INTELLIGENCE
                if 'sector' in filtered_df.columns and 'ret_1d' in filtered_df.columns:
                    sector_performance = filtered_df.groupby('sector')['ret_1d'].mean().fillna(0)
                    if not sector_performance.empty:
                        top_sector = sector_performance.idxmax()
                        top_sector_return = sector_performance.max()
                        sector_status = "🔥 HOT" if top_sector_return > 2 else "📈 LEADING" if top_sector_return > 0.5 else "⚖️ MIXED"
                        UIComponents.render_metric_card("🔄 Sector Leader", f"{sector_status}", f"{top_sector} (+{top_sector_return:.1f}%)")
                    else:
                        UIComponents.render_metric_card("🔄 Sector Leader", "📊 ANALYZING", "Data processing...")
                else:
                    UIComponents.render_metric_card("🔄 Sector Leader", "⚠️ NO DATA", "Sector data unavailable")
            
            with exec_cols[1]:
                # 💰 EARNINGS MOMENTUM INTELLIGENCE
                if 'eps_change_pct' in filtered_df.columns:
                    valid_eps = filtered_df['eps_change_pct'].dropna()
                    if len(valid_eps) > 0:
                        avg_eps_growth = valid_eps.mean()
                        positive_eps = len(valid_eps[valid_eps > 0])
                        total_eps = len(valid_eps)
                        eps_win_rate = safe_percentage(positive_eps, total_eps, default=0.0)
                        
                        if avg_eps_growth > 15:
                            eps_status = "🚀 EXPLOSIVE"
                        elif avg_eps_growth > 5:
                            eps_status = "📈 STRONG"
                        elif avg_eps_growth > 0:
                            eps_status = "💚 POSITIVE"
                        else:
                            eps_status = "📉 DECLINING"
                        
                        UIComponents.render_metric_card("💰 Earnings Momentum", f"{eps_status}", f"{avg_eps_growth:+.1f}% avg • {eps_win_rate:.0f}% positive")
                    else:
                        UIComponents.render_metric_card("💰 Earnings Momentum", "📊 LIMITED", "Insufficient EPS data")
                else:
                    UIComponents.render_metric_card("💰 Earnings Momentum", "⚠️ NO DATA", "EPS data unavailable")
            
            with exec_cols[2]:
                # 🎯 LONG-TERM STRENGTH INTELLIGENCE
                if 'long_term_strength' in filtered_df.columns:
                    valid_lts = filtered_df['long_term_strength'].dropna()
                    if len(valid_lts) > 0:
                        avg_lts = valid_lts.mean()
                        strong_lts_count = len(valid_lts[valid_lts >= 70])
                        lts_quality = safe_percentage(strong_lts_count, len(valid_lts), default=0.0)
                        
                        if avg_lts >= 75:
                            lts_status = "💎 ELITE"
                        elif avg_lts >= 65:
                            lts_status = "🏆 STRONG"
                        elif avg_lts >= 55:
                            lts_status = "📈 GOOD"
                        else:
                            lts_status = "⚖️ AVERAGE"
                        
                        UIComponents.render_metric_card("🎯 Long-Term Strength", f"{lts_status}", f"{avg_lts:.1f} avg • {lts_quality:.0f}% strong")
                    else:
                        UIComponents.render_metric_card("🎯 Long-Term Strength", "📊 CALCULATING", "Processing trends...")
                else:
                    UIComponents.render_metric_card("🎯 Long-Term Strength", "⚠️ NO DATA", "LTS data unavailable")
            
            with exec_cols[3]:
                # ⏱️ MULTI-TIMEFRAME ALIGNMENT
                if 'momentum_harmony' in filtered_df.columns:
                    valid_harmony = filtered_df['momentum_harmony'].dropna()
                    if len(valid_harmony) > 0:
                        avg_harmony = valid_harmony.mean()
                        aligned_count = len(valid_harmony[valid_harmony >= 3])
                        alignment_rate = safe_percentage(aligned_count, len(valid_harmony), default=0.0)
                        
                        if avg_harmony >= 3.5:
                            harmony_status = "🟢 PERFECT"
                        elif avg_harmony >= 2.8:
                            harmony_status = "🟢 ALIGNED"
                        elif avg_harmony >= 2.0:
                            harmony_status = "🟡 MIXED"
                        else:
                            harmony_status = "🔴 DIVERGING"
                        
                        UIComponents.render_metric_card("⏱️ Multi-Timeframe", f"{harmony_status}", f"{avg_harmony:.1f}/4 • {alignment_rate:.0f}% aligned")
                    else:
                        UIComponents.render_metric_card("⏱️ Multi-Timeframe", "📊 ANALYZING", "Timeframe analysis...")
                else:
                    UIComponents.render_metric_card("⏱️ Multi-Timeframe", "⚠️ NO DATA", "Harmony data unavailable")
            
            with exec_cols[4]:
                # 📊 VALUATION HEALTH INTELLIGENCE
                if 'pe' in filtered_df.columns:
                    valid_pe = filtered_df['pe'].dropna()
                    # Filter out extreme PE values for realistic assessment
                    reasonable_pe = valid_pe[(valid_pe > 0) & (valid_pe < 100)]
                    
                    if len(reasonable_pe) > 0:
                        avg_pe = reasonable_pe.mean()
                        reasonable_count = len(reasonable_pe[(reasonable_pe >= 10) & (reasonable_pe <= 25)])
                        valuation_health = safe_percentage(reasonable_count, len(reasonable_pe), default=0.0)
                        
                        if avg_pe <= 15:
                            pe_status = "💰 CHEAP"
                        elif avg_pe <= 25:
                            pe_status = "💵 FAIR"
                        elif avg_pe <= 40:
                            pe_status = "💸 EXPENSIVE"
                        else:
                            pe_status = "🚨 OVERVALUED"
                        
                        UIComponents.render_metric_card("📊 Valuation Health", f"{pe_status}", f"PE: {avg_pe:.1f}x • {valuation_health:.0f}% reasonable")
                    else:
                        UIComponents.render_metric_card("📊 Valuation Health", "📊 LIMITED", "Insufficient PE data")
                else:
                    UIComponents.render_metric_card("📊 Valuation Health", "⚠️ NO DATA", "PE data unavailable")
            
            # ================================================================================================
            # 📈 ADVANCED VISUALIZATION SUITE - PROFESSIONAL CHARTS
            # ================================================================================================
            
            st.markdown("---")
            st.markdown("### 📊 **ADVANCED MARKET VISUALIZATION SUITE**")
            
            viz_tabs = st.tabs([
                "🎯 Score Analytics", 
                "📊 Performance Matrix", 
                "🔥 Momentum Heatmap",
                "💎 Pattern Intelligence",
                "🏢 Sector Analysis",
                "🏭 Industry Analysis",
                "⚡ Risk Dashboard"
            ])
            
            # Tab 1: Score Analytics
            with viz_tabs[0]:
                score_cols = st.columns(2)
                
                with score_cols[0]:
                    st.markdown("#### 📊 **Master Score Distribution**")
                    fig_dist = Visualizer.create_score_distribution(filtered_df)
                    st.plotly_chart(fig_dist, width="stretch", theme="streamlit")
                    
                    # Score quality analysis
                    if 'master_score' in filtered_df.columns:
                        score_stats = {
                            "Elite (80-100)": len(filtered_df[filtered_df['master_score'] >= 80]),
                            "Strong (70-79)": len(filtered_df[(filtered_df['master_score'] >= 70) & (filtered_df['master_score'] < 80)]),
                            "Good (60-69)": len(filtered_df[(filtered_df['master_score'] >= 60) & (filtered_df['master_score'] < 70)]),
                            "Average (50-59)": len(filtered_df[(filtered_df['master_score'] >= 50) & (filtered_df['master_score'] < 60)]),
                            "Below Avg (<50)": len(filtered_df[filtered_df['master_score'] < 50])
                        }
                        
                        st.markdown("**🎯 Score Quality Distribution:**")
                        for category, count in score_stats.items():
                            pct = (count / total_analyzed * 100) if total_analyzed > 0 else 0
                            st.write(f"• {category}: **{count}** stocks ({pct:.1f}%)")
                
                with score_cols[1]:
                    st.markdown("#### 🔥 **Component Score Analysis**")
                    
                    # Check if we have at least some score columns
                    available_score_cols = [col for col in ['momentum_score', 'acceleration_score', 'breakout_score', 'position_score', 'volume_score', 'rvol_score'] if col in filtered_df.columns]
                    
                    if len(available_score_cols) >= 2:  # Need at least 2 score columns
                        # Build component data dynamically based on available columns
                        components = []
                        scores = []
                        
                        if 'momentum_score' in filtered_df.columns:
                            components.append('Momentum')
                            scores.append(filtered_df['momentum_score'].mean())
                        
                        if 'acceleration_score' in filtered_df.columns:
                            components.append('Acceleration')
                            scores.append(filtered_df['acceleration_score'].mean())
                        
                        if 'breakout_score' in filtered_df.columns:
                            components.append('Breakout')
                            scores.append(filtered_df['breakout_score'].mean())
                        
                        if 'position_score' in filtered_df.columns:
                            components.append('Position')
                            scores.append(filtered_df['position_score'].mean())
                        
                        if 'volume_score' in filtered_df.columns:
                            components.append('Volume')
                            scores.append(filtered_df['volume_score'].mean())
                        
                        if 'rvol_score' in filtered_df.columns:
                            components.append('RVOL')
                            scores.append(filtered_df['rvol_score'].mean())
                        elif 'rvol' in filtered_df.columns:
                            components.append('RVOL')
                            scores.append(min(filtered_df['rvol'].mean() * 20, 100))  # Normalized to 0-100, capped at 100
                        
                        # Create component dataframe
                        component_data = {
                            'Component': components,
                            'Avg Score': scores,
                            'Quality': ['🔥 Strong' if score >= 65 else '📈 Good' if score >= 55 else '⚖️ Average' if score >= 40 else '📉 Weak' for score in scores]
                        }
                        
                        component_df = pd.DataFrame(component_data)
                        
                        st.dataframe(
                            component_df,
                            width='stretch',
                            hide_index=True,
                            column_config={
                                'Component': st.column_config.TextColumn("Component", width="medium"),
                                'Avg Score': st.column_config.ProgressColumn("Avg Score", min_value=0, max_value=100, format="%.1f"),
                                'Quality': st.column_config.TextColumn("Quality", width="small")
                            }
                        )
                        
                        # Component insights
                        if not component_df.empty:
                            best_component = component_df.loc[component_df['Avg Score'].idxmax(), 'Component']
                            best_score = component_df['Avg Score'].max()
                            st.success(f"🏆 **Strongest Component**: {best_component} ({best_score:.1f} avg score)")
                            
                            # Show component count
                            st.info(f"📊 Analyzing {len(components)} score components from your dataset")
                    else:
                        st.warning("⚠️ **Insufficient Data**: Need at least 2 score components for analysis")
                        st.info("💡 **Available columns**: " + ", ".join(available_score_cols) if available_score_cols else "No score columns found")
            
            # Tab 2: Performance Matrix
            with viz_tabs[1]:
                perf_cols = st.columns(2)
                
                with perf_cols[0]:
                    st.markdown("#### 💰 **Return Performance Analysis**")
                    
                    if any(col in filtered_df.columns for col in ['ret_1d', 'ret_3d', 'ret_7d', 'ret_30d']):
                        return_analysis = {}
                        
                        for period, col in [('1D', 'ret_1d'), ('3D', 'ret_3d'), ('7D', 'ret_7d'), ('30D', 'ret_30d')]:
                            if col in filtered_df.columns:
                                positive_returns = len(filtered_df[filtered_df[col] > 0])
                                avg_return = filtered_df[col].mean()
                                max_return = filtered_df[col].max()
                                
                                return_analysis[period] = {
                                    'Positive %': (positive_returns / len(filtered_df) * 100),
                                    'Avg Return': avg_return,
                                    'Max Return': max_return,
                                    'Quality': '🔥' if avg_return > 5 else '📈' if avg_return > 2 else '⚖️' if avg_return > 0 else '📉'
                                }
                        
                        if return_analysis:
                            return_df = pd.DataFrame(return_analysis).T
                            
                            st.dataframe(
                                return_df,
                                width='stretch',
                                column_config={
                                    'Positive %': st.column_config.ProgressColumn("Win Rate", min_value=0, max_value=100, format="%.1f"),
                                    'Avg Return': st.column_config.NumberColumn("Avg Return", format="%.2f%%"),
                                    'Max Return': st.column_config.NumberColumn("Max Return", format="%.2f%%"),
                                    'Quality': st.column_config.TextColumn("Quality", width="small")
                                }
                            )
                            
                            # Performance insights
                            best_period = return_df['Avg Return'].idxmax()
                            best_avg = return_df.loc[best_period, 'Avg Return']
                            st.success(f"🎯 **Best Timeframe**: {best_period} with {best_avg:.2f}% avg return")
                    else:
                        st.info("📊 Return data not available in current dataset")
                
                with perf_cols[1]:
                    st.markdown("#### 📊 **Volume & Liquidity Matrix**")
                    
                    if 'rvol' in filtered_df.columns:
                        volume_ranges = {
                            'Extreme (>5x)': len(filtered_df[filtered_df['rvol'] > 5]),
                            'Very High (3-5x)': len(filtered_df[(filtered_df['rvol'] >= 3) & (filtered_df['rvol'] <= 5)]),
                            'High (2-3x)': len(filtered_df[(filtered_df['rvol'] >= 2) & (filtered_df['rvol'] < 3)]),
                            'Above Avg (1.5-2x)': len(filtered_df[(filtered_df['rvol'] >= 1.5) & (filtered_df['rvol'] < 2)]),
                            'Normal (<1.5x)': len(filtered_df[filtered_df['rvol'] < 1.5])
                        }
                        
                        volume_df = pd.DataFrame([
                            {'Range': k, 'Count': v, 'Percentage': (v/len(filtered_df)*100)}
                            for k, v in volume_ranges.items()
                        ])
                        
                        st.dataframe(
                            volume_df,
                            width='stretch',
                            hide_index=True,
                            column_config={
                                'Range': st.column_config.TextColumn("Volume Range", width="medium"),
                                'Count': st.column_config.NumberColumn("Stocks", width="small"),
                                'Percentage': st.column_config.ProgressColumn("% of Total", min_value=0, max_value=100, format="%.1f")
                            }
                        )
                        
                        # Volume insights
                        high_volume = volume_ranges['Extreme (>5x)'] + volume_ranges['Very High (3-5x)'] + volume_ranges['High (2-3x)']
                        high_vol_pct = (high_volume / len(filtered_df) * 100)
                        
                        if high_vol_pct > 30:
                            st.success(f"🔥 **High Activity Market**: {high_vol_pct:.1f}% stocks showing elevated volume")
                        elif high_vol_pct > 15:
                            st.info(f"📊 **Moderate Activity**: {high_vol_pct:.1f}% stocks with high volume")
                        else:
                            st.warning(f"📉 **Low Activity**: Only {high_vol_pct:.1f}% stocks with high volume")
            
            # Tab 3: Momentum Heatmap
            with viz_tabs[2]:
                st.markdown("#### 🔥 **Multi-Dimensional Momentum Analysis**")
                
                # Check for momentum-related columns
                momentum_cols = [col for col in ['momentum_score', 'acceleration_score'] if col in filtered_df.columns]
                
                if len(momentum_cols) >= 1:
                    momentum_viz_cols = st.columns(2)
                    
                    with momentum_viz_cols[0]:
                        if len(momentum_cols) == 2:  # Both momentum and acceleration available
                            st.markdown("**🎯 Momentum vs Acceleration Matrix**")
                            
                            fig_momentum = go.Figure()
                            
                            # Add scatter plot with color coding by master score
                            fig_momentum.add_trace(
                                go.Scatter(
                                    x=filtered_df['momentum_score'],
                                    y=filtered_df['acceleration_score'],
                                    mode='markers',
                                    marker=dict(
                                        size=8,
                                        color=filtered_df['master_score'] if 'master_score' in filtered_df.columns else 'blue',
                                        colorscale='RdYlGn',
                                        showscale=True,
                                        colorbar=dict(title="Master Score")
                                    ),
                                    text=filtered_df['ticker'] if 'ticker' in filtered_df.columns else filtered_df.index,
                                    hovertemplate='<b>%{text}</b><br>Momentum: %{x}<br>Acceleration: %{y}<extra></extra>'
                                )
                            )
                            
                            # Add quadrant lines
                            fig_momentum.add_hline(y=60, line_dash="dash", line_color="gray", opacity=0.5)
                            fig_momentum.add_vline(x=60, line_dash="dash", line_color="gray", opacity=0.5)
                            
                            fig_momentum.update_layout(
                                title="Momentum-Acceleration Matrix",
                                xaxis_title="Momentum Score",
                                yaxis_title="Acceleration Score",
                                template='plotly_white',
                                height=400
                            )
                            
                            st.plotly_chart(fig_momentum, width="stretch")
                        else:
                            # Show single momentum metric
                            available_col = momentum_cols[0]
                            st.markdown(f"**📊 {available_col.replace('_', ' ').title()} Distribution**")
                            
                            fig_single = go.Figure()
                            fig_single.add_trace(
                                go.Histogram(
                                    x=filtered_df[available_col],
                                    nbinsx=20,
                                    marker_color='lightblue',
                                    opacity=0.7
                                )
                            )
                            
                            fig_single.update_layout(
                                title=f"{available_col.replace('_', ' ').title()} Distribution",
                                xaxis_title=available_col.replace('_', ' ').title(),
                                yaxis_title="Count",
                                template='plotly_white',
                                height=400
                            )
                            
                            st.plotly_chart(fig_single, width="stretch")
                    
                    with momentum_viz_cols[1]:
                        if len(momentum_cols) == 2:  # Both momentum and acceleration available
                            st.markdown("**📊 Momentum Quadrant Analysis**")
                            
                            # Quadrant analysis
                            high_momentum = filtered_df['momentum_score'] >= 60
                            high_acceleration = filtered_df['acceleration_score'] >= 60
                            
                            quadrants = {
                                '🚀 Explosive (High/High)': len(filtered_df[high_momentum & high_acceleration]),
                                '📈 Building (High/Low)': len(filtered_df[high_momentum & ~high_acceleration]),
                                '⚡ Accelerating (Low/High)': len(filtered_df[~high_momentum & high_acceleration]),
                                '⚖️ Consolidating (Low/Low)': len(filtered_df[~high_momentum & ~high_acceleration])
                            }
                            
                            quadrant_df = pd.DataFrame([
                                {'Quadrant': k, 'Count': v, 'Percentage': (v/len(filtered_df)*100)}
                                for k, v in quadrants.items()
                            ])
                            
                            st.dataframe(
                                quadrant_df,
                                width='stretch',
                                hide_index=True,
                                column_config={
                                    'Quadrant': st.column_config.TextColumn("Momentum Quadrant", width="medium"),
                                    'Count': st.column_config.NumberColumn("Stocks", width="small"),
                                    'Percentage': st.column_config.ProgressColumn("% of Total", min_value=0, max_value=100, format="%.1f")
                                }
                            )
                            
                            # Quadrant insights
                            explosive_pct = (quadrants['🚀 Explosive (High/High)'] / len(filtered_df) * 100)
                            if explosive_pct > 20:
                                st.success(f"🚀 **Explosive Market**: {explosive_pct:.1f}% stocks in high momentum/acceleration")
                            elif explosive_pct > 10:
                                st.info(f"📈 **Building Momentum**: {explosive_pct:.1f}% stocks showing explosive potential")
                            else:
                                st.warning(f"⚖️ **Consolidation Phase**: Only {explosive_pct:.1f}% explosive stocks")
                        else:
                            # Show insights for single momentum metric
                            available_col = momentum_cols[0]
                            col_name = available_col.replace('_', ' ').title()
                            st.markdown(f"**📊 {col_name} Analysis**")
                            
                            high_threshold = filtered_df[available_col].quantile(0.7)
                            high_count = len(filtered_df[filtered_df[available_col] >= high_threshold])
                            high_pct = (high_count / len(filtered_df) * 100)
                            
                            metric_analysis = pd.DataFrame([
                                {'Category': f'🔥 High {col_name}', 'Count': high_count, 'Percentage': high_pct},
                                {'Category': f'📈 Medium {col_name}', 'Count': len(filtered_df) - high_count - len(filtered_df[filtered_df[available_col] <= filtered_df[available_col].quantile(0.3)]), 'Percentage': 100 - high_pct - (len(filtered_df[filtered_df[available_col] <= filtered_df[available_col].quantile(0.3)]) / len(filtered_df) * 100)},
                                {'Category': f'⚖️ Low {col_name}', 'Count': len(filtered_df[filtered_df[available_col] <= filtered_df[available_col].quantile(0.3)]), 'Percentage': len(filtered_df[filtered_df[available_col] <= filtered_df[available_col].quantile(0.3)]) / len(filtered_df) * 100}
                            ])
                            
                            st.dataframe(
                                metric_analysis,
                                width='stretch',
                                hide_index=True,
                                column_config={
                                    'Category': st.column_config.TextColumn(f"{col_name} Range", width="medium"),
                                    'Count': st.column_config.NumberColumn("Stocks", width="small"),
                                    'Percentage': st.column_config.ProgressColumn("% of Total", min_value=0, max_value=100, format="%.1f")
                                }
                            )
                            
                            if high_pct > 30:
                                st.success(f"🔥 **Strong {col_name}**: {high_pct:.1f}% stocks showing high {available_col.replace('_', ' ')}")
                            elif high_pct > 15:
                                st.info(f"📈 **Moderate {col_name}**: {high_pct:.1f}% stocks with elevated {available_col.replace('_', ' ')}")
                            else:
                                st.warning(f"⚖️ **Low {col_name}**: Only {high_pct:.1f}% stocks with high {available_col.replace('_', ' ')}")
                else:
                    st.warning("⚠️ **Momentum data not available** - No momentum or acceleration scores found in dataset")
            
            # Tab 4: Pattern Intelligence
            with viz_tabs[3]:
                st.markdown("#### 💎 **Advanced Pattern Recognition Intelligence**")
                
                pattern_intel_cols = st.columns(2)
                
                with pattern_intel_cols[0]:
                    st.markdown("**🔍 Pattern Frequency Analysis**")
                    
                    if 'patterns' in filtered_df.columns:
                        pattern_counts = {}
                        for patterns in filtered_df['patterns'].dropna():
                            if patterns:
                                for p in patterns.split(' | '):
                                    pattern_counts[p] = pattern_counts.get(p, 0) + 1
                        
                        if pattern_counts:
                            pattern_df = pd.DataFrame(
                                list(pattern_counts.items()),
                                columns=['Pattern', 'Count']
                            ).sort_values('Count', ascending=False).head(15)
                            
                            pattern_df['Percentage'] = (pattern_df['Count'] / len(filtered_df) * 100)
                            pattern_df['Quality'] = pattern_df['Percentage'].apply(
                                lambda x: '🔥' if x > 15 else '📈' if x > 8 else '⚖️' if x > 3 else '📉'
                            )
                            
                            st.dataframe(
                                pattern_df,
                                width='stretch',
                                hide_index=True,
                                column_config={
                                    'Pattern': st.column_config.TextColumn("Pattern Type", width="medium"),
                                    'Count': st.column_config.NumberColumn("Occurrences", width="small"),
                                    'Percentage': st.column_config.ProgressColumn("% of Stocks", min_value=0, max_value=50, format="%.1f"),
                                    'Quality': st.column_config.TextColumn("Quality", width="small")
                                }
                            )
                            
                            # Pattern insights
                            dominant_pattern = pattern_df.iloc[0]['Pattern']
                            dominant_pct = pattern_df.iloc[0]['Percentage']
                            st.success(f"🎯 **Dominant Pattern**: {dominant_pattern} ({dominant_pct:.1f}% of stocks)")
                        else:
                            st.info("📊 **No patterns detected** in current selection")
                    else:
                        st.warning("⚠️ **Pattern data not available** - No pattern column found in dataset")
                
                with pattern_intel_cols[1]:
                    st.markdown("**📊 Pattern Quality Matrix**")
                    
                    if 'breakout_score' in filtered_df.columns:
                        # Pattern quality analysis
                        pattern_quality = {
                            'Elite Patterns (>80)': len(filtered_df[filtered_df['breakout_score'] > 80]),
                            'Strong Patterns (70-80)': len(filtered_df[(filtered_df['breakout_score'] >= 70) & (filtered_df['breakout_score'] <= 80)]),
                            'Good Patterns (60-70)': len(filtered_df[(filtered_df['breakout_score'] >= 60) & (filtered_df['breakout_score'] < 70)]),
                            'Average Patterns (50-60)': len(filtered_df[(filtered_df['breakout_score'] >= 50) & (filtered_df['breakout_score'] < 60)]),
                            'Weak Patterns (<50)': len(filtered_df[filtered_df['breakout_score'] < 50])
                        }
                        
                        quality_df = pd.DataFrame([
                            {'Quality Level': k, 'Count': v, 'Percentage': (v/len(filtered_df)*100)}
                            for k, v in pattern_quality.items()
                        ])
                        
                        st.dataframe(
                            quality_df,
                            width='stretch',
                            hide_index=True,
                            column_config={
                                'Quality Level': st.column_config.TextColumn("Pattern Quality", width="medium"),
                                'Count': st.column_config.NumberColumn("Stocks", width="small"),
                                'Percentage': st.column_config.ProgressColumn("% of Total", min_value=0, max_value=100, format="%.1f")
                            }
                        )
                        
                        # Quality insights
                        elite_patterns = pattern_quality['Elite Patterns (>80)']
                        strong_patterns = pattern_quality['Strong Patterns (70-80)']
                        high_quality_pct = ((elite_patterns + strong_patterns) / len(filtered_df) * 100)
                        
                        if high_quality_pct > 25:
                            st.success(f"💎 **High Pattern Quality**: {high_quality_pct:.1f}% elite/strong patterns")
                        elif high_quality_pct > 15:
                            st.info(f"📈 **Good Pattern Quality**: {high_quality_pct:.1f}% high-quality patterns")
                        else:
                            st.warning(f"⚖️ **Mixed Pattern Quality**: {high_quality_pct:.1f}% high-quality patterns")
            
            # Tab 5: Sector Analysis 
            with viz_tabs[4]:
                st.markdown("#### 🏢 **Comprehensive Sector Intelligence**")
                
                sector_overview_df_local = MarketIntelligence.detect_sector_rotation(filtered_df)
                
                if not sector_overview_df_local.empty:
                    sector_intel_cols = st.columns(2)
                    
                    with sector_intel_cols[0]:
                        st.markdown("**🎯 Sector Leadership Analysis**")
                        
                        # Enhanced sector display
                        display_cols_overview = ['flow_score', 'ldi_score', 'leadership_density', 'analyzed_stocks', 
                                               'total_stocks', 'avg_score', 'elite_avg_score', 'ldi_quality']
                        
                        available_overview_cols = [col for col in display_cols_overview if col in sector_overview_df_local.columns]
                        sector_overview_display = sector_overview_df_local[available_overview_cols].copy()
                        
                        # Add quality indicators
                        if 'ldi_score' in sector_overview_display.columns:
                            sector_overview_display['Sector_Quality'] = sector_overview_display['ldi_score'].apply(
                                lambda x: '💎 Elite' if x > 20 else '🔥 Strong' if x > 15 else '📈 Good' if x > 10 else '⚖️ Average'
                            )
                        
                        st.dataframe(
                            sector_overview_display,
                            width='stretch',
                            column_config={
                                'ldi_score': st.column_config.NumberColumn('LDI Score', format="%.1f%%"),
                                'flow_score': st.column_config.ProgressColumn('Flow Score', min_value=0, max_value=100, format="%.1f"),
                                'avg_score': st.column_config.ProgressColumn('Avg Score', min_value=0, max_value=100, format="%.1f"),
                                'Sector_Quality': st.column_config.TextColumn('Quality', width="small")
                            }
                        )
                        
                        # Sector insights
                        if 'ldi_score' in sector_overview_df_local.columns and len(sector_overview_df_local) > 0:
                            top_sector = sector_overview_df_local.index[0]
                            top_ldi = sector_overview_df_local['ldi_score'].iloc[0]
                            st.success(f"🏆 **Leading Sector**: {top_sector} (LDI: {top_ldi:.1f}%)")
                    
                    with sector_intel_cols[1]:
                        st.markdown("**📊 Sector Distribution Matrix**")
                        
                        if 'sector' in filtered_df.columns:
                            sector_dist = filtered_df['sector'].value_counts().head(10)
                            sector_pct = (sector_dist / len(filtered_df) * 100)
                            
                            sector_matrix = pd.DataFrame({
                                'Sector': sector_dist.index,
                                'Stock Count': sector_dist.values,
                                'Percentage': sector_pct.values,
                                'Representation': sector_pct.map(
                                    lambda x: '🔥 Dominant' if x > 20 else '📈 Strong' if x > 10 else '⚖️ Moderate' if x > 5 else '📉 Light'
                                ).values
                            })
                            
                            st.dataframe(
                                sector_matrix,
                                width='stretch',
                                hide_index=True,
                                column_config={
                                    'Sector': st.column_config.TextColumn("Sector", width="medium"),
                                    'Stock Count': st.column_config.NumberColumn("Stocks", width="small"),
                                    'Percentage': st.column_config.ProgressColumn("% of Total", min_value=0, max_value=50, format="%.1f"),
                                    'Representation': st.column_config.TextColumn("Level", width="small")
                                }
                            )
                        else:
                            st.info("Sector data not available")
                else:
                    st.info("No sector data available for analysis")
            
            # Tab 6: Industry Analysis
            with viz_tabs[5]:
                st.markdown("#### 🏭 **Comprehensive Industry Intelligence Platform**")
                
                industry_intel_cols = st.columns(2)
                
                with industry_intel_cols[0]:
                    st.markdown("**🔍 Industry Performance Rankings**")
                    
                    if 'industry' in filtered_df.columns:
                        # Industry performance analysis
                        industry_metrics = filtered_df.groupby('industry').agg({
                            'master_score': ['mean', 'std', 'count'],
                            'momentum_score': 'mean' if 'momentum_score' in filtered_df.columns else lambda x: None,
                            'acceleration_score': 'mean' if 'acceleration_score' in filtered_df.columns else lambda x: None,
                            'volume_score': 'mean' if 'volume_score' in filtered_df.columns else lambda x: None,
                            'price': 'mean' if 'price' in filtered_df.columns else lambda x: None
                        }).round(2)
                        
                        # Flatten column names
                        industry_metrics.columns = ['avg_score', 'score_volatility', 'stock_count', 
                                                  'momentum_avg', 'acceleration_avg', 'volume_avg', 'avg_price']
                        
                        # Calculate industry strength index (FIXED: Proper normalization to 0-100 range)
                        # Ensure all components are clipped to 0-100 range before combining
                        momentum_norm = industry_metrics['momentum_avg'].fillna(50).clip(0, 100)
                        acceleration_norm = industry_metrics['acceleration_avg'].fillna(50).clip(0, 100)
                        volume_norm = industry_metrics['volume_avg'].fillna(50).clip(0, 100)
                        score_norm = industry_metrics['avg_score'].clip(0, 100)
                        
                        industry_metrics['strength_index'] = (
                            (score_norm * 0.4) +
                            (momentum_norm * 0.2) +
                            (acceleration_norm * 0.2) +
                            (volume_norm * 0.2)
                        ).clip(0, 100).round(1)  # Final clipping to ensure 0-100 range
                        
                        # Add quality indicators
                        industry_metrics['quality'] = industry_metrics['strength_index'].apply(
                            lambda x: '🚀 Elite' if x > 80 else '🔥 Strong' if x > 70 else '📈 Good' if x > 60 else '⚖️ Average' if x > 50 else '📉 Weak'
                        )
                        
                        # Sort by strength index
                        industry_ranking = industry_metrics.sort_values('strength_index', ascending=False)
                        
                        st.dataframe(
                            industry_ranking[['avg_score', 'strength_index', 'stock_count', 'score_volatility', 'quality']],
                            width='stretch',
                            column_config={
                                'avg_score': st.column_config.ProgressColumn('Avg Score', min_value=0, max_value=100, format="%.1f"),
                                'strength_index': st.column_config.ProgressColumn('Strength Index', min_value=0, max_value=100, format="%.1f"),
                                'stock_count': st.column_config.NumberColumn('Stock Count', width="small"),
                                'score_volatility': st.column_config.NumberColumn('Volatility', width="small", format="%.1f"),
                                'quality': st.column_config.TextColumn('Quality', width="small")
                            }
                        )
                        
                        # Industry insights
                        if len(industry_ranking) > 0:
                            top_industry = industry_ranking.index[0]
                            top_strength = industry_ranking['strength_index'].iloc[0]
                            top_count = industry_ranking['stock_count'].iloc[0]
                            st.success(f"🏆 **Leading Industry**: {top_industry} (Strength: {top_strength:.1f}, {top_count} stocks)")
                    else:
                        st.warning("⚠️ **Industry data not available** - No industry column found in dataset")
                
                with industry_intel_cols[1]:
                    st.markdown("**📊 Industry Concentration Analysis**")
                    
                    if 'industry' in filtered_df.columns:
                        industry_dist = filtered_df['industry'].value_counts().head(12)
                        industry_pct = (industry_dist / len(filtered_df) * 100)
                        
                        # Create concentration matrix
                        concentration_matrix = pd.DataFrame({
                            'Industry': industry_dist.index,
                            'Stock Count': industry_dist.values,
                            'Market Share': industry_pct.values,
                            'Concentration': industry_pct.apply(
                                lambda x: '🔥 Dominant' if x > 15 else '📈 Major' if x > 8 else '⚖️ Moderate' if x > 4 else '📉 Niche'
                            ).values
                        })
                        
                        st.dataframe(
                            concentration_matrix,
                            width='stretch',
                            hide_index=True,
                            column_config={
                                'Industry': st.column_config.TextColumn("Industry", width="medium"),
                                'Stock Count': st.column_config.NumberColumn("Stocks", width="small"),
                                'Market Share': st.column_config.ProgressColumn("% of Market", min_value=0, max_value=50, format="%.1f"),
                                'Concentration': st.column_config.TextColumn("Level", width="small")
                            }
                        )
                        
                        # Concentration insights
                        total_industries = len(filtered_df['industry'].unique())
                        top_5_concentration = industry_pct.head(5).sum()
                        
                        if top_5_concentration > 60:
                            st.warning(f"⚠️ **High Concentration**: Top 5 industries control {top_5_concentration:.1f}% of market")
                        elif top_5_concentration > 40:
                            st.info(f"📊 **Moderate Concentration**: Top 5 industries hold {top_5_concentration:.1f}% of market")
                        else:
                            st.success(f"🎯 **Diversified Market**: Well-distributed across {total_industries} industries")
                        
                        # Industry momentum analysis
                        if 'momentum_score' in filtered_df.columns:
                            st.markdown("**🚀 Industry Momentum Leaders**")
                            
                            industry_momentum = filtered_df.groupby('industry')['momentum_score'].mean().sort_values(ascending=False).head(8)
                            momentum_leaders = pd.DataFrame({
                                'Industry': industry_momentum.index,
                                'Avg Momentum': industry_momentum.values.round(1),
                                'Momentum Level': industry_momentum.apply(
                                    lambda x: '🚀 Explosive' if x > 80 else '🔥 Strong' if x > 70 else '📈 Building' if x > 60 else '⚖️ Stable'
                                ).values
                            })
                            
                            st.dataframe(
                                momentum_leaders,
                                width='stretch',
                                hide_index=True,
                                column_config={
                                    'Industry': st.column_config.TextColumn("Industry", width="medium"),
                                    'Avg Momentum': st.column_config.ProgressColumn("Momentum Score", min_value=0, max_value=100, format="%.1f"),
                                    'Momentum Level': st.column_config.TextColumn("Status", width="small")
                                }
                            )
                    else:
                        st.warning("⚠️ **Industry data not available** - No industry column found in dataset")
            
            # Tab 7: Risk Dashboard
            with viz_tabs[6]:
                st.markdown("#### ⚡ **Advanced Risk Management Dashboard**")
                
                risk_cols = st.columns(2)
                
                with risk_cols[0]:
                    st.markdown("**🛡️ Risk Distribution Analysis**")
                    
                    # Calculate risk metrics
                    if 'rvol' in filtered_df.columns and 'master_score' in filtered_df.columns:
                        # Risk categorization based on volume and score
                        high_risk = (filtered_df['rvol'] > 4) | (filtered_df['master_score'] < 50)
                        medium_risk = ((filtered_df['rvol'] >= 2) & (filtered_df['rvol'] <= 4)) & (filtered_df['master_score'] >= 50)
                        low_risk = (filtered_df['rvol'] < 2) & (filtered_df['master_score'] >= 70)
                        
                        risk_distribution = {
                            '🚨 High Risk': len(filtered_df[high_risk]),
                            '⚠️ Medium Risk': len(filtered_df[medium_risk]),
                            '🛡️ Low Risk': len(filtered_df[low_risk])
                        }
                        
                        risk_df = pd.DataFrame([
                            {'Risk Level': k, 'Count': v, 'Percentage': (v/len(filtered_df)*100)}
                            for k, v in risk_distribution.items()
                        ])
                        
                        st.dataframe(
                            risk_df,
                            width='stretch',
                            hide_index=True,
                            column_config={
                                'Risk Level': st.column_config.TextColumn("Risk Category", width="medium"),
                                'Count': st.column_config.NumberColumn("Stocks", width="small"),
                                'Percentage': st.column_config.ProgressColumn("% of Portfolio", min_value=0, max_value=100, format="%.1f")
                            }
                        )
                        
                        # Risk insights
                        high_risk_pct = (risk_distribution['🚨 High Risk'] / len(filtered_df) * 100)
                        if high_risk_pct > 40:
                            st.error(f"🚨 **High Risk Portfolio**: {high_risk_pct:.1f}% high-risk positions")
                        elif high_risk_pct > 25:
                            st.warning(f"⚠️ **Moderate Risk**: {high_risk_pct:.1f}% high-risk positions")
                        else:
                            st.success(f"🛡️ **Controlled Risk**: Only {high_risk_pct:.1f}% high-risk positions")
                
                with risk_cols[1]:
                    st.markdown("**📊 Position Sizing Recommendations**")
                    
                    if 'master_score' in filtered_df.columns and 'rvol' in filtered_df.columns:
                        # Position sizing based on score and volatility
                        filtered_df_copy = filtered_df.copy()
                        
                        # Calculate recommended position size (1-5% of portfolio)
                        filtered_df_copy['position_size'] = (
                            (filtered_df_copy['master_score'] / 100 * 3) +  # Base on score (0-3%)
                            np.where(filtered_df_copy['rvol'] < 2, 2, 1)     # Bonus for low volatility
                        ).clip(0.5, 5.0)  # Min 0.5%, Max 5%
                        
                        # Position categories
                        position_categories = pd.cut(
                            filtered_df_copy['position_size'], 
                            bins=[0, 1, 2, 3, 5], 
                            labels=['🔸 Small (0.5-1%)', '🔹 Medium (1-2%)', '🔷 Large (2-3%)', '💎 Max (3-5%)']
                        ).value_counts()
                        
                        position_df = pd.DataFrame({
                            'Position Size': position_categories.index,
                            'Count': position_categories.values,
                            'Percentage': (position_categories.values / len(filtered_df) * 100)
                        })
                        
                        st.dataframe(
                            position_df,
                            width='stretch',
                            hide_index=True,
                            column_config={
                                'Position Size': st.column_config.TextColumn("Recommended Size", width="medium"),
                                'Count': st.column_config.NumberColumn("Stocks", width="small"),
                                'Percentage': st.column_config.ProgressColumn("% Allocation", min_value=0, max_value=100, format="%.1f")
                            }
                        )
                        
                        # Position sizing insights
                        max_positions = position_categories.get('💎 Max (3-5%)', 0)
                        max_pct = (max_positions / len(filtered_df) * 100) if len(filtered_df) > 0 else 0
                        
                        if max_pct > 15:
                            st.success(f"💎 **High Conviction Opportunities**: {max_pct:.1f}% qualify for maximum position size")
                        elif max_pct > 8:
                            st.info(f"🔷 **Solid Opportunities**: {max_pct:.1f}% qualify for large positions")
                        else:
                            st.warning(f"🔸 **Conservative Allocation**: Only {max_pct:.1f}% qualify for large positions")
            
            # ================================================================================================
            # 🎯 EXECUTIVE SUMMARY AND ACTIONABLE INSIGHTS
            # ================================================================================================
            
            st.markdown("---")
            st.markdown("### 🎯 **EXECUTIVE SUMMARY & ACTIONABLE INSIGHTS**")
            
            # Ensure variables are always available for this section
            total_analyzed = len(filtered_df)
            elite_threshold = 80
            strong_threshold = 70
            elite_stocks = len(filtered_df[filtered_df['master_score'] >= elite_threshold]) if 'master_score' in filtered_df.columns else 0
            strong_stocks = len(filtered_df[filtered_df['master_score'] >= strong_threshold]) if 'master_score' in filtered_df.columns else 0
            elite_pct = (elite_stocks / total_analyzed * 100) if total_analyzed > 0 else 0
            
            summary_cols = st.columns(2)
            
            with summary_cols[0]:
                st.markdown("#### 📋 **Key Findings**")
                
                findings = []
                
                # Market sentiment finding
                if elite_stocks > total_analyzed * 0.15:
                    findings.append(f"🔥 **Strong Market**: {elite_pct:.1f}% elite stocks indicate bullish conditions")
                elif strong_stocks > total_analyzed * 0.25:
                    findings.append(f"📈 **Positive Market**: {(strong_stocks/total_analyzed*100):.1f}% strong stocks")
                else:
                    findings.append(f"⚖️ **Neutral Market**: {(strong_stocks/total_analyzed*100):.1f}% strong stocks - selective approach needed")
                
                # Volume finding
                if 'rvol' in filtered_df.columns:
                    high_vol_stocks = len(filtered_df[filtered_df['rvol'] > 2.5])
                    high_vol_pct = (high_vol_stocks / total_analyzed * 100)
                    if high_vol_pct > 25:
                        findings.append(f"📊 **High Activity**: {high_vol_pct:.1f}% stocks show elevated volume")
                    else:
                        findings.append(f"📉 **Low Activity**: Only {high_vol_pct:.1f}% stocks with high volume")
                
                # Momentum finding
                if 'momentum_score' in filtered_df.columns:
                    avg_momentum = filtered_df['momentum_score'].mean()
                    if avg_momentum > 65:
                        findings.append(f"🚀 **Strong Momentum**: {avg_momentum:.1f} average momentum score")
                    elif avg_momentum > 55:
                        findings.append(f"📈 **Building Momentum**: {avg_momentum:.1f} average momentum score")
                    else:
                        findings.append(f"⚖️ **Weak Momentum**: {avg_momentum:.1f} average momentum score")
                
                # Risk finding
                if 'rvol' in filtered_df.columns:
                    high_risk_count = len(filtered_df[filtered_df['rvol'] > 4])
                    risk_pct = (high_risk_count / total_analyzed * 100)
                    if risk_pct > 30:
                        findings.append(f"🚨 **High Risk Environment**: {risk_pct:.1f}% highly volatile stocks")
                    elif risk_pct > 15:
                        findings.append(f"⚠️ **Moderate Risk**: {risk_pct:.1f}% highly volatile stocks")
                    else:
                        findings.append(f"🛡️ **Low Risk Environment**: Only {risk_pct:.1f}% highly volatile stocks")
                
                for finding in findings:
                    st.write(f"• {finding}")
            
            with summary_cols[1]:
                st.markdown("#### 🎯 **Strategic Recommendations**")
                
                recommendations = []
                
                # Position sizing recommendation
                if elite_stocks > total_analyzed * 0.1:
                    recommendations.append("💎 **Increase Position Sizes** on elite-scoring stocks (80+)")
                else:
                    recommendations.append("🔸 **Conservative Sizing** - limited high-conviction opportunities")
                
                # Diversification recommendation
                if 'sector' in filtered_df.columns:
                    sector_concentration = filtered_df['sector'].value_counts().iloc[0] / len(filtered_df)
                    if sector_concentration > 0.4:
                        recommendations.append("🌐 **Increase Diversification** - high sector concentration detected")
                    else:
                        recommendations.append("✅ **Good Diversification** - balanced sector exposure")
                
                # Risk management recommendation
                if 'rvol' in filtered_df.columns:
                    avg_rvol = filtered_df['rvol'].mean()
                    if avg_rvol > 3:
                        recommendations.append("🛡️ **Implement Stop Losses** - high volatility environment")
                    elif avg_rvol > 2:
                        recommendations.append("⚠️ **Monitor Positions Closely** - elevated volatility")
                    else:
                        recommendations.append("📈 **Normal Risk Management** - stable environment")
                
                # Timing recommendation
                if 'momentum_score' in filtered_df.columns:
                    momentum_trend = filtered_df['momentum_score'].mean()
                    if momentum_trend > 65:
                        recommendations.append("⚡ **Aggressive Entry** - strong momentum environment")
                    elif momentum_trend > 55:
                        recommendations.append("📈 **Selective Entry** - building momentum")
                    else:
                        recommendations.append("⏳ **Wait for Setup** - weak momentum environment")
                
                for rec in recommendations:
                    st.write(f"• {rec}")
            
            # ================================================================================================
            # 🏆 TOP PICKS SUMMARY
            # ================================================================================================
            
            st.markdown("---")
            st.markdown("### 🏆 **INSTITUTIONAL TOP PICKS**")
            
            if 'master_score' in filtered_df.columns:
                # Get top picks based on multiple criteria
                top_picks = filtered_df[
                    (filtered_df['master_score'] >= 75) &
                    (filtered_df['rvol'] >= 1.5 if 'rvol' in filtered_df.columns else True)
                ].nlargest(10, 'master_score')
                
                if not top_picks.empty:
                    # Display top picks with key metrics
                    display_columns = ['ticker', 'company_name', 'master_score']
                    if 'momentum_score' in top_picks.columns:
                        display_columns.append('momentum_score')
                    if 'rvol' in top_picks.columns:
                        display_columns.append('rvol')
                    if 'ret_1d' in top_picks.columns:
                        display_columns.append('ret_1d')
                    
                    picks_display = top_picks[display_columns].copy()
                    
                    # Add quality rating
                    picks_display['Quality'] = picks_display['master_score'].apply(
                        lambda x: '💎 Elite' if x >= 85 else '🔥 Premium' if x >= 80 else '📈 Strong'
                    )
                    
                    st.dataframe(
                        picks_display,
                        width='stretch',
                        hide_index=True,
                        column_config={
                            'ticker': st.column_config.TextColumn("Ticker", width="small"),
                            'company_name': st.column_config.TextColumn("Company", width="medium"),
                            'master_score': st.column_config.ProgressColumn("Master Score", min_value=0, max_value=100, format="%.1f"),
                            'momentum_score': st.column_config.ProgressColumn("Momentum", min_value=0, max_value=100, format="%.1f"),
                            'rvol': st.column_config.NumberColumn("RVOL", format="%.1fx"),
                            'ret_1d': st.column_config.NumberColumn("1D Return", format="%.2f%%"),
                            'Quality': st.column_config.TextColumn("Quality", width="small")
                        }
                    )
                    
                    # Top pick insights
                    elite_picks = len(picks_display[picks_display['Quality'] == '💎 Elite'])
                    premium_picks = len(picks_display[picks_display['Quality'] == '🔥 Premium'])
                    
                    if elite_picks > 0:
                        st.success(f"💎 **{elite_picks} Elite-Grade Opportunities** identified for institutional allocation")
                    if premium_picks > 0:
                        st.success(f"🔥 **{premium_picks} Premium-Grade Opportunities** suitable for aggressive positions")
                else:
                    st.info("📊 No stocks meet institutional-grade criteria in current selection")
        
        else:
            st.warning("📊 No data available for comprehensive analysis. Please adjust your filters.")
            st.info("💡 **Tip**: Try expanding your filters or checking different tabs for data availability")
    
    # Tab 4: Search
    # Tab 4: Search
    with tabs[4]:
        st.markdown("### 🔍 Advanced Stock Search")
        
        # Search interface
        col1, col2 = st.columns([4, 1])
        
        with col1:
            search_query = st.text_input(
                "Search stocks",
                placeholder="Enter ticker or company name...",
                help="Search by ticker symbol or company name",
                key="search_input"
            )
        
        with col2:
            st.markdown("<br>", unsafe_allow_html=True)
            search_clicked = st.button("🔎 Search", type="primary", width="stretch", key="search_btn")
        
        # Perform search
        if search_query or search_clicked:
            with st.spinner("Searching..."):
                search_results = SearchEngine.search_stocks(filtered_df, search_query)

            if not search_results.empty:
                # ENSURE PATTERN CONFIDENCE IS CALCULATED FOR SEARCH RESULTS
                if 'patterns' in search_results.columns and 'pattern_confidence' not in search_results.columns:
                    search_results = PatternDetector._calculate_pattern_confidence(search_results)
            
            if not search_results.empty:
                st.success(f"Found {len(search_results)} matching stock(s)")
                
                # Create summary dataframe for search results
                summary_columns = ['ticker', 'company_name', 'rank', 'master_score', 'price', 
                                  'ret_30d', 'rvol', 'market_state', 'category']
                
                available_summary_cols = [col for col in summary_columns if col in search_results.columns]
                search_summary = search_results[available_summary_cols].copy()
                
                # Format the summary data
                if 'price' in search_summary.columns:
                    search_summary['price_display'] = search_summary['price'].apply(
                        lambda x: f"₹{x:,.0f}" if pd.notna(x) else '-'
                    )
                    search_summary = search_summary.drop('price', axis=1)
                
                if 'ret_30d' in search_summary.columns:
                    search_summary['ret_30d_display'] = search_summary['ret_30d'].apply(
                        lambda x: f"{x:+.1f}%" if pd.notna(x) else '-'
                    )
                    search_summary = search_summary.drop('ret_30d', axis=1)
                
                if 'rvol' in search_summary.columns:
                    search_summary['rvol_display'] = search_summary['rvol'].apply(
                        lambda x: f"{x:.1f}x" if pd.notna(x) else '-'
                    )
                    search_summary = search_summary.drop('rvol', axis=1)
                
                # Rename columns for display
                column_rename = {
                    'ticker': 'Ticker',
                    'company_name': 'Company',
                    'rank': 'Rank',
                    'master_score': 'Score',
                    'price_display': 'Price',
                    'ret_30d_display': '30D Return',
                    'rvol_display': 'RVOL',
                    'market_state': 'Market State',
                    'category': 'Category'
                }
                
                search_summary = search_summary.rename(columns=column_rename)
                
                # Display search results summary with optimized column_config
                st.markdown("#### 📊 Search Results Overview")
                st.dataframe(
                    search_summary,
                    width="stretch",
                    hide_index=True,
                    column_config={
                        'Ticker': st.column_config.TextColumn(
                            'Ticker',
                            help="Stock symbol - Click expander below for details",
                            width="small"
                        ),
                        'Company': st.column_config.TextColumn(
                            'Company',
                            help="Company name",
                            width="large"
                        ),
                        'Rank': st.column_config.NumberColumn(
                            'Rank',
                            help="Overall ranking position",
                            format="%d",
                            width="small"
                        ),
                        'Score': st.column_config.ProgressColumn(
                            'Score',
                            help="Master Score (0-100)",
                            format="%.1f",
                            min_value=0,
                            max_value=100,
                            width="small"
                        ),
                        'Price': st.column_config.TextColumn(
                            'Price',
                            help="Current stock price",
                            width="small"
                        ),
                        '30D Return': st.column_config.TextColumn(
                            '30D Return',
                            help="30-day return percentage",
                            width="small"
                        ),
                        'RVOL': st.column_config.TextColumn(
                            'RVOL',
                            help="Relative Volume",
                            width="small"
                        ),
                        'Market State': st.column_config.TextColumn(
                            'Market State',
                            help="Current momentum market state",
                            width="medium"
                        ),
                        'Category': st.column_config.TextColumn(
                            'Category',
                            help="Market cap category",
                            width="medium"
                        )
                    }
                )
                
                st.markdown("---")
                st.markdown("#### 📋 Detailed Stock Information")
                
                # Display each result in expandable sections
                for idx, stock in search_results.iterrows():
                    with st.expander(
                        f"📊 {stock['ticker']} - {stock['company_name']} "
                        f"(Rank #{int(stock['rank'])})",
                        expanded=(len(search_results) == 1)  # Auto-expand if only one result
                    ):
                        # Header metrics
                        metric_cols = st.columns(6)
                        
                        with metric_cols[0]:
                            UIComponents.render_metric_card(
                                "Master Score",
                                f"{stock['master_score']:.1f}",
                                f"Rank #{int(stock['rank'])}"
                            )
                        
                        with metric_cols[1]:
                            price_value = f"₹{stock['price']:,.0f}" if pd.notna(stock.get('price')) else "N/A"
                            ret_1d_value = f"{stock['ret_1d']:+.1f}%" if pd.notna(stock.get('ret_1d')) else None
                            UIComponents.render_metric_card("Price", price_value, ret_1d_value)
                        
                        with metric_cols[2]:
                            UIComponents.render_metric_card(
                                "From Low",
                                f"{stock['from_low_pct']:.0f}%" if pd.notna(stock.get('from_low_pct')) else "N/A",
                                "52-week range position"
                            )
                        
                        with metric_cols[3]:
                            ret_30d = stock.get('ret_30d', 0)
                            UIComponents.render_metric_card(
                                "30D Return",
                                f"{ret_30d:+.1f}%" if pd.notna(ret_30d) else "N/A",
                                "↑" if ret_30d > 0 else "↓" if ret_30d < 0 else "→"
                            )
                        
                        with metric_cols[4]:
                            rvol = stock.get('rvol', 1)
                            UIComponents.render_metric_card(
                                "RVOL",
                                f"{rvol:.1f}x" if pd.notna(rvol) else "N/A",
                                "High" if rvol > 2 else "Normal" if rvol > 0.5 else "Low"
                            )
                        
                        with metric_cols[5]:
                            UIComponents.render_metric_card(
                                "Market State",
                                stock.get('market_state', 'N/A'),
                                stock.get('category', 'N/A')
                            )
                        
                        # Score breakdown with optimized display
                        st.markdown("#### 📈 Score Components")
                        
                        # Create score breakdown dataframe
                        score_data = {
                            'Component': ['Position', 'Volume', 'Momentum', 'Acceleration', 'Breakout', 'RVOL'],
                            'Score': [
                                stock.get('position_score', 0),
                                stock.get('volume_score', 0),
                                stock.get('momentum_score', 0),
                                stock.get('acceleration_score', 0),
                                stock.get('breakout_score', 0),
                                stock.get('rvol_score', 0)
                            ],
                            'Weight': [
                                f"{CONFIG.POSITION_WEIGHT:.0%}",
                                f"{CONFIG.VOLUME_WEIGHT:.0%}",
                                f"{CONFIG.MOMENTUM_WEIGHT:.0%}",
                                f"{CONFIG.ACCELERATION_WEIGHT:.0%}",
                                f"{CONFIG.BREAKOUT_WEIGHT:.0%}",
                                f"{CONFIG.RVOL_WEIGHT:.0%}"
                            ],
                            'Contribution': [
                                stock.get('position_score', 0) * CONFIG.POSITION_WEIGHT,
                                stock.get('volume_score', 0) * CONFIG.VOLUME_WEIGHT,
                                stock.get('momentum_score', 0) * CONFIG.MOMENTUM_WEIGHT,
                                stock.get('acceleration_score', 0) * CONFIG.ACCELERATION_WEIGHT,
                                stock.get('breakout_score', 0) * CONFIG.BREAKOUT_WEIGHT,
                                stock.get('rvol_score', 0) * CONFIG.RVOL_WEIGHT
                            ]
                        }
                        
                        score_df = pd.DataFrame(score_data)
                        
                        # Add quality indicator
                        score_df['Quality'] = score_df['Score'].apply(
                            lambda x: '🟢 Strong' if x >= 80 
                            else '🟡 Good' if x >= 60 
                            else '🟠 Fair' if x >= 40 
                            else '🔴 Weak'
                        )
                        
                        # Display score breakdown with column_config
                        st.dataframe(
                            score_df,
                            width="stretch",
                            hide_index=True,
                            column_config={
                                'Component': st.column_config.TextColumn(
                                    'Component',
                                    help="Score component name",
                                    width="medium"
                                ),
                                'Score': st.column_config.ProgressColumn(
                                    'Score',
                                    help="Component score (0-100)",
                                    format="%.1f",
                                    min_value=0,
                                    max_value=100,
                                    width="small"
                                ),
                                'Weight': st.column_config.TextColumn(
                                    'Weight',
                                    help="Component weight in master score",
                                    width="small"
                                ),
                                'Contribution': st.column_config.NumberColumn(
                                    'Contribution',
                                    help="Points contributed to master score",
                                    format="%.1f",
                                    width="small"
                                ),
                                'Quality': st.column_config.TextColumn(
                                    'Quality',
                                    help="Component strength indicator",
                                    width="small"
                                )
                            }
                        )
                        
                        # Patterns
                        if stock.get('patterns'):
                            st.markdown(f"**🎯 Patterns Detected:**")
                            patterns_list = stock['patterns'].split(' | ')
                            pattern_cols = st.columns(min(3, len(patterns_list)))
                            for i, pattern in enumerate(patterns_list):
                                with pattern_cols[i % 3]:
                                    st.info(pattern)
                        
                        # Additional details in organized tabs
                        detail_tabs = st.tabs(["📊 Classification", "📈 Performance", "💰 Fundamentals", "🔍 Technicals", "📊 Volume", "🎯 Advanced"])
                        
                        with detail_tabs[0]:  # Classification
                            class_col1, class_col2 = st.columns(2)
                            
                            with class_col1:
                                st.markdown("**📊 Stock Classification**")
                                classification_data = {
                                    'Attribute': ['Sector', 'Industry', 'Category', 'Market Cap'],
                                    'Value': [
                                        stock.get('sector', 'Unknown'),
                                        stock.get('industry', 'Unknown'),
                                        stock.get('category', 'Unknown'),
                                        stock.get('market_cap', 'N/A')
                                    ]
                                }
                                class_df = pd.DataFrame(classification_data)
                                st.dataframe(
                                    class_df,
                                    width="stretch",
                                    hide_index=True,
                                    column_config={
                                        'Attribute': st.column_config.TextColumn('Attribute', width="medium"),
                                        'Value': st.column_config.TextColumn('Value', width="large")
                                    }
                                )
                            
                            with class_col2:
                                st.markdown("**📈 Tier Classifications**")
                                tier_data = {
                                    'Tier Type': [],
                                    'Classification': []
                                }
                                
                                if 'price_tier' in stock.index:
                                    tier_data['Tier Type'].append('Price Tier')
                                    tier_data['Classification'].append(stock.get('price_tier', 'N/A'))
                                
                                if 'eps_tier' in stock.index:
                                    tier_data['Tier Type'].append('EPS Tier')
                                    tier_data['Classification'].append(stock.get('eps_tier', 'N/A'))
                                
                                if 'pe_tier' in stock.index:
                                    tier_data['Tier Type'].append('PE Tier')
                                    tier_data['Classification'].append(stock.get('pe_tier', 'N/A'))

                                if 'eps_change_tier' in stock.index:
                                    tier_data['Tier Type'].append('EPS Growth')
                                    tier_data['Classification'].append(stock.get('eps_change_tier', 'N/A'))
                                
                                if tier_data['Tier Type']:
                                    tier_df = pd.DataFrame(tier_data)
                                    st.dataframe(
                                        tier_df,
                                        width="stretch",
                                        hide_index=True,
                                        column_config={
                                            'Tier Type': st.column_config.TextColumn('Type', width="medium"),
                                            'Classification': st.column_config.TextColumn('Class', width="medium")
                                        }
                                    )
                                else:
                                    st.info("No tier data available")
                        
                        with detail_tabs[1]:  # Performance
                            st.markdown("**📈 Historical Performance**")
                            
                            perf_data = {
                                'Period': [],
                                'Return': [],
                                'Status': []
                            }
                            
                            periods = [
                                ('1 Day', 'ret_1d'),
                                ('3 Days', 'ret_3d'),
                                ('7 Days', 'ret_7d'),
                                ('30 Days', 'ret_30d'),
                                ('3 Months', 'ret_3m'),
                                ('6 Months', 'ret_6m'),
                                ('1 Year', 'ret_1y'),
                                ('3 Years', 'ret_3y'),
                                ('5 Years', 'ret_5y')
                            ]
                            
                            for period_name, col_name in periods:
                                if col_name in stock.index and pd.notna(stock[col_name]):
                                    perf_data['Period'].append(period_name)
                                    ret_val = stock[col_name]
                                    perf_data['Return'].append(f"{ret_val:+.1f}%")
                                    
                                    if ret_val > 10:
                                        perf_data['Status'].append('🟢 Strong')
                                    elif ret_val > 0:
                                        perf_data['Status'].append('🟡 Positive')
                                    elif ret_val > -10:
                                        perf_data['Status'].append('🟠 Negative')
                                    else:
                                        perf_data['Status'].append('🔴 Weak')
                            
                            if perf_data['Period']:
                                perf_df = pd.DataFrame(perf_data)
                                st.dataframe(
                                    perf_df,
                                    width="stretch",
                                    hide_index=True,
                                    column_config={
                                        'Period': st.column_config.TextColumn('Period', width="medium"),
                                        'Return': st.column_config.TextColumn('Return', width="small"),
                                        'Status': st.column_config.TextColumn('Status', width="small")
                                    }
                                )
                            else:
                                st.info("No performance data available")
                        
                        with detail_tabs[2]:  # Fundamentals
                            if show_fundamentals:
                                st.markdown("**💰 Fundamental Analysis**")
                                
                                fund_data = {
                                    'Metric': [],
                                    'Value': [],
                                    'Assessment': []
                                }
                                
                                # PE Ratio
                                if 'pe' in stock.index and pd.notna(stock['pe']):
                                    fund_data['Metric'].append('PE Ratio')
                                    pe_val = stock['pe']
                                    
                                    if pe_val <= 0:
                                        fund_data['Value'].append('Loss/Negative')
                                        fund_data['Assessment'].append('🔴 No Earnings')
                                    elif pe_val < 15:
                                        fund_data['Value'].append(f"{pe_val:.1f}x")
                                        fund_data['Assessment'].append('🟢 Undervalued')
                                    elif pe_val < 25:
                                        fund_data['Value'].append(f"{pe_val:.1f}x")
                                        fund_data['Assessment'].append('🟡 Fair Value')
                                    elif pe_val < 50:
                                        fund_data['Value'].append(f"{pe_val:.1f}x")
                                        fund_data['Assessment'].append('🟠 Expensive')
                                    else:
                                        fund_data['Value'].append(f"{pe_val:.1f}x")
                                        fund_data['Assessment'].append('🔴 Very Expensive')
                                
                                # EPS
                                if 'eps_current' in stock.index and pd.notna(stock['eps_current']):
                                    fund_data['Metric'].append('Current EPS')
                                    fund_data['Value'].append(f"₹{stock['eps_current']:.2f}")
                                    fund_data['Assessment'].append('📊 Earnings/Share')
                                
                                # EPS Change
                                if 'eps_change_pct' in stock.index and pd.notna(stock['eps_change_pct']):
                                    fund_data['Metric'].append('EPS Growth')
                                    eps_chg = stock['eps_change_pct']
                                    
                                    if eps_chg >= 100:
                                        fund_data['Value'].append(f"{eps_chg:+.0f}%")
                                        fund_data['Assessment'].append('🚀 Explosive Growth')
                                    elif eps_chg >= 50:
                                        fund_data['Value'].append(f"{eps_chg:+.1f}%")
                                        fund_data['Assessment'].append('🔥 High Growth')
                                    elif eps_chg >= 20:
                                        fund_data['Value'].append(f"{eps_chg:+.1f}%")
                                        fund_data['Assessment'].append('🟢 Good Growth')
                                    elif eps_chg >= 0:
                                        fund_data['Value'].append(f"{eps_chg:+.1f}%")
                                        fund_data['Assessment'].append('🟡 Modest Growth')
                                    else:
                                        fund_data['Value'].append(f"{eps_chg:+.1f}%")
                                        fund_data['Assessment'].append('🔴 Declining')
                                
                                if fund_data['Metric']:
                                    fund_df = pd.DataFrame(fund_data)
                                    st.dataframe(
                                        fund_df,
                                        width="stretch",
                                        hide_index=True,
                                        column_config={
                                            'Metric': st.column_config.TextColumn('Metric', width="medium"),
                                            'Value': st.column_config.TextColumn('Value', width="small"),
                                            'Assessment': st.column_config.TextColumn('Assessment', width="medium")
                                        }
                                    )
                                else:
                                    st.info("No fundamental data available")
                            else:
                                st.info("Enable 'Hybrid' display mode to see fundamental data")
                        
                        with detail_tabs[3]:  # Technicals
                            st.markdown("**🔍 Technical Analysis**")
                            
                            tech_col1, tech_col2 = st.columns(2)
                            
                            with tech_col1:
                                st.markdown("**📊 52-Week Range**")
                                range_data = {
                                    'Metric': [],
                                    'Value': []
                                }
                                
                                if 'low_52w' in stock.index and pd.notna(stock['low_52w']):
                                    range_data['Metric'].append('52W Low')
                                    range_data['Value'].append(f"₹{stock['low_52w']:,.0f}")
                                
                                if 'high_52w' in stock.index and pd.notna(stock['high_52w']):
                                    range_data['Metric'].append('52W High')
                                    range_data['Value'].append(f"₹{stock['high_52w']:,.0f}")
                                
                                if 'from_low_pct' in stock.index and pd.notna(stock['from_low_pct']):
                                    range_data['Metric'].append('From Low')
                                    range_data['Value'].append(f"{stock['from_low_pct']:.0f}%")
                                
                                if 'from_high_pct' in stock.index and pd.notna(stock['from_high_pct']):
                                    range_data['Metric'].append('From High')
                                    range_data['Value'].append(f"{stock['from_high_pct']:.0f}%")
                                
                                if range_data['Metric']:
                                    range_df = pd.DataFrame(range_data)
                                    st.dataframe(
                                        range_df,
                                        width="stretch",
                                        hide_index=True,
                                        column_config={
                                            'Metric': st.column_config.TextColumn('Metric', width="medium"),
                                            'Value': st.column_config.TextColumn('Value', width="medium")
                                        }
                                    )
                            
                            with tech_col2:
                                st.markdown("**📈 Moving Averages**")
                                sma_data = {
                                    'SMA': [],
                                    'Value': [],
                                    'Position': []
                                }
                                
                                current_price = stock.get('price', 0)
                                
                                for sma_col, sma_label in [('sma_20d', '20 DMA'), ('sma_50d', '50 DMA'), ('sma_200d', '200 DMA')]:
                                    if sma_col in stock.index and pd.notna(stock[sma_col]) and stock[sma_col] > 0:
                                        sma_value = stock[sma_col]
                                        sma_data['SMA'].append(sma_label)
                                        sma_data['Value'].append(f"₹{sma_value:,.0f}")
                                        
                                        if current_price > sma_value:
                                            pct_diff = ((current_price - sma_value) / sma_value) * 100
                                            sma_data['Position'].append(f"🟢 +{pct_diff:.1f}%")
                                        else:
                                            pct_diff = ((sma_value - current_price) / sma_value) * 100
                                            sma_data['Position'].append(f"🔴 -{pct_diff:.1f}%")
                                
                                if sma_data['SMA']:
                                    sma_df = pd.DataFrame(sma_data)
                                    st.dataframe(
                                        sma_df,
                                        width="stretch",
                                        hide_index=True,
                                        column_config={
                                            'SMA': st.column_config.TextColumn('SMA', width="small"),
                                            'Value': st.column_config.TextColumn('Value', width="medium"),
                                            'Position': st.column_config.TextColumn('Position', width="small")
                                        }
                                    )
                            
                            # Trend Analysis
                            if 'trend_quality' in stock.index and pd.notna(stock['trend_quality']):
                                tq = stock['trend_quality']
                                if tq >= 85:
                                    trend_status = f"🔥 Exceptional ({tq:.0f})"
                                    trend_color = "success"
                                elif tq >= 70:
                                    trend_status = f"🚀 Strong ({tq:.0f})"
                                    trend_color = "success"
                                elif tq >= 55:
                                    trend_status = f"✅ Good ({tq:.0f})"
                                    trend_color = "success"
                                elif tq >= 40:
                                    trend_status = f"➡️ Neutral ({tq:.0f})"
                                    trend_color = "warning"
                                elif tq >= 25:
                                    trend_status = f"⚠️ Weak ({tq:.0f})"
                                    trend_color = "warning"
                                else:
                                    trend_status = f"🔻 Poor ({tq:.0f})"
                                    trend_color = "error"
                                
                                getattr(st, trend_color)(f"**Trend Status:** {trend_status}")
                        
                        with detail_tabs[4]:  # Volume Analysis
                            st.markdown("**📊 Volume Analysis**")
                            
                            # 🏆 ADVANCED VOLUME QUALITY SCORE (VQS) - ULTRA-SIMPLE SCORING
                            st.markdown("---")
                            st.markdown("**🏆 Advanced Volume Quality Score (VQS)**")
                            st.caption("4 Components: VolumeStrength (50%) + SmartMoney (20%) + Consistency (20%) + Efficiency (10%)")
                            
                            # Extract Advanced VQS components
                            if 'vqs_score' in stock.index and pd.notna(stock['vqs_score']):
                                vqs_score = stock['vqs_score']
                                vqs_grade = stock.get('vqs_grade', 'N/A')
                                vqs_status = stock.get('vqs_status', 'N/A')
                                vqs_volume = stock.get('vqs_volume_strength', 50)
                                vqs_smart = stock.get('vqs_smart_money', 50)
                                vqs_consistency = stock.get('vqs_consistency', 50)
                                vqs_eff = stock.get('vqs_efficiency', 50)
                                vqs_quality = stock.get('vqs_data_quality', 1.0)
                                
                                # Determine color based on grade
                                grade_colors = {
                                    'A+': ('success', '👑 Elite'),
                                    'A': ('success', '🌟 Excellent'),
                                    'B': ('success', '✅ Strong'),
                                    'C': ('info', '⚪ Average'),
                                    'D': ('warning', '⚠️ Weak'),
                                    'F': ('error', '❌ Poor')
                                }
                                vqs_color, vqs_status_display = grade_colors.get(vqs_grade, ('info', '⚪ Average'))
                                
                                # Display Advanced VQS
                                vqs_col1, vqs_col2, vqs_col3 = st.columns([1, 2, 2])
                                
                                with vqs_col1:
                                    st.metric("VQS Grade", vqs_grade)
                                    getattr(st, vqs_color)(f"**{vqs_status_display}**")
                                    st.caption(f"Score: {vqs_score:.1f}/100")
                                    st.caption(f"Data Quality: {vqs_quality*100:.0f}%")
                                
                                with vqs_col2:
                                    st.markdown("**🔧 4 Key Components:**")
                                    st.caption(f"🔵 Volume Strength: **{vqs_volume:.0f}/100** (50%)")
                                    st.caption(f"   Liquidity + Activity (size + surge)")
                                    st.caption(f"🟢 Smart Money: **{vqs_smart:.0f}/100** (20%)")
                                    st.caption(f"   WHO is buying (institutions)")
                                    st.caption(f"🟣 Consistency: **{vqs_consistency:.0f}/100** (20%)")
                                    st.caption(f"   Pattern reliability over time")
                                    st.caption(f"🟡 Efficiency: **{vqs_eff:.0f}/100** (10%)")
                                    st.caption(f"   Price movement per volume")
                                    st.caption("")
                                    raw_calc = vqs_volume*0.5 + vqs_smart*0.2 + vqs_consistency*0.2 + vqs_eff*0.1
                                    st.caption(f"💡 Calculation: {raw_calc:.1f} × {vqs_quality:.2f} = {vqs_score:.1f}")
                                
                                with vqs_col3:
                                    st.markdown("**📖 Grading Scale:**")
                                    st.caption("👑 A+ (80+): Elite quality")
                                    st.caption("🌟 A (70-79): Excellent")
                                    st.caption("✅ B (60-69): Strong")
                                    st.caption("⚪ C (45-59): Average")
                                    st.caption("⚠️ D (30-44): Weak")
                                    st.caption("❌ F (<30): Poor")
                                    st.caption("")
                                    
                                    # Component-based interpretation
                                    if vqs_grade in ['A+', 'A']:
                                        st.success("🎯 **Top quality volume!**")
                                        if vqs_smart >= 70:
                                            st.caption("💰 Strong institutional backing")
                                        if vqs_consistency >= 80:
                                            st.caption("📈 Highly reliable pattern")
                                    elif vqs_grade == 'B':
                                        st.info("✅ **Good volume quality**")
                                        if vqs_consistency < 50:
                                            st.caption("⚠️ Watch consistency")
                                    elif vqs_grade == 'C':
                                        st.info("⚪ **Average - be selective**")
                                        if vqs_smart < 40:
                                            st.caption("⚠️ Weak institutional interest")
                                    else:
                                        st.warning("⚠️ **Low quality - caution!**")
                                        if vqs_consistency < 40:
                                            st.caption("❌ Unreliable pattern")
                            else:
                                st.info("🏆 VQS data not available")
                            
                            st.markdown("---")
                            
                            # ⚡ ADVANCED VER (VOLUME EFFICIENCY RATIO) - Multi-Dimensional Algorithm
                            st.markdown("**⚡ Advanced Volume Efficiency Ratio (VER)**")
                            st.caption("Multi-dimensional algorithm using 30+ data columns: Base VER × Quality × Regime × Risk")
                            
                            # Extract Advanced VER components
                            if 'advanced_ver' in stock.index and pd.notna(stock['advanced_ver']):
                                adv_ver = stock['advanced_ver']
                                adv_ver_base = stock.get('advanced_ver_base', 0)
                                adv_ver_quality = stock.get('advanced_ver_quality_mult', 1.0)
                                adv_ver_regime = stock.get('advanced_ver_regime_adj', 1.0)
                                adv_ver_risk = stock.get('advanced_ver_risk_factor', 1.0)
                                adv_ver_score = stock.get('advanced_ver_score', 0)
                                adv_ver_grade = stock.get('advanced_ver_grade', 'N/A')
                                adv_ver_status = stock.get('advanced_ver_status', 'N/A')
                                adv_ver_confidence = stock.get('advanced_ver_confidence', 0)
                                
                                # Determine color based on grade
                                grade_colors = {
                                    'A+': ('success', '🚀 Extreme'),
                                    'A': ('success', '🔥 Elite+'),
                                    'B': ('success', '✨ Strong'),
                                    'C': ('info', '➡️ Normal'),
                                    'D': ('warning', '⚠️ Below Average'),
                                    'F': ('error', '❌ Poor')
                                }
                                adv_ver_color, adv_ver_label = grade_colors.get(adv_ver_grade, ('info', '➡️ Normal'))
                                
                                # Display Advanced VER
                                aver_col1, aver_col2, aver_col3 = st.columns([1, 2, 2])
                                
                                with aver_col1:
                                    st.metric("Advanced VER Grade", adv_ver_grade)
                                    getattr(st, adv_ver_color)(f"**{adv_ver_label}**")
                                    st.caption(f"Score: {adv_ver_score:.1f}/100")
                                    st.caption(f"VER Value: {adv_ver:.2f}")
                                    st.caption(f"Confidence: {adv_ver_confidence:.0f}%")
                                
                                with aver_col2:
                                    st.markdown("**🔧 Component Breakdown:**")
                                    st.caption(f"• Base VER: {adv_ver_base:.2f}")
                                    st.caption(f"  └─ Multi-timeframe (1d/7d/30d)")
                                    st.caption(f"• Quality Multiplier: {adv_ver_quality:.2f}x")
                                    st.caption(f"  └─ VQS + VMI + Harmony")
                                    st.caption(f"• Regime Adjustment: {adv_ver_regime:.2f}x")
                                    st.caption(f"  └─ Market State + Volatility")
                                    st.caption(f"• Risk Factor: {adv_ver_risk:.2f}x")
                                    st.caption(f"  └─ Money Flow + Smart Money")
                                    st.caption("")
                                    st.caption("💡 **Formula:** Base × Quality × Regime × Risk")
                                
                                with aver_col3:
                                    st.markdown("**📖 Grade Scale:**")
                                    st.caption("🚀 A+ (>10.0): Extreme efficiency")
                                    st.caption("🔥 A (>5.0): Elite+ efficiency")
                                    st.caption("✨ B (>2.5): Strong efficiency")
                                    st.caption("➡️ C (>1.5): Normal efficiency")
                                    st.caption("⚠️ D (>0.8): Below average")
                                    st.caption("❌ F (<0.8): Poor efficiency")
                                    st.caption("")
                                    
                                    # Interpretation based on grade
                                    if adv_ver_grade == 'A+':
                                        st.success("🎯 **Institutional-grade efficiency**")
                                        st.caption("Exceptional price movement per unit volume")
                                    elif adv_ver_grade == 'A':
                                        st.success("🎯 **Elite efficiency**")
                                        st.caption("Strong price impact with volume")
                                    elif adv_ver_grade == 'B':
                                        st.info("✅ **Good efficiency**")
                                        st.caption("Above-average volume utilization")
                                    elif adv_ver_grade == 'C':
                                        st.info("⚪ **Normal efficiency**")
                                        st.caption("Typical price-volume relationship")
                                    elif adv_ver_grade == 'D':
                                        st.warning("⚠️ **Low efficiency**")
                                        st.caption("High volume, low price impact")
                                    else:  # F
                                        st.error("❌ **Poor efficiency**")
                                        st.caption("Volume not translating to movement")
                            else:
                                st.info("⚡ Advanced VER data not available for this stock")
                            
                            st.markdown("---")
                            
                            vol_col1, vol_col2 = st.columns(2)
                            
                            with vol_col1:
                                st.markdown("**📈 Current Volume Metrics**")
                                volume_data = {
                                    'Metric': [],
                                    'Value': []
                                }
                                
                                # Current Volume
                                if 'volume_1d' in stock.index and pd.notna(stock['volume_1d']):
                                    vol_val = stock['volume_1d']
                                    if vol_val >= 1_000_000:
                                        vol_str = f"{vol_val/1_000_000:.1f}M"
                                    elif vol_val >= 1_000:
                                        vol_str = f"{vol_val/1_000:.0f}K" 
                                    else:
                                        vol_str = f"{vol_val:.0f}"
                                    volume_data['Metric'].append('Current Volume')
                                    volume_data['Value'].append(vol_str)
                                
                                # Relative Volume (RVOL)
                                if 'rvol' in stock.index and pd.notna(stock['rvol']):
                                    rvol_val = stock['rvol']
                                    volume_data['Metric'].append('Relative Volume')
                                    volume_data['Value'].append(f"{rvol_val:.2f}x")
                                    
                                    # Volume interpretation
                                    if rvol_val >= 10:
                                        vol_status = "🌋 Volcanic Activity"
                                        vol_color = "error"
                                    elif rvol_val >= 5:
                                        vol_status = "💥 Explosive Volume"
                                        vol_color = "error" 
                                    elif rvol_val >= 2:
                                        vol_status = "🔥 High Activity"
                                        vol_color = "warning"
                                    elif rvol_val >= 1.5:
                                        vol_status = "📈 Growing Interest"
                                        vol_color = "success"
                                    elif rvol_val >= 0.5:
                                        vol_status = "➡️ Normal Activity"
                                        vol_color = "info"
                                    else:
                                        vol_status = "😴 Low Activity"
                                        vol_color = "warning"
                                    
                                    getattr(st, vol_color)(f"**Volume Status:** {vol_status}")
                                
                                # VMI (Volume Momentum Index)
                                if 'vmi' in stock.index and pd.notna(stock['vmi']):
                                    vmi_val = stock['vmi']
                                    volume_data['Metric'].append('VMI')
                                    volume_data['Value'].append(f"{vmi_val:.2f}")
                                
                                # Volume Tier
                                if 'volume_tier' in stock.index and pd.notna(stock['volume_tier']):
                                    volume_data['Metric'].append('Volume Tier')
                                    volume_data['Value'].append(stock['volume_tier'])
                                
                                if volume_data['Metric']:
                                    vol_df = pd.DataFrame(volume_data)
                                    st.dataframe(
                                        vol_df,
                                        width='stretch',
                                        hide_index=True
                                    )
                                else:
                                    st.info("No volume data available")
                            
                            with vol_col2:
                                st.markdown("**📊 Volume Ratios & Trends**")
                                
                                ratio_data = {
                                    'Period': [],
                                    'Ratio': [],
                                    'Status': []
                                }
                                
                                # Volume ratios
                                volume_ratios = [
                                    ('vol_ratio_1d_90d', '1D vs 90D'),
                                    ('vol_ratio_7d_90d', '7D vs 90D'),
                                    ('vol_ratio_30d_90d', '30D vs 90D'),
                                    ('vol_ratio_1d_180d', '1D vs 180D'),
                                    ('vol_ratio_7d_180d', '7D vs 180D'),
                                    ('vol_ratio_30d_180d', '30D vs 180D'),
                                    ('vol_ratio_90d_180d', '90D vs 180D')
                                ]
                                
                                for col_name, display_name in volume_ratios:
                                    if col_name in stock.index and pd.notna(stock[col_name]):
                                        ratio_val = stock[col_name]
                                        ratio_data['Period'].append(display_name)
                                        ratio_data['Ratio'].append(f"{ratio_val:.2f}")
                                        
                                        # Status interpretation
                                        if ratio_val >= 2.0:
                                            status = "🔥 Very High"
                                        elif ratio_val >= 1.5:
                                            status = "📈 High"
                                        elif ratio_val >= 1.2:
                                            status = "➕ Above Normal"
                                        elif ratio_val >= 0.8:
                                            status = "➡️ Normal"
                                        elif ratio_val >= 0.5:
                                            status = "➖ Below Normal"
                                        else:
                                            status = "📉 Low"
                                        
                                        ratio_data['Status'].append(status)
                                
                                if ratio_data['Period']:
                                    ratio_df = pd.DataFrame(ratio_data)
                                    st.dataframe(
                                        ratio_df,
                                        width='stretch',
                                        hide_index=True
                                    )
                                else:
                                    st.info("No volume ratio data available")
                            
                            # 🌡️ VOLUME DIVERGENCE HEAT MAP - Visual Pattern Recognition
                            st.markdown("---")
                            st.markdown("**🌡️ Volume Divergence Heat Map**")
                            st.caption("Instant visual detection of accumulation/distribution patterns")
                            
                            # Build heat map data
                            heat_map_data = []
                            periods = ['1D', '7D', '30D']
                            period_to_col_map = {
                                '1D': ['vol_ratio_1d_90d', 'vol_ratio_1d_180d'],
                                '7D': ['vol_ratio_7d_90d', 'vol_ratio_7d_180d'],
                                '30D': ['vol_ratio_30d_90d', 'vol_ratio_30d_180d']
                            }
                            
                            for period_label in periods:
                                row_data = {'Period': period_label}
                                vol_cols = period_to_col_map[period_label]
                                
                                # vs 90d column
                                if vol_cols[0] in stock.index and pd.notna(stock[vol_cols[0]]):
                                    ratio = stock[vol_cols[0]]
                                    
                                    # Color coding with emojis
                                    if ratio >= 2.0:
                                        cell_display = f"🔴 {ratio:.2f}x"
                                        cell_status = "Extreme ↑"
                                    elif ratio >= 1.5:
                                        cell_display = f"🟠 {ratio:.2f}x"
                                        cell_status = "High ↑"
                                    elif ratio >= 1.2:
                                        cell_display = f"🟡 {ratio:.2f}x"
                                        cell_status = "Elevated"
                                    elif ratio >= 0.8:
                                        cell_display = f"🟢 {ratio:.2f}x"
                                        cell_status = "Normal"
                                    elif ratio >= 0.5:
                                        cell_display = f"🔵 {ratio:.2f}x"
                                        cell_status = "Low ↓"
                                    else:
                                        cell_display = f"⚫ {ratio:.2f}x"
                                        cell_status = "Extreme ↓"
                                    
                                    row_data['vs 90d'] = f"{cell_display} ({cell_status})"
                                else:
                                    row_data['vs 90d'] = 'N/A'
                                
                                # vs 180d column
                                if vol_cols[1] in stock.index and pd.notna(stock[vol_cols[1]]):
                                    ratio = stock[vol_cols[1]]
                                    
                                    # Color coding with emojis
                                    if ratio >= 2.0:
                                        cell_display = f"🔴 {ratio:.2f}x"
                                        cell_status = "Extreme ↑"
                                    elif ratio >= 1.5:
                                        cell_display = f"🟠 {ratio:.2f}x"
                                        cell_status = "High ↑"
                                    elif ratio >= 1.2:
                                        cell_display = f"🟡 {ratio:.2f}x"
                                        cell_status = "Elevated"
                                    elif ratio >= 0.8:
                                        cell_display = f"🟢 {ratio:.2f}x"
                                        cell_status = "Normal"
                                    elif ratio >= 0.5:
                                        cell_display = f"🔵 {ratio:.2f}x"
                                        cell_status = "Low ↓"
                                    else:
                                        cell_display = f"⚫ {ratio:.2f}x"
                                        cell_status = "Extreme ↓"
                                    
                                    row_data['vs 180d'] = f"{cell_display} ({cell_status})"
                                else:
                                    row_data['vs 180d'] = 'N/A'
                                
                                heat_map_data.append(row_data)
                            
                            # Display heat map
                            if heat_map_data:
                                heat_df = pd.DataFrame(heat_map_data)
                                st.dataframe(heat_df, hide_index=True, use_container_width=True)
                                
                                # 🎯 Pattern Detection & Interpretation
                                patterns_detected = []
                                
                                # Get key ratios for pattern detection
                                r1d_90d = stock.get('vol_ratio_1d_90d', np.nan)
                                r7d_90d = stock.get('vol_ratio_7d_90d', np.nan)
                                r30d_90d = stock.get('vol_ratio_30d_90d', np.nan)
                                
                                if all(pd.notna(r) for r in [r1d_90d, r7d_90d, r30d_90d]):
                                    # Pattern 1: Explosive (all timeframes high)
                                    if all(r >= 1.5 for r in [r1d_90d, r7d_90d, r30d_90d]):
                                        patterns_detected.append("🔥 **Explosive Pattern**: All timeframes showing elevated volume (strong institutional accumulation)")
                                    
                                    # Pattern 2: Acceleration (recent >> historical)
                                    elif r1d_90d > 2.0 and r7d_90d > 1.5 and r30d_90d < 1.2:
                                        patterns_detected.append("⚡ **Acceleration Pattern**: Recent surge in volume (potential breakout initiation)")
                                    
                                    # Pattern 3: Deceleration (recent << historical)
                                    elif r1d_90d < 0.8 and r7d_90d < 0.8 and r30d_90d > 1.2:
                                        patterns_detected.append("📉 **Deceleration Pattern**: Volume fading after strength (distribution warning)")
                                    
                                    # Pattern 4: Sustained accumulation
                                    elif r1d_90d > 1.5 and r30d_90d > 1.5:
                                        patterns_detected.append("🌊 **Sustained Pattern**: Multi-period volume strength (quality institutional accumulation)")
                                    
                                    # Pattern 5: Normal/Neutral
                                    elif all(0.8 <= r <= 1.2 for r in [r1d_90d, r7d_90d, r30d_90d]):
                                        patterns_detected.append("⚪ **Normal Pattern**: Balanced volume across timeframes (no unusual activity)")
                                    
                                    # Pattern 6: Mixed signals
                                    else:
                                        patterns_detected.append("🔀 **Mixed Pattern**: Divergent volume signals across timeframes (monitor for clarification)")
                                
                                # Display detected patterns
                                if patterns_detected:
                                    st.markdown("**📊 Pattern Analysis:**")
                                    for pattern in patterns_detected:
                                        st.info(pattern)
                            else:
                                st.info("Insufficient data for heat map visualization")
                            
                            # Volume Score Section
                            if 'volume_score' in stock.index and pd.notna(stock['volume_score']):
                                st.markdown("---")
                                st.markdown("**🎯 Volume Score Analysis**")
                                
                                vol_score = stock['volume_score']
                                score_col1, score_col2, score_col3 = st.columns(3)
                                
                                with score_col1:
                                    if vol_score >= 80:
                                        score_status = "🔥 Excellent"
                                        score_color = "success"
                                    elif vol_score >= 60:
                                        score_status = "✅ Good"
                                        score_color = "success"
                                    elif vol_score >= 40:
                                        score_status = "⚠️ Average"
                                        score_color = "warning"
                                    else:
                                        score_status = "❌ Poor"
                                        score_color = "error"
                                    
                                    st.metric("Volume Score", f"{vol_score:.0f}/100")
                                    getattr(st, score_color)(f"**Status:** {score_status}")
                                
                                with score_col2:
                                    # VMI Tier Classification
                                    if 'vmi_tier' in stock.index and pd.notna(stock['vmi_tier']):
                                        st.markdown("**VMI Classification**")
                                        st.info(f"📊 {stock['vmi_tier']}")
                                
                                with score_col3:
                                    # Volume Activity Level
                                    if 'rvol' in stock.index and pd.notna(stock['rvol']):
                                        rvol_val = stock['rvol']
                                        st.markdown("**Activity Level**")
                                        
                                        # Determine activity level based on RVOL
                                        if rvol_val >= 2.0:
                                            activity = "🔥 High Activity"
                                        elif rvol_val >= 1.0:
                                            activity = "📈 Normal+"
                                        elif rvol_val >= 0.5:
                                            activity = "➡️ Normal"
                                        else:
                                            activity = "😴 Low"
                                        
                                        st.info(f"📊 {activity}")
                            
                            # ⚡ VOLUME EFFICIENCY RATIO (VER) - Performance Measurement
                            st.markdown("---")
                            st.markdown("**⚡ Volume Efficiency Ratio (VER)**")
                            st.caption("How effectively is volume moving price? Higher = More Efficient")
                            
                            # Calculate VER for multiple timeframes
                            ver_data = []
                            ver_scores = []
                            
                            # FIXED: Use vol_ratio_1d_90d instead of rvol for consistency
                            # All timeframes now use vol_ratio columns (percentage → ratio conversion)
                            # Ensures consistent data processing pipeline across all periods
                            timeframes = [
                                ('Daily', 'ret_1d', 'vol_ratio_1d_90d', 1),
                                ('Weekly', 'ret_7d', 'vol_ratio_7d_90d', 7),
                                ('Monthly', 'ret_30d', 'vol_ratio_30d_90d', 30)
                            ]
                            
                            for label, ret_col, vol_col, days in timeframes:
                                if all(col in stock.index for col in [ret_col, vol_col]):
                                    ret_val = stock[ret_col]
                                    vol_val = stock[vol_col]
                                    
                                    if pd.notna(ret_val) and pd.notna(vol_val) and vol_val > 0:
                                        # Calculate VER
                                        ver = abs(ret_val) / vol_val
                                        
                                        # Interpretation
                                        if ver > 5.0:
                                            efficiency = "🚀 Extreme"
                                            interpretation = "Supply shock - potential parabolic move"
                                            ver_score = 100
                                        elif ver > 2.0:
                                            efficiency = "🔥 High"
                                            interpretation = "Strong demand, tight supply"
                                            ver_score = 85
                                        elif ver >= 1.0:
                                            efficiency = "✅ Good"
                                            interpretation = "Healthy price-volume balance"
                                            ver_score = 70
                                        elif ver >= 0.5:
                                            efficiency = "⚪ Normal"
                                            interpretation = "Balanced efficiency"
                                            ver_score = 50
                                        elif ver >= 0.2:
                                            efficiency = "⚠️ Low"
                                            interpretation = "Volume not translating to price"
                                            ver_score = 30
                                        else:
                                            efficiency = "🚨 Very Low"
                                            interpretation = "Manipulation risk or distribution"
                                            ver_score = 10
                                        
                                        # Special case: Distribution pattern
                                        if ret_val < 0 and vol_val > 1.5:
                                            interpretation = "📉 Distribution signal (price down + volume up)"
                                            efficiency = "⚠️ Bearish"
                                        
                                        ver_data.append({
                                            'Timeframe': label,
                                            'VER': f"{ver:.2f}",
                                            'Efficiency': efficiency,
                                            'Signal': interpretation
                                        })
                                        
                                        ver_scores.append(ver)
                            
                            # Display VER table
                            if ver_data:
                                ver_df = pd.DataFrame(ver_data)
                                st.dataframe(ver_df, hide_index=True, use_container_width=True)
                                
                                # Overall VER assessment
                                if ver_scores:
                                    avg_ver = np.mean(ver_scores)
                                    
                                    if avg_ver > 2.0:
                                        st.success("💎 **Overall**: High volume efficiency - institutional quality demand driving price")
                                    elif avg_ver > 1.0:
                                        st.info("✅ **Overall**: Good efficiency - healthy price-volume dynamics")
                                    elif avg_ver > 0.5:
                                        st.info("⚪ **Overall**: Moderate efficiency - balanced volume-price relationship")
                                    elif avg_ver > 0.2:
                                        st.warning("⚠️ **Overall**: Below average efficiency - monitor for volume effectiveness")
                                    else:
                                        st.error("🚨 **Overall**: Low efficiency - volume not moving price effectively (potential manipulation/distribution)")
                            else:
                                st.info("Insufficient data to calculate Volume Efficiency Ratio")
                            
                            # Enhanced Liquidity Analysis
                            if all(col in stock.index for col in ['volume_1d', 'price']) and all(pd.notna(stock[col]) for col in ['volume_1d', 'price']):
                                st.markdown("---")
                                st.markdown("**💧 Liquidity Analysis**")
                                
                                volume = stock['volume_1d']
                                price = stock['price']
                                daily_turnover = volume * price
                                
                                # Enhanced turnover calculations
                                turnover_metrics = []
                                
                                # Daily Turnover (current volume × current price)
                                turnover_metrics.append({
                                    'label': 'Daily Turnover',
                                    'value': daily_turnover,
                                    'basis': 'Current Volume × Current Price'
                                })
                                
                                # 30-Day Avg Daily Turnover (30-day avg volume × current price)
                                # Using current price for all periods ensures consistency and accuracy
                                if 'volume_30d' in stock.index and pd.notna(stock['volume_30d']):
                                    turnover_30d = stock['volume_30d'] * price
                                    turnover_metrics.append({
                                        'label': 'Avg Daily (30d)',
                                        'value': turnover_30d,
                                        'basis': 'Avg Daily Vol (30d) × Current Price'
                                    })
                                
                                # 90-Day Avg Daily Turnover (90-day avg volume × current price)
                                if 'volume_90d' in stock.index and pd.notna(stock['volume_90d']):
                                    turnover_90d = stock['volume_90d'] * price
                                    turnover_metrics.append({
                                        'label': 'Avg Daily (90d)', 
                                        'value': turnover_90d,
                                        'basis': 'Avg Daily Vol (90d) × Current Price'
                                    })
                                
                                # 180-Day Avg Daily Turnover (180-day avg volume × current price)
                                if 'volume_180d' in stock.index and pd.notna(stock['volume_180d']):
                                    turnover_180d = stock['volume_180d'] * price
                                    turnover_metrics.append({
                                        'label': 'Avg Daily (180d)',
                                        'value': turnover_180d,
                                        'basis': 'Avg Daily Vol (180d) × Current Price'
                                    })
                                
                                # Display turnover metrics in a structured layout
                                if len(turnover_metrics) >= 2:
                                    # Create columns based on number of metrics
                                    num_cols = min(len(turnover_metrics), 4)
                                    cols = st.columns(num_cols)
                                    
                                    for i, metric in enumerate(turnover_metrics):
                                        with cols[i % num_cols]:
                                            value = metric['value']
                                            formatted_value = f"₹{value/10_000_000:.1f}Cr" if value >= 10_000_000 else f"₹{value/100_000:.1f}L"
                                            st.metric(
                                                label=metric['label'],
                                                value=formatted_value,
                                                help=f"Based on {metric['basis']}"
                                            )
                                else:
                                    # Fallback to original single column display
                                    st.metric("Daily Turnover", f"₹{daily_turnover/10_000_000:.1f}Cr" if daily_turnover >= 10_000_000 else f"₹{daily_turnover/100_000:.1f}L")
                                
                                # Professional note about calculation methodology
                                if len(turnover_metrics) > 1:
                                    st.caption("💡 **Methodology:** Historical turnover calculated as avg daily volume × current price (consistent, transparent)")
                                
                                # Overall liquidity classification based on daily turnover
                                st.markdown("---")
                                liq_col1, liq_col2 = st.columns([1, 2])
                                
                                with liq_col1:
                                    # Liquidity status
                                    if daily_turnover >= 100_000_000:  # 10Cr+
                                        liq_status = "🌊 Highly Liquid"
                                        liq_color = "success"
                                    elif daily_turnover >= 10_000_000:  # 1Cr+
                                        liq_status = "💧 Good Liquidity"
                                        liq_color = "success"
                                    elif daily_turnover >= 1_000_000:  # 10L+
                                        liq_status = "💦 Moderate Liquidity"
                                        liq_color = "warning"
                                    else:
                                        liq_status = "🏜️ Low Liquidity"
                                        liq_color = "error"
                                    
                                    getattr(st, liq_color)(f"**Status:** {liq_status}")
                                
                                with liq_col2:
                                    # Growth analysis (if multiple periods available)
                                    if len(turnover_metrics) >= 3:
                                        growth_info = "📈 **Growth Analysis**\n"
                                        daily_val = turnover_metrics[0]['value']
                                        
                                        for metric in turnover_metrics[1:]:
                                            period_val = metric['value']
                                            growth_pct = ((daily_val - period_val) / period_val * 100) if period_val > 0 else 0
                                            
                                            if growth_pct > 10:
                                                growth_status = f"🚀 +{growth_pct:.0f}% growth"
                                            elif growth_pct > 0:
                                                growth_status = f"📈 +{growth_pct:.1f}% growth"
                                            elif growth_pct > -10:
                                                growth_status = f"📉 {growth_pct:.1f}% decline"
                                            else:
                                                growth_status = f"🔻 {growth_pct:.0f}% decline"
                                            
                                            growth_info += f"vs {metric['label']}: {growth_status}\n"
                                        
                                        st.info(growth_info)
                                
                                # � SMART MONEY VOLUME SIGNATURE - Institutional Footprint Detection
                                st.markdown("---")
                                st.markdown("**🎯 Smart Money Volume Signature**")
                                st.caption("Detecting institutional accumulation patterns and smart money activity")
                                
                                # Initialize signature score and components
                                signature_score = 0
                                signature_components = []
                                signature_details = []
                                
                                # COMPONENT 1: Accumulation Pattern Detection (40 points max)
                                # Detect "volume at lows" pattern
                                if all(col in stock.index for col in ['rvol', 'from_low_pct']):
                                    rvol_val = stock['rvol']
                                    from_low_val = stock['from_low_pct']
                                    
                                    if pd.notna(rvol_val) and pd.notna(from_low_val):
                                        if rvol_val > 1.5 and from_low_val < 25:
                                            signature_score += 40
                                            signature_components.append("✅ Volume at lows (strong accumulation)")
                                            signature_details.append(f"High volume ({rvol_val:.1f}x) near 52w low ({from_low_val:.0f}%)")
                                        elif rvol_val > 1.5 and from_low_val < 50:
                                            signature_score += 20
                                            signature_components.append("✅ Elevated volume in lower range")
                                            signature_details.append(f"Elevated volume in lower half of range")
                                        elif rvol_val > 2.0 and 'from_high_pct' in stock.index and pd.notna(stock['from_high_pct']):
                                            from_high_val = stock['from_high_pct']
                                            if from_high_val < 10:
                                                signature_score -= 20  # Penalty for volume at highs
                                                signature_components.append("⚠️ High volume near 52w high (distribution risk)")
                                                signature_details.append(f"Volume spike ({rvol_val:.1f}x) near highs (risk)")
                                
                                # COMPONENT 2: Volume Steadiness (30 points max)
                                # Detect steady vs spiky patterns (institutional vs retail)
                                if all(col in stock.index for col in ['volume_1d', 'volume_7d', 'volume_30d']):
                                    vol_1d = stock['volume_1d']
                                    vol_7d = stock['volume_7d']
                                    vol_30d = stock['volume_30d']
                                    
                                    if all(pd.notna(v) and v > 0 for v in [vol_1d, vol_7d, vol_30d]):
                                        volumes = np.array([vol_1d, vol_7d, vol_30d])
                                        mean_vol = np.mean(volumes)
                                        std_vol = np.std(volumes)
                                        cv = std_vol / mean_vol if mean_vol > 0 else 1.0  # Coefficient of Variation
                                        
                                        if cv < 0.3:
                                            signature_score += 30
                                            signature_components.append("✅ Steady volume pattern (institutional)")
                                            signature_details.append(f"Low volatility (CV: {cv:.2f}) - steady accumulation")
                                        elif cv < 0.6:
                                            signature_score += 15
                                            signature_components.append("⚪ Moderate volume stability")
                                            signature_details.append(f"Moderate volatility (CV: {cv:.2f})")
                                        else:
                                            signature_score += 5
                                            signature_components.append("⚠️ Volatile volume pattern (retail/speculation)")
                                            signature_details.append(f"High volatility (CV: {cv:.2f}) - spiky pattern")
                                
                                # COMPONENT 3: Volume-Price Absorption (30 points max)
                                # High volume with narrow price range = absorption
                                if all(col in stock.index for col in ['rvol', 'ret_1d']):
                                    rvol_val = stock['rvol']
                                    ret_1d_val = stock['ret_1d']
                                    
                                    if pd.notna(rvol_val) and pd.notna(ret_1d_val):
                                        if rvol_val > 2.0 and abs(ret_1d_val) < 2.0:
                                            signature_score += 30
                                            signature_components.append("✅ High volume + narrow range (absorption)")
                                            signature_details.append(f"Absorption: {rvol_val:.1f}x volume, {abs(ret_1d_val):.1f}% price move")
                                        elif rvol_val > 1.5 and abs(ret_1d_val) < 5.0:
                                            signature_score += 15
                                            signature_components.append("⚪ Elevated volume with moderate range")
                                            signature_details.append(f"Moderate absorption pattern")
                                        elif rvol_val > 2.0 and abs(ret_1d_val) > 10.0:
                                            signature_score += 5
                                            signature_components.append("⚠️ High volume + extreme range (volatile)")
                                            signature_details.append(f"Volatile: {rvol_val:.1f}x volume, {abs(ret_1d_val):.1f}% move")
                                
                                # Cap score at 0-100 range
                                signature_score = max(0, min(100, signature_score))
                                
                                # Display signature score and interpretation
                                sig_col1, sig_col2 = st.columns([1, 2])
                                
                                with sig_col1:
                                    # Score classification
                                    if signature_score >= 70:
                                        signature_status = "🎯 Strong Institutional"
                                        signature_color = "success"
                                        signature_desc = "High probability of institutional accumulation"
                                    elif signature_score >= 50:
                                        signature_status = "✅ Likely Smart Money"
                                        signature_color = "success"
                                        signature_desc = "Signs of professional buying activity"
                                    elif signature_score >= 30:
                                        signature_status = "⚪ Mixed Activity"
                                        signature_color = "warning"
                                        signature_desc = "Both retail and institutional participation"
                                    else:
                                        signature_status = "⚠️ Retail Dominated"
                                        signature_color = "warning"
                                        signature_desc = "Predominantly retail activity or manipulation risk"
                                    
                                    st.metric("Signature Score", f"{signature_score}/100")
                                    getattr(st, signature_color)(f"**{signature_status}**")
                                    st.caption(signature_desc)
                                
                                with sig_col2:
                                    if signature_components:
                                        st.markdown("**📋 Detected Patterns:**")
                                        for component in signature_components:
                                            st.markdown(f"- {component}")
                                        
                                        # Show detailed breakdown in expander
                                        if signature_details:
                                            with st.expander("🔍 Detailed Analysis"):
                                                for detail in signature_details:
                                                    st.caption(f"• {detail}")
                                    else:
                                        st.info("Insufficient data to detect smart money patterns")
                                
                                # �🌊 LIQUIDITY GROWTH ANALYTICS - Advanced Analysis
                                if all(col in stock.index for col in ['composite_growth_score', 'growth_quality_tier', 'smart_money_flow']):
                                    st.markdown("---")
                                    st.markdown("**🌊 Liquidity Growth Analytics**")
                                    
                                    # Status Banner - Overall Growth Health (V9.py pattern: color-coded status)
                                    composite_score = stock.get('composite_growth_score', 0)
                                    growth_trend = stock.get('growth_trend', 'Unknown')
                                    smart_money = stock.get('smart_money_flow', 'Unknown')
                                    
                                    # Determine overall growth health status (V9.py pattern)
                                    if composite_score >= 80:
                                        growth_status = "💎 Elite Growth Quality"
                                        status_color = "success"
                                        status_msg = f"**{growth_status}** - {growth_trend} | {smart_money}"
                                    elif composite_score >= 65:
                                        growth_status = "🌟 Excellent Growth"
                                        status_color = "success"
                                        status_msg = f"**{growth_status}** - {growth_trend} | {smart_money}"
                                    elif composite_score >= 50:
                                        growth_status = "✅ Good Growth"
                                        status_color = "info"
                                        status_msg = f"**{growth_status}** - {growth_trend} | {smart_money}"
                                    elif composite_score >= 35:
                                        growth_status = "⚪ Average Growth"
                                        status_color = "warning"
                                        status_msg = f"**{growth_status}** - {growth_trend} | {smart_money}"
                                    else:
                                        growth_status = "⚠️ Weak Growth"
                                        status_color = "error"
                                        status_msg = f"**{growth_status}** - {growth_trend} | {smart_money}"
                                    
                                    # Display status banner (V9.py pattern: getattr for dynamic color)
                                    getattr(st, status_color)(status_msg)
                                    
                                    # Key Metrics Grid (V9.py pattern: 4-column compact layout)
                                    st.markdown("---")
                                    metric_cols = st.columns(4)
                                    
                                    with metric_cols[0]:
                                        # 🎯 Enhanced: Show percentile tier as delta
                                        percentile_tier = stock.get('composite_percentile_tier', '')
                                        risk_adjusted = stock.get('risk_adjusted_composite', composite_score)
                                        
                                        st.metric(
                                            label="Composite Score",
                                            value=f"{composite_score:.1f}/100",
                                            delta=f"{percentile_tier}" if percentile_tier else None,
                                            help=f"Overall liquidity growth health\n" +
                                                 f"Market Position: {percentile_tier}\n" +
                                                 f"Risk-Adjusted: {risk_adjusted:.1f}/100"
                                        )
                                    
                                    with metric_cols[1]:
                                        momentum = stock.get('growth_momentum', 0)
                                        momentum_delta = "↗️" if momentum > 20 else "→" if momentum > 0 else "↘️"
                                        momentum_pct = stock.get('momentum_percentile', 0)
                                        
                                        st.metric(
                                            label="Momentum",
                                            value=f"{momentum:+.1f}%",
                                            delta=f"{momentum_delta}",
                                            help=f"Growth acceleration rate\n" +
                                                 f"Percentile: {momentum_pct:.0f}%" if momentum_pct > 0 else "Growth acceleration rate"
                                        )
                                    
                                    with metric_cols[2]:
                                        consistency = stock.get('growth_consistency', 0)
                                        st.metric(
                                            label="Consistency",
                                            value=f"{consistency:.0f}/100",
                                            help="Pattern reliability"
                                        )
                                    
                                    with metric_cols[3]:
                                        alignment = stock.get('growth_alignment', 0)
                                        st.metric(
                                            label="Alignment",
                                            value=f"{alignment:.0f}/100",
                                            help="Period synchronization"
                                        )
                                    
                                    # Growth Period Analysis (V9.py pattern: 2-column status + analysis)
                                    st.markdown("---")
                                    period_col1, period_col2 = st.columns([1, 2])
                                    
                                    with period_col1:
                                        st.markdown("**📈 Momentum Status**")
                                        growth_30_90 = stock.get('growth_30_to_90', 0)
                                        growth_90_180 = stock.get('growth_90_to_180', 0)
                                        
                                        # Momentum interpretation (Recent vs Historical)
                                        # Positive = Recent activity higher than historical (ACCELERATING)
                                        if growth_30_90 > 200 or (growth_30_90 > 100 and growth_90_180 > 100):
                                            st.success("🌊💎 **Elite Momentum** - Explosive acceleration")
                                        elif growth_30_90 > 100 or (growth_30_90 > 50 and growth_90_180 > 50):
                                            st.success("🚀 **Explosive** - Massive volume surge")
                                        elif growth_30_90 > 50 or (growth_30_90 > 25 and growth_90_180 > 25):
                                            st.success("📈 **Strong Momentum** - High acceleration")
                                        elif growth_30_90 > 25 or (growth_30_90 > 10 and growth_90_180 > 10):
                                            st.info("✅ **Steady Growth** - Consistent acceleration")
                                        elif momentum > 50:
                                            st.info("⚡ **Rapidly Accelerating** - Recent surge")
                                        elif momentum > 20:
                                            st.info("⚡ **Accelerating** - Building momentum")
                                        elif growth_30_90 > 0 and growth_90_180 > 0:
                                            st.info("➡️ **Stable Growth** - Mild acceleration")
                                        elif growth_30_90 < -50 or growth_90_180 < -50:
                                            st.warning("📉 **Strong Decline** - Severe deceleration")
                                        elif growth_30_90 < -25 or growth_90_180 < -25:
                                            st.warning("📉 **Declining** - Activity slowing")
                                        elif growth_30_90 < 0 or growth_90_180 < 0:
                                            st.warning("⚠️ **Weak** - Below historical average")
                                        else:
                                            st.warning("⚪ **Mixed** - Divergent signals")
                                    
                                    with period_col2:
                                        st.markdown("**📊 Momentum Breakdown**")
                                        
                                        # Momentum display (Recent vs Historical comparison)
                                        # Positive = Recent > Historical (GOOD)
                                        growth_daily_30 = stock.get('growth_daily_to_30', 0)
                                        growth_info = f"""
                                        - **Daily vs 30d:** {growth_daily_30:+.1f}% {'🚀' if growth_daily_30 > 50 else '📈' if growth_daily_30 > 0 else '📉'}
                                        - **30d vs 90d:** {growth_30_90:+.1f}% {'🚀' if growth_30_90 > 50 else '📈' if growth_30_90 > 0 else '📉'}
                                        - **90d vs 180d:** {growth_90_180:+.1f}% {'🚀' if growth_90_180 > 50 else '📈' if growth_90_180 > 0 else '📉'}
                                        - **Acceleration:** {momentum:+.1f}% {'⚡ Rapidly Accelerating' if momentum > 50 else '⚡ Accelerating' if momentum > 20 else '➡️ Stable' if momentum > 0 else '🔻 Decelerating'}
                                        - **Score:** {stock.get('growth_acceleration', 0):.0f}/100 {'🌊💎' if stock.get('growth_acceleration', 0) >= 90 else '🚀' if stock.get('growth_acceleration', 0) >= 75 else '✅' if stock.get('growth_acceleration', 0) >= 60 else '➡️' if stock.get('growth_acceleration', 0) >= 50 else '⚪'}
                                        """
                                        st.markdown(growth_info)
                                        
                                        # Add interpretation helper
                                        if growth_30_90 > 100:
                                            st.info("💡 **30-day activity is 2x+ higher than 90-day average**")
                                        elif growth_30_90 > 50:
                                            st.info("💡 **Recent activity significantly exceeds historical levels**")
                                    
                                    # Smart Money Detection (V9.py pattern: prominent alerts)
                                    if "Elite Institutional" in smart_money:
                                        st.success("🌊💎 **ELITE INSTITUTIONAL ACCUMULATION** - Exceptional smart money flow with explosive momentum")
                                    elif "Strong Accumulation" in smart_money:
                                        st.success("🎯 **Strong Institutional Accumulation Detected** - Clear signs of smart money buying pressure")
                                    elif "Moderate Accumulation" in smart_money:
                                        st.info("✅ **Moderate Institutional Interest** - Positive accumulation signals")
                                    elif "Mild Interest" in smart_money:
                                        st.info("📊 **Early Accumulation Phase** - Initial institutional interest")
                                    elif "Strong Distribution" in smart_money or "Distribution" in smart_money:
                                        st.error("❌ **Distribution Pattern Detected** - Institutional selling pressure")
                                    elif "Weak Distribution" in smart_money:
                                        st.warning("⚠️ **Weak Distribution** - Declining institutional interest")
                                    
                                    # Expandable Detailed Analytics (V9.py pattern: hide complexity)
                                    with st.expander("🔬 Advanced Analytics & Interpretation", expanded=False):
                                        # Quality & Consistency Matrix
                                        st.markdown("**📊 Quality Metrics**")
                                        quality_cols = st.columns(3)
                                        
                                        with quality_cols[0]:
                                            st.metric("Quality Tier", stock.get('growth_quality_tier', 'Unknown'))
                                        
                                        with quality_cols[1]:
                                            st.metric("Consistency", f"{consistency:.0f}/100")
                                        
                                        with quality_cols[2]:
                                            st.metric("Alignment", f"{alignment:.0f}/100")
                                        
                                        st.markdown("---")
                                        
                                        # Professional Interpretation (V9.py pattern: educational content)
                                        st.markdown("**💡 Professional Interpretation**")
                                        
                                        interpretation_data = {
                                            'Indicator': [],
                                            'Value': [],
                                            'Signal': []
                                        }
                                        
                                        # Composite Score
                                        interpretation_data['Indicator'].append('Composite Score')
                                        interpretation_data['Value'].append(f"{composite_score:.1f}/100")
                                        if composite_score >= 80:
                                            interpretation_data['Signal'].append('💎 Elite - Exceptional liquidity growth')
                                        elif composite_score >= 65:
                                            interpretation_data['Signal'].append('🌟 Excellent - Strong institutional interest')
                                        elif composite_score >= 50:
                                            interpretation_data['Signal'].append('✅ Good - Healthy growth pattern')
                                        elif composite_score >= 35:
                                            interpretation_data['Signal'].append('⚪ Average - Mixed signals')
                                        else:
                                            interpretation_data['Signal'].append('⚠️ Weak - Caution advised')
                                        
                                        # Momentum
                                        interpretation_data['Indicator'].append('Growth Momentum')
                                        interpretation_data['Value'].append(f"{momentum:+.1f}%")
                                        if momentum > 50:
                                            interpretation_data['Signal'].append('🚀 Explosive - Rapid acceleration')
                                        elif momentum > 20:
                                            interpretation_data['Signal'].append('⚡ Strong - Clear acceleration')
                                        elif momentum > 0:
                                            interpretation_data['Signal'].append('➡️ Positive - Mild acceleration')
                                        elif momentum > -20:
                                            interpretation_data['Signal'].append('📉 Mild - Slight deceleration')
                                        else:
                                            interpretation_data['Signal'].append('🔻 Negative - Strong deceleration')
                                        
                                        # Consistency
                                        interpretation_data['Indicator'].append('Consistency')
                                        interpretation_data['Value'].append(f"{consistency:.0f}/100")
                                        if consistency >= 70:
                                            interpretation_data['Signal'].append('🎯 High - Very reliable pattern')
                                        elif consistency >= 50:
                                            interpretation_data['Signal'].append('✅ Good - Dependable growth')
                                        elif consistency >= 30:
                                            interpretation_data['Signal'].append('⚪ Fair - Some variance')
                                        else:
                                            interpretation_data['Signal'].append('⚠️ Low - Erratic pattern')
                                        
                                        # Alignment
                                        interpretation_data['Indicator'].append('Alignment')
                                        interpretation_data['Value'].append(f"{alignment:.0f}/100")
                                        if alignment >= 80:
                                            interpretation_data['Signal'].append('🎪 Perfect - All periods synchronized')
                                        elif alignment >= 60:
                                            interpretation_data['Signal'].append('✅ Good - Aligned growth')
                                        else:
                                            interpretation_data['Signal'].append('⚠️ Divergent - Mixed timeframes')
                                        
                                        # 🎯 Risk-Adjusted Score (Professional Addition)
                                        if 'risk_adjusted_composite' in stock.index and pd.notna(stock['risk_adjusted_composite']):
                                            risk_adjusted = stock['risk_adjusted_composite']
                                            risk_adjustment = composite_score - risk_adjusted
                                            
                                            interpretation_data['Indicator'].append('Risk-Adjusted Score')
                                            interpretation_data['Value'].append(f"{risk_adjusted:.1f}/100")
                                            
                                            if risk_adjustment > 10:
                                                interpretation_data['Signal'].append(f'⚠️ High Risk - Penalty: -{risk_adjustment:.1f} pts (Unstable growth)')
                                            elif risk_adjustment > 5:
                                                interpretation_data['Signal'].append(f'⚪ Moderate Risk - Penalty: -{risk_adjustment:.1f} pts')
                                            elif risk_adjustment > 0:
                                                interpretation_data['Signal'].append(f'✅ Low Risk - Penalty: -{risk_adjustment:.1f} pts')
                                            else:
                                                interpretation_data['Signal'].append('💎 No Risk - Stable sustained growth')
                                        
                                        # 🎯 Percentile Ranking (Market Context)
                                        if 'composite_growth_percentile' in stock.index and pd.notna(stock['composite_growth_percentile']):
                                            percentile = stock['composite_growth_percentile']
                                            percentile_tier = stock.get('composite_percentile_tier', '')
                                            
                                            interpretation_data['Indicator'].append('Market Position')
                                            interpretation_data['Value'].append(f"{percentile:.0f}th percentile")
                                            interpretation_data['Signal'].append(f'{percentile_tier} - Relative to all stocks')
                                        
                                        interp_df = pd.DataFrame(interpretation_data)
                                        st.dataframe(interp_df, hide_index=True, width="stretch")
                                        
                                        st.markdown("---")
                                        st.markdown("**🎓 Key Insights**")
                                        
                                        # Dynamic insights based on actual metrics
                                        insights = []
                                        
                                        # Composite + Accumulation insight
                                        if composite_score >= 80 and "Accumulation" in smart_money:
                                            insights.append("💎 **Elite Quality + Institutional Buying** = Top-tier opportunity, high confidence")
                                        elif composite_score >= 65 and "Accumulation" in smart_money:
                                            insights.append("🌟 **Strong Quality + Smart Money** = Excellent growth with institutional support")
                                        
                                        # Momentum + Consistency insight
                                        if momentum > 20 and consistency >= 70:
                                            insights.append("🚀 **Accelerating + High Consistency** = Sustainable explosive growth (rare combination)")
                                        elif momentum > 0 and consistency >= 50:
                                            insights.append("✅ **Positive Momentum + Good Consistency** = Reliable growth trend")
                                        
                                        # Alignment insight
                                        if alignment >= 80:
                                            insights.append("🎪 **Perfect Alignment** = All timeframes synchronized (strong trend confirmation)")
                                        elif alignment >= 60:
                                            insights.append("✅ **Good Alignment** = Multi-period growth confirmed")
                                        
                                        # Risk-adjusted insight
                                        if 'risk_adjusted_composite' in stock.index:
                                            risk_adjusted = stock['risk_adjusted_composite']
                                            risk_penalty = composite_score - risk_adjusted
                                            
                                            if risk_penalty > 10:
                                                insights.append(f"⚠️ **High Risk Penalty (-{risk_penalty:.1f} pts)** = Unstable explosive growth (caution: recent >> historical)")
                                            elif risk_penalty < 2 and composite_score >= 70:
                                                insights.append(f"💎 **Low Risk Premium** = Stable sustained growth (reward: institutional-grade pattern)")
                                        
                                        # Percentile insight
                                        if 'composite_growth_percentile' in stock.index:
                                            percentile = stock['composite_growth_percentile']
                                            if percentile >= 95:
                                                insights.append(f"🌊💎 **Top 5% Market Position** = Elite tier (outperforms 95% of stocks)")
                                            elif percentile >= 90:
                                                insights.append(f"🚀 **Top 10% Market Position** = Excellent relative performance")
                                            elif percentile >= 75:
                                                insights.append(f"✅ **Top 25% Market Position** = Above-average quality")
                                        
                                        # Growth rate context
                                        g1 = stock.get('growth_30_to_90', 0)
                                        g2 = stock.get('growth_90_to_180', 0)
                                        if g1 > 100 and g2 > 50:
                                            insights.append("🌊 **Multi-Period Explosive Growth** = Recent 30d is 2x+ higher, 90d also strong (sustained institutional interest)")
                                        elif g1 > 50 and g2 > 25:
                                            insights.append("📈 **Strong Multi-Period Growth** = Both recent and historical periods accelerating")
                                        
                                        # Display insights
                                        if insights:
                                            for insight in insights:
                                                st.markdown(f"- {insight}")
                                        else:
                                            # Default insights if no specific patterns detected
                                            st.markdown("""
                                            - **High Composite + Strong Accumulation** = Institutional confidence, potential breakout
                                            - **Positive Momentum + High Consistency** = Sustainable growth trend
                                            - **High Alignment** = All timeframes confirm trend strength
                                            - **Accelerating (momentum > 20%)** = Growth rate increasing, early institutional phase
                                            - **Elite Quality (80+)** = Top-tier liquidity expansion, liquid bluechip behavior
                                            """)
                        
                        with detail_tabs[5]:  # Advanced Metrics
                            st.markdown("**🎯 Advanced Metrics**")
                            
                            adv_data = {
                                'Metric': [],
                                'Value': [],
                                'Description': []
                            }
                            
                            # VMI
                            if 'vmi' in stock.index and pd.notna(stock['vmi']):
                                adv_data['Metric'].append('VMI')
                                adv_data['Value'].append(f"{stock['vmi']:.2f}")
                                adv_data['Description'].append('Volume Momentum Index')
                            
                            # Position Tension
                            if 'position_tension' in stock.index and pd.notna(stock['position_tension']):
                                adv_data['Metric'].append('Position Tension')
                                adv_data['Value'].append(f"{stock['position_tension']:.0f}")
                                adv_data['Description'].append('Range position stress')
                            
                            # Momentum Harmony
                            if 'momentum_harmony' in stock.index and pd.notna(stock['momentum_harmony']):
                                harmony_val = int(stock['momentum_harmony'])
                                harmony_emoji = "🟢" if harmony_val >= 3 else "🟡" if harmony_val >= 2 else "🔴"
                                adv_data['Metric'].append('Momentum Harmony')
                                adv_data['Value'].append(f"{harmony_emoji} {harmony_val}/4")
                                adv_data['Description'].append('Multi-timeframe alignment')
                            
                            # Money Flow
                            if 'money_flow_mm' in stock.index and pd.notna(stock['money_flow_mm']):
                                adv_data['Metric'].append('Money Flow')
                                adv_data['Value'].append(f"₹{stock['money_flow_mm']:.1f}M")
                                adv_data['Description'].append('Price × Volume × RVOL')

                            
                            # Overall Market Strength
                            if 'overall_market_strength' in stock.index and pd.notna(stock['overall_market_strength']):
                                adv_data['Metric'].append('Market Strength')
                                adv_data['Value'].append(f"{stock['overall_market_strength']:.1f}")
                                adv_data['Description'].append('Combined momentum, acceleration & breakout strength')

                            # Long Term Strength
                            if 'long_term_strength' in stock.index and pd.notna(stock['long_term_strength']):
                                adv_data['Metric'].append('Long Term Strength')
                                adv_data['Value'].append(f"{stock['long_term_strength']:.1f}")
                                adv_data['Description'].append('Long-term trend consistency & momentum harmony')
                            
                            # Pattern Confidence
                            if 'pattern_confidence' in stock.index and pd.notna(stock['pattern_confidence']):
                                adv_data['Metric'].append('Pattern Confidence')
                                adv_data['Value'].append(f"{stock['pattern_confidence']:.1f}%")
                                adv_data['Description'].append('Pattern strength score')
                            
                            if adv_data['Metric']:
                                adv_df = pd.DataFrame(adv_data)
                                st.dataframe(
                                    adv_df,
                                    width="stretch",
                                    hide_index=True,
                                    column_config={
                                        'Metric': st.column_config.TextColumn(
                                            'Metric',
                                            help="Advanced metric name",
                                            width="medium"
                                        ),
                                        'Value': st.column_config.TextColumn(
                                            'Value',
                                            help="Metric value",
                                            width="small"
                                        ),
                                        'Description': st.column_config.TextColumn(
                                            'Description',
                                            help="What this metric measures",
                                            width="large"
                                        )
                                    }
                                )
                            else:
                                st.info("No advanced metrics available")
            
            else:
                st.warning("No stocks found matching your search criteria.")
                
                # Provide search suggestions
                st.markdown("#### 💡 Search Tips:")
                st.markdown("""
                - **Ticker Search:** Enter exact ticker symbol (e.g., RELIANCE, TCS, INFY)
                - **Company Search:** Enter part of company name (e.g., Tata, Infosys, Reliance)
                - **Partial Match:** Search works with partial text (e.g., 'REL' finds RELIANCE)
                - **Case Insensitive:** Search is not case-sensitive
                """)
        
        else:
            # Show search instructions when no search is active
            st.info("Enter a ticker symbol or company name to search")
            
            # Show top performers as suggestions
            st.markdown("#### 🏆 Today's Top Performers")
            
            if not filtered_df.empty:
                top_performers = filtered_df.nlargest(5, 'master_score')[['ticker', 'company_name', 'master_score', 'ret_1d', 'rvol']]
                
                suggestions_data = []
                for _, row in top_performers.iterrows():
                    suggestions_data.append({
                        'Ticker': row['ticker'],
                        'Company': row['company_name'][:30] + '...' if len(row['company_name']) > 30 else row['company_name'],
                        'Score': row['master_score'],
                        '1D Return': f"{row['ret_1d']:+.1f}%" if pd.notna(row['ret_1d']) else '-',
                        'RVOL': f"{row['rvol']:.1f}x" if pd.notna(row['rvol']) else '-'
                    })
                
                suggestions_df = pd.DataFrame(suggestions_data)
                
                st.dataframe(
                    suggestions_df,
                    width="stretch",
                    hide_index=True,
                    column_config={
                        'Ticker': st.column_config.TextColumn('Ticker', width="small"),
                        'Company': st.column_config.TextColumn('Company', width="large"),
                        'Score': st.column_config.ProgressColumn(
                            'Score',
                            format="%.1f",
                            min_value=0,
                            max_value=100,
                            width="small"
                        ),
                        '1D Return': st.column_config.TextColumn('1D Return', width="small"),
                        'RVOL': st.column_config.TextColumn('RVOL', width="small")
                    }
                )
                
                st.caption("💡 Tip: Click on any ticker above and copy it to search")    
                
    with tabs[5]:
        st.markdown("### 📥 Export Data")
        
        st.markdown("#### 📋 Export Templates")
        export_template = st.radio(
            "Choose export template:",
            options=[
                "Full Analysis (All Data)",
                "Day Trader Focus",
                "Swing Trader Focus",
                "Investor Focus"
            ],
            key="export_template_radio",
            help="Select a template based on your trading style"
        )
        
        template_map = {
            "Full Analysis (All Data)": "full",
            "Day Trader Focus": "day_trader",
            "Swing Trader Focus": "swing_trader",
            "Investor Focus": "investor"
        }
        
        selected_template = template_map[export_template]
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### 📊 Excel Report")
            st.markdown(
                "Comprehensive multi-sheet report including:\n"
                "- Top 100 stocks with all scores\n"
                "- Market intelligence dashboard\n"
                "- Sector rotation analysis\n"
                "- Pattern frequency analysis\n"
                "- Wave Radar signals\n"
                "- Summary statistics"
            )
            
            if st.button("Generate Excel Report", type="primary", width="stretch"):
                if len(filtered_df) == 0:
                    st.error("No data to export. Please adjust your filters.")
                else:
                    with st.spinner("Creating Excel report..."):
                        try:
                            excel_file = ExportEngine.create_excel_report(
                                filtered_df, template=selected_template
                            )
                            
                            st.download_button(
                                label="📥 Download Excel Report",
                                data=excel_file,
                                file_name=f"wave_detection_report_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}.xlsx",
                                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                            )
                            
                            st.success("Excel report generated successfully!")
                            
                        except Exception as e:
                            st.error(f"Error generating Excel report: {str(e)}")
                            logger.error(f"Excel export error: {str(e)}", exc_info=True)
        
        with col2:
            st.markdown("#### 📄 CSV Export")
            st.markdown(
                "Enhanced CSV format with:\n"
                "- All ranking scores\n"
                "- Advanced metrics (VMI, Money Flow)\n"
                "- Pattern detections\n"
                "- Market states\n"
                "- Category classifications\n"
                "- Optimized for further analysis"
            )
            
            if st.button("Generate CSV Export", width="stretch"):
                if len(filtered_df) == 0:
                    st.error("No data to export. Please adjust your filters.")
                else:
                    try:
                        csv_data = ExportEngine.create_csv_export(filtered_df)
                        
                        st.download_button(
                            label="📥 Download CSV File",
                            data=csv_data,
                            file_name=f"wave_detection_data_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}.csv",
                            mime="text/csv"
                        )
                        
                        st.success("CSV export generated successfully!")
                        
                    except Exception as e:
                        st.error(f"Error generating CSV: {str(e)}")
                        logger.error(f"CSV export error: {str(e)}", exc_info=True)
        
        st.markdown("---")
        st.markdown("#### 📊 Export Preview")
        
        export_stats = {
            "Total Stocks": len(filtered_df),
            "Average Score": f"{filtered_df['master_score'].mean():.1f}" if not filtered_df.empty else "N/A",
            "Stocks with Patterns": (filtered_df['patterns'] != '').sum() if 'patterns' in filtered_df.columns else 0,
            "High RVOL (>2x)": (filtered_df['rvol'] > 2).sum() if 'rvol' in filtered_df.columns else 0,
            "Positive 30D Returns": (filtered_df['ret_30d'] > 0).sum() if 'ret_30d' in filtered_df.columns else 0,
            "Data Quality": f"{st.session_state.data_quality.get('completeness', 0):.1f}%"
        }
        
        stat_cols = st.columns(3)
        for i, (label, value) in enumerate(export_stats.items()):
            with stat_cols[i % 3]:
                UIComponents.render_metric_card(label, value)
    
    with tabs[6]:
        st.markdown("### ℹ️ About Wave Detection 3.0")
        
        # Main content in clean two-column layout
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.markdown("#### 🎯 System Overview")
            st.markdown("""
            Wave Detection 3.0 is a professional-grade stock ranking system designed for institutional-quality market analysis. 
            The system combines advanced technical analysis, volume dynamics, and pattern recognition to identify high-potential 
            investment opportunities.
            
            #### 📊 Master Score 3.0 Algorithm
            Our proprietary ranking algorithm evaluates stocks across six key dimensions:
            
            - **Position Analysis (27%)** - 52-week range positioning and momentum
            - **Volume Dynamics (23%)** - Multi-timeframe volume pattern analysis  
            - **Momentum Tracking (22%)** - 30-day price momentum measurement
            - **Breakout Probability (18%)** - Technical breakout readiness assessment
            - **RVOL Integration (10%)** - Real-time relative volume analysis
            
            #### 🔍 Pattern Detection System
            The system employs 41 sophisticated pattern detection algorithms organized into seven categories:
            
            **Technical Patterns (13)**
            - Core momentum and volume patterns
            - Mathematical indicators (Premium Momentum, Entropy, Velocity)
            - Advanced institutional detection
            
            **Fundamental Patterns (9)** *(Hybrid Mode)*
            - Value momentum analysis
            - Earnings-based patterns  
            - Quality and growth indicators
            
            **Market Intelligence (19)**
            - Range analysis patterns
            - Reversal and rotation detection
            - Market psychology indicators
            
            #### 🧠 Market State Intelligence
            Advanced 8-regime market detection system that adapts scoring based on current market conditions:
            - **Bull Markets:** STRONG_UPTREND, UPTREND, PULLBACK
            - **Neutral:** NEUTRAL, UNCLEAR  
            - **Bear Markets:** DISTRIBUTION, DOWNTREND, STRONG_DOWNTREND
            """)
        
        with col2:
            st.markdown("#### ⚡ Performance Specifications")
            
            # Clean performance metrics
            UIComponents.render_metric_card("Initial Load Time", "< 2 seconds")
            UIComponents.render_metric_card("Filter Response", "< 200ms") 
            UIComponents.render_metric_card("Search Speed", "< 50ms")
            UIComponents.render_metric_card("Stock Capacity", "2,000+")
            
            st.markdown("#### 🔧 Technical Features")
            st.markdown("""
            - **Data Sources:** Google Sheets, CSV upload
            - **Caching:** Smart 1-hour cache with validation
            - **UI:** Mobile-responsive design
            - **Export:** Professional Excel templates
            - **Error Handling:** Graceful degradation
            - **Memory:** Optimized for large datasets
            """)
            
            st.markdown("#### 📈 Display Modes")
            st.markdown("""
            **Technical Mode** (Default)
            - Pure momentum analysis
            - Volume pattern focus
            - Technical indicators only
            
            **Hybrid Mode**
            - Technical + fundamental analysis
            - PE ratio evaluation
            - EPS growth tracking
            - Value pattern detection
            """)
            
            st.markdown("#### 🇮🇳 Market Optimization")
            st.markdown("""
            - Currency: ₹ (Indian Rupee)
            - Timezone: IST (Indian Standard Time)
            - Exchanges: NSE/BSE categories
            - Number format: Indian conventions
            """)
        
        # System architecture section
        st.markdown("---")
        st.markdown("#### 🏗️ System Architecture")
        
        arch_col1, arch_col2, arch_col3 = st.columns(3)
        
        with arch_col1:
            st.markdown("""
            **Core Engines**
            - RankingEngine: Score calculation
            - PatternDetector: 41-pattern system
            - DataValidator: Quality assurance
            - FilterEngine: Real-time filtering
            """)
        
        with arch_col2:
            st.markdown("""
            **Management Layer**
            - SessionStateManager: State persistence
            - CacheManager: Performance optimization
            - ConfigManager: Dynamic configuration
            - UIComponents: Responsive interface
            """)
        
        with arch_col3:
            st.markdown("""
            **Analytics Modules**
            - MarketStateAnalyzer: Regime detection
            - VolumeAnalyzer: Volume dynamics
            - MomentumEngine: Trend analysis
            - ExportManager: Professional reports
            """)
        
        # Current session statistics
        st.markdown("---")
        st.markdown("#### 📊 Current Session")
        
        session_cols = st.columns(4)
        
        with session_cols[0]:
            UIComponents.render_metric_card(
                "Stocks Loaded",
                f"{len(ranked_df):,}" if ranked_df is not None else "0"
            )
        
        with session_cols[1]:
            # Use ranked_df_display which is always available at this point
            display_count = len(ranked_df_display) if 'ranked_df_display' in locals() else len(ranked_df) if ranked_df is not None else 0
            UIComponents.render_metric_card(
                "Filtered Results",
                f"{display_count:,}"
            )
        
        with session_cols[2]:
            # Initialize data_quality if not exists
            if 'data_quality' not in st.session_state:
                st.session_state.data_quality = {'completeness': 85.0}
            
            data_quality = st.session_state.data_quality.get('completeness', 0)
            UIComponents.render_metric_card(
                "Data Quality",
                f"{'🟢' if data_quality > 80 else '🟡' if data_quality > 60 else '🔴'} {data_quality:.1f}%"
            )
        
        with session_cols[3]:
            # Initialize last_refresh if not exists
            if 'last_refresh' not in st.session_state:
                st.session_state.last_refresh = datetime.now(timezone.utc)
            
            cache_minutes = int((datetime.now(timezone.utc) - st.session_state.last_refresh).total_seconds() / 60)
            UIComponents.render_metric_card(
                "Cache Status",
                f"{'🟢' if cache_minutes < 60 else '🔴'} {cache_minutes}min ago"
            )
        
        # Footer
        st.markdown("---")
        st.markdown("""
        <div style="
            text-align: center; 
            color: #666; 
            padding: 1rem;
            background: #f8f9fa;
            border-radius: 5px;
            margin-top: 2rem;
        ">
            <strong>🌊 Wave Detection 3.0</strong><br>
            <small>Professional Stock Ranking System for Institutional-Grade Market Analysis</small>
        </div>
        """, unsafe_allow_html=True)

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        st.error(f"Critical Application Error: {str(e)}")
        logger.error(f"Application crashed: {str(e)}", exc_info=True)
        
        if st.button("🔄 Restart Application"):
            st.cache_data.clear()
            st.rerun()
        
        if st.button("📧 Report Issue"):
            st.info("Please take a screenshot and report this error.")
